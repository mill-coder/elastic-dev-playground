{
  "version": "8.17",
  "plugins": {
    "filter": [
      "aggregate",
      "anonymize",
      "cidr",
      "clone",
      "csv",
      "date",
      "de_dot",
      "dissect",
      "dns",
      "drop",
      "elastic_integration",
      "elasticsearch",
      "fingerprint",
      "geoip",
      "grok",
      "http",
      "jdbc_static",
      "jdbc_streaming",
      "json",
      "kv",
      "memcached",
      "metrics",
      "mutate",
      "prune",
      "ruby",
      "sleep",
      "split",
      "syslog_pri",
      "throttle",
      "translate",
      "truncate",
      "urldecode",
      "useragent",
      "uuid",
      "xml"
    ],
    "input": [
      "azure_event_hubs",
      "beats",
      "cloudwatch",
      "couchdb_changes",
      "dead_letter_queue",
      "elastic_serverless_forwarder",
      "elasticsearch",
      "exec",
      "file",
      "ganglia",
      "gelf",
      "generator",
      "graphite",
      "heartbeat",
      "http",
      "http_poller",
      "jdbc",
      "jms",
      "kafka",
      "logstash",
      "pipe",
      "rabbitmq",
      "redis",
      "s3",
      "snmp",
      "snmptrap",
      "sqs",
      "stdin",
      "syslog",
      "tcp",
      "twitter",
      "udp",
      "unix"
    ],
    "output": [
      "cloudwatch",
      "csv",
      "elastic_app_search",
      "elastic_workplace_search",
      "elasticsearch",
      "email",
      "file",
      "graphite",
      "http",
      "kafka",
      "logstash",
      "lumberjack",
      "nagios",
      "null",
      "pipe",
      "rabbitmq",
      "redis",
      "s3",
      "sns",
      "sqs",
      "stdout",
      "tcp",
      "udp",
      "webhdfs"
    ]
  },
  "codecs": [
    "avro",
    "cef",
    "cloudfront",
    "cloudtrail",
    "collectd",
    "dots",
    "edn",
    "edn_lines",
    "es_bulk",
    "fluent",
    "graphite",
    "json",
    "json_lines",
    "line",
    "msgpack",
    "multiline",
    "netflow",
    "plain",
    "rubydebug"
  ],
  "commonOptions": {
    "filter": [
      "add_field",
      "add_tag",
      "enable_metric",
      "id",
      "periodic_flush",
      "remove_field",
      "remove_tag"
    ],
    "input": [
      "add_field",
      "codec",
      "enable_metric",
      "id",
      "tags",
      "type"
    ],
    "output": [
      "codec",
      "enable_metric",
      "id",
      "workers"
    ]
  },
  "pluginOptions": {
    "codec/avro": [
      "encoding",
      "schema_uri",
      "tag_on_failure",
      "target"
    ],
    "codec/cef": [
      "default_timezone",
      "delimiter",
      "device",
      "fields",
      "locale",
      "name",
      "product",
      "raw_data_field",
      "reverse_mapping",
      "severity",
      "signature",
      "vendor",
      "version"
    ],
    "codec/cloudfront": [
      "charset"
    ],
    "codec/cloudtrail": [
      "charset"
    ],
    "codec/collectd": [
      "authfile",
      "nan_handling",
      "nan_tag",
      "nan_value",
      "prune_intervals",
      "security_level",
      "target",
      "typesdb"
    ],
    "codec/edn": [
      "target"
    ],
    "codec/edn_lines": [
      "target"
    ],
    "codec/es_bulk": [
      "target"
    ],
    "codec/fluent": [
      "nanosecond_precision",
      "target"
    ],
    "codec/graphite": [
      "exclude_metrics",
      "fields_are_metrics",
      "include_metrics",
      "metrics",
      "metrics_format"
    ],
    "codec/json": [
      "charset",
      "target"
    ],
    "codec/json_lines": [
      "charset",
      "decode_size_limit_bytes",
      "delimiter",
      "target"
    ],
    "codec/line": [
      "charset",
      "delimiter",
      "format"
    ],
    "codec/msgpack": [
      "format",
      "target"
    ],
    "codec/multiline": [
      "auto_flush_interval",
      "charset",
      "max_bytes",
      "max_lines",
      "multiline_tag",
      "negate",
      "pattern",
      "patterns_dir",
      "what"
    ],
    "codec/netflow": [
      "cache_save_path",
      "cache_ttl",
      "include_flowset_id",
      "ipfix_definitions",
      "netflow_definitions",
      "target",
      "versions"
    ],
    "codec/plain": [
      "charset",
      "format"
    ],
    "codec/rubydebug": [
      "metadata"
    ],
    "filter/aggregate": [
      "aggregate_maps_path",
      "code",
      "end_of_task",
      "inactivity_timeout",
      "map_action",
      "push_map_as_event_on_timeout",
      "push_previous_map_as_event",
      "task_id",
      "timeout",
      "timeout_code",
      "timeout_tags",
      "timeout_task_id_field",
      "timeout_timestamp_field"
    ],
    "filter/anonymize": [
      "algorithm",
      "fields",
      "key"
    ],
    "filter/cidr": [
      "address",
      "network",
      "network_path",
      "refresh_interval",
      "separator"
    ],
    "filter/clone": [
      "clones"
    ],
    "filter/csv": [
      "autodetect_column_names",
      "autogenerate_column_names",
      "columns",
      "convert",
      "quote_char",
      "separator",
      "skip_empty_columns",
      "skip_empty_rows",
      "skip_header",
      "source",
      "target"
    ],
    "filter/date": [
      "locale",
      "match",
      "tag_on_failure",
      "target",
      "timezone"
    ],
    "filter/de_dot": [
      "fields",
      "nested",
      "recursive",
      "separator"
    ],
    "filter/dissect": [
      "convert_datatype",
      "mapping",
      "tag_on_failure"
    ],
    "filter/dns": [
      "action",
      "failed_cache_size",
      "failed_cache_ttl",
      "hit_cache_size",
      "hit_cache_ttl",
      "hostsfile",
      "max_retries",
      "nameserver",
      "resolve",
      "reverse",
      "tag_on_timeout",
      "timeout"
    ],
    "filter/drop": [
      "percentage"
    ],
    "filter/elasticsearch": [
      "aggregation_fields",
      "api_key",
      "ca_file",
      "cloud_auth",
      "cloud_id",
      "docinfo_fields",
      "enable_sort",
      "fields",
      "hosts",
      "index",
      "keystore",
      "keystore_password",
      "password",
      "proxy",
      "query",
      "query_template",
      "result_size",
      "retry_on_failure",
      "retry_on_status",
      "sort",
      "ssl",
      "ssl_certificate",
      "ssl_certificate_authorities",
      "ssl_cipher_suites",
      "ssl_enabled",
      "ssl_key",
      "ssl_keystore_password",
      "ssl_keystore_path",
      "ssl_keystore_type",
      "ssl_supported_protocols",
      "ssl_truststore_password",
      "ssl_truststore_path",
      "ssl_truststore_type",
      "ssl_verification_mode",
      "tag_on_failure",
      "user"
    ],
    "filter/fingerprint": [
      "base64encode",
      "concatenate_all_fields",
      "concatenate_sources",
      "key",
      "method",
      "source",
      "target"
    ],
    "filter/geoip": [
      "cache_size",
      "database",
      "default_database_type",
      "fields",
      "source",
      "tag_on_failure",
      "target"
    ],
    "filter/grok": [
      "break_on_match",
      "keep_empty_captures",
      "match",
      "named_captures_only",
      "overwrite",
      "pattern_definitions",
      "patterns_dir",
      "patterns_files_glob",
      "tag_on_failure",
      "tag_on_timeout",
      "target",
      "timeout_millis",
      "timeout_scope"
    ],
    "filter/http": [
      "body",
      "body_format",
      "headers",
      "query",
      "tag_on_json_failure",
      "tag_on_request_failure",
      "target_body",
      "target_headers",
      "url",
      "verb"
    ],
    "filter/jdbc_static": [
      "jdbc_connection_string",
      "jdbc_driver_class",
      "jdbc_driver_library",
      "jdbc_password",
      "jdbc_user",
      "loader_schedule",
      "loaders",
      "local_db_objects",
      "local_lookups",
      "staging_directory",
      "tag_on_default_use",
      "tag_on_failure"
    ],
    "filter/jdbc_streaming": [
      "cache_expiration",
      "cache_size",
      "default_hash",
      "jdbc_connection_string",
      "jdbc_driver_class",
      "jdbc_driver_library",
      "jdbc_password",
      "jdbc_user",
      "jdbc_validate_connection",
      "jdbc_validation_timeout",
      "parameters",
      "prepared_statement_bind_values",
      "prepared_statement_name",
      "prepared_statement_warn_on_constant_usage",
      "sequel_opts",
      "statement",
      "tag_on_default_use",
      "tag_on_failure",
      "target",
      "use_cache",
      "use_prepared_statements"
    ],
    "filter/json": [
      "skip_on_invalid_json",
      "source",
      "tag_on_failure",
      "target"
    ],
    "filter/kv": [
      "allow_duplicate_values",
      "allow_empty_values",
      "default_keys",
      "exclude_keys",
      "field_split",
      "field_split_pattern",
      "include_brackets",
      "include_keys",
      "prefix",
      "recursive",
      "remove_char_key",
      "remove_char_value",
      "source",
      "tag_on_failure",
      "tag_on_timeout",
      "target",
      "timeout_millis",
      "transform_key",
      "transform_value",
      "trim_key",
      "trim_value",
      "value_split",
      "value_split_pattern",
      "whitespace"
    ],
    "filter/memcached": [
      "get",
      "hosts",
      "namespace",
      "set",
      "tag_on_failure",
      "ttl"
    ],
    "filter/metrics": [
      "clear_interval",
      "flush_interval",
      "ignore_older_than",
      "meter",
      "percentiles",
      "rates",
      "timer"
    ],
    "filter/mutate": [
      "capitalize",
      "coerce",
      "convert",
      "copy",
      "gsub",
      "join",
      "lowercase",
      "merge",
      "rename",
      "replace",
      "split",
      "strip",
      "tag_on_failure",
      "update",
      "uppercase"
    ],
    "filter/prune": [
      "blacklist_names",
      "blacklist_values",
      "interpolate",
      "whitelist_names",
      "whitelist_values"
    ],
    "filter/ruby": [
      "code",
      "init",
      "path",
      "script_params",
      "tag_on_exception",
      "tag_with_exception_message"
    ],
    "filter/sleep": [
      "every",
      "replay",
      "time"
    ],
    "filter/split": [
      "field",
      "target",
      "terminator"
    ],
    "filter/syslog_pri": [
      "facility_labels",
      "severity_labels",
      "syslog_pri_field_name",
      "use_labels"
    ],
    "filter/throttle": [
      "after_count",
      "before_count",
      "key",
      "max_age",
      "max_counters",
      "period",
      "periodic_flush"
    ],
    "filter/translate": [
      "destination",
      "dictionary",
      "dictionary_path",
      "exact",
      "fallback",
      "field",
      "iterate_on",
      "override",
      "refresh_behaviour",
      "refresh_interval",
      "regex",
      "source",
      "target",
      "yaml_dictionary_code_point_limit"
    ],
    "filter/truncate": [
      "fields",
      "length_bytes"
    ],
    "filter/urldecode": [
      "all_fields",
      "charset",
      "field",
      "tag_on_failure"
    ],
    "filter/useragent": [
      "lru_cache_size",
      "prefix",
      "regexes",
      "source",
      "target"
    ],
    "filter/uuid": [
      "overwrite",
      "target"
    ],
    "filter/xml": [
      "force_array",
      "force_content",
      "namespaces",
      "parse_options",
      "remove_namespaces",
      "source",
      "store_xml",
      "suppress_empty",
      "target",
      "xpath"
    ],
    "input/azure_event_hubs": [
      "checkpoint_interval",
      "config_mode",
      "consumer_group",
      "decorate_events",
      "event_hub_connections",
      "event_hubs",
      "initial_position",
      "initial_position_look_back",
      "max_batch_size",
      "prefetch_count",
      "receive_timeout",
      "storage_connection",
      "storage_container",
      "threads"
    ],
    "input/beats": [
      "add_hostname",
      "cipher_suites",
      "client_inactivity_timeout",
      "enrich",
      "event_loop_threads",
      "executor_threads",
      "host",
      "include_codec_tag",
      "port",
      "ssl",
      "ssl_certificate",
      "ssl_certificate_authorities",
      "ssl_cipher_suites",
      "ssl_client_authentication",
      "ssl_enabled",
      "ssl_handshake_timeout",
      "ssl_key",
      "ssl_key_passphrase",
      "ssl_peer_metadata",
      "ssl_supported_protocols",
      "ssl_verify_mode",
      "tls_max_version",
      "tls_min_version"
    ],
    "input/cloudwatch": [
      "combined",
      "filters",
      "interval",
      "metrics",
      "namespace",
      "period",
      "statistics"
    ],
    "input/couchdb_changes": [
      "always_reconnect",
      "ca_file",
      "db",
      "heartbeat",
      "host",
      "ignore_attachments",
      "initial_sequence",
      "keep_id",
      "keep_revision",
      "password",
      "port",
      "reconnect_delay",
      "secure",
      "sequence_path",
      "timeout",
      "username"
    ],
    "input/dead_letter_queue": [
      "clean_consumed",
      "commit_offsets",
      "path",
      "pipeline_id",
      "sincedb_path",
      "start_timestamp"
    ],
    "input/elastic_serverless_forwarder": [
      "auth_basic_password",
      "auth_basic_username",
      "host",
      "port",
      "ssl",
      "ssl_certificate",
      "ssl_certificate_authorities",
      "ssl_cipher_suites",
      "ssl_client_authentication",
      "ssl_enabled",
      "ssl_handshake_timeout",
      "ssl_key",
      "ssl_key_passphrase",
      "ssl_supported_protocols",
      "ssl_verification_mode"
    ],
    "input/elasticsearch": [
      "api_key",
      "ca_file",
      "cloud_auth",
      "cloud_id",
      "connect_timeout_seconds",
      "docinfo",
      "docinfo_fields",
      "docinfo_target",
      "hosts",
      "index",
      "password",
      "proxy",
      "query",
      "request_timeout_seconds",
      "response_type",
      "retries",
      "schedule",
      "scroll",
      "search_api",
      "size",
      "slices",
      "socket_timeout_seconds",
      "ssl",
      "ssl_certificate",
      "ssl_certificate_authorities",
      "ssl_certificate_verification",
      "ssl_cipher_suites",
      "ssl_enabled",
      "ssl_key",
      "ssl_keystore_password",
      "ssl_keystore_path",
      "ssl_keystore_type",
      "ssl_supported_protocols",
      "ssl_truststore_password",
      "ssl_truststore_path",
      "ssl_truststore_type",
      "ssl_verification_mode",
      "target",
      "user"
    ],
    "input/exec": [
      "command",
      "interval",
      "schedule"
    ],
    "input/file": [
      "check_archive_validity",
      "close_older",
      "delimiter",
      "discover_interval",
      "exclude",
      "exit_after_read",
      "file_chunk_count",
      "file_chunk_size",
      "file_completed_action",
      "file_completed_log_path",
      "file_sort_by",
      "file_sort_direction",
      "ignore_older",
      "max_open_files",
      "mode",
      "path",
      "sincedb_clean_after",
      "sincedb_path",
      "sincedb_write_interval",
      "start_position",
      "stat_interval"
    ],
    "input/ganglia": [
      "host",
      "port"
    ],
    "input/gelf": [
      "host",
      "port",
      "port_tcp",
      "port_udp",
      "remap",
      "strip_leading_underscore",
      "use_tcp",
      "use_udp"
    ],
    "input/generator": [
      "count",
      "lines",
      "message"
    ],
    "input/heartbeat": [
      "count",
      "interval",
      "message",
      "sequence"
    ],
    "input/http": [
      "additional_codecs",
      "cipher_suites",
      "host",
      "keystore",
      "keystore_password",
      "max_content_length",
      "max_pending_requests",
      "password",
      "port",
      "remote_host_target_field",
      "request_headers_target_field",
      "response_code",
      "response_headers",
      "ssl",
      "ssl_certificate",
      "ssl_certificate_authorities",
      "ssl_cipher_suites",
      "ssl_client_authentication",
      "ssl_enabled",
      "ssl_handshake_timeout",
      "ssl_key",
      "ssl_key_passphrase",
      "ssl_keystore_password",
      "ssl_keystore_path",
      "ssl_keystore_type",
      "ssl_supported_protocols",
      "ssl_truststore_password",
      "ssl_truststore_path",
      "ssl_truststore_type",
      "ssl_verify_mode",
      "threads",
      "tls_max_version",
      "tls_min_version",
      "user",
      "verify_mode"
    ],
    "input/http_poller": [
      "metadata_target",
      "schedule",
      "target",
      "urls"
    ],
    "input/jdbc": [
      "charset",
      "clean_run",
      "columns_charset",
      "connection_retry_attempts",
      "connection_retry_attempts_wait_time",
      "jdbc_connection_string",
      "jdbc_default_timezone",
      "jdbc_driver_class",
      "jdbc_driver_library",
      "jdbc_fetch_size",
      "jdbc_page_size",
      "jdbc_paging_enabled",
      "jdbc_paging_mode",
      "jdbc_password",
      "jdbc_password_filepath",
      "jdbc_pool_timeout",
      "jdbc_user",
      "jdbc_validate_connection",
      "jdbc_validation_timeout",
      "last_run_metadata_path",
      "lowercase_column_names",
      "parameters",
      "plugin_timezone",
      "prepared_statement_bind_values",
      "prepared_statement_name",
      "record_last_run",
      "schedule",
      "sequel_opts",
      "sql_log_level",
      "statement",
      "statement_filepath",
      "statement_retry_attempts",
      "statement_retry_attempts_wait_time",
      "target",
      "tracking_column",
      "tracking_column_type",
      "use_column_value",
      "use_prepared_statements"
    ],
    "input/jms": [
      "broker_url",
      "destination",
      "durable_subscriber",
      "durable_subscriber_client_id",
      "durable_subscriber_name",
      "factory",
      "factory_settings",
      "headers_target",
      "include_body",
      "include_header",
      "include_headers",
      "include_properties",
      "interval",
      "jndi_context",
      "jndi_name",
      "keystore",
      "keystore_password",
      "oracle_aq_buffered_messages",
      "password",
      "properties_target",
      "pub_sub",
      "require_jars",
      "runner",
      "selector",
      "skip_headers",
      "skip_properties",
      "system_properties",
      "target",
      "timeout",
      "truststore",
      "truststore_password",
      "use_jms_timestamp",
      "username",
      "yaml_file",
      "yaml_section"
    ],
    "input/kafka": [
      "auto_commit_interval_ms",
      "auto_create_topics",
      "auto_offset_reset",
      "bootstrap_servers",
      "check_crcs",
      "client_dns_lookup",
      "client_id",
      "client_rack",
      "codec",
      "consumer_threads",
      "decorate_events",
      "enable_auto_commit",
      "exclude_internal_topics",
      "fetch_max_bytes",
      "fetch_max_wait_ms",
      "fetch_min_bytes",
      "group_id",
      "group_instance_id",
      "heartbeat_interval_ms",
      "isolation_level",
      "jaas_path",
      "kerberos_config",
      "key_deserializer_class",
      "max_partition_fetch_bytes",
      "max_poll_interval_ms",
      "max_poll_records",
      "partition_assignment_strategy",
      "poll_timeout_ms",
      "receive_buffer_bytes",
      "reconnect_backoff_ms",
      "retry_backoff_ms",
      "sasl_client_callback_handler_class",
      "sasl_jaas_config",
      "sasl_kerberos_service_name",
      "sasl_mechanism",
      "schema_registry_key",
      "schema_registry_proxy",
      "schema_registry_secret",
      "schema_registry_ssl_keystore_location",
      "schema_registry_ssl_keystore_password",
      "schema_registry_ssl_keystore_type",
      "schema_registry_ssl_truststore_location",
      "schema_registry_ssl_truststore_password",
      "schema_registry_ssl_truststore_type",
      "schema_registry_url",
      "schema_registry_validation",
      "security_protocol",
      "send_buffer_bytes",
      "session_timeout_ms",
      "ssl_endpoint_identification_algorithm",
      "ssl_key_password",
      "ssl_keystore_location",
      "ssl_keystore_password",
      "ssl_keystore_type",
      "ssl_truststore_location",
      "ssl_truststore_password",
      "ssl_truststore_type",
      "topics",
      "topics_pattern",
      "value_deserializer_class"
    ],
    "input/logstash": [
      "host",
      "password",
      "port",
      "ssl_certificate",
      "ssl_certificate_authorities",
      "ssl_cipher_suites",
      "ssl_client_authentication",
      "ssl_enabled",
      "ssl_handshake_timeout",
      "ssl_key",
      "ssl_key_passphrase",
      "ssl_keystore_password",
      "ssl_keystore_path",
      "ssl_supported_protocols",
      "username"
    ],
    "input/pipe": [
      "command"
    ],
    "input/rabbitmq": [
      "ack",
      "arguments",
      "auto_delete",
      "durable",
      "exchange",
      "exchange_type",
      "exclusive",
      "key",
      "metadata_enabled",
      "passive",
      "prefetch_count",
      "queue",
      "subscription_retry_interval_seconds"
    ],
    "input/redis": [
      "batch_count",
      "command_map",
      "data_type",
      "db",
      "host",
      "key",
      "password",
      "path",
      "port",
      "ssl",
      "timeout"
    ],
    "input/s3": [
      "additional_settings",
      "backup_add_prefix",
      "backup_to_bucket",
      "backup_to_dir",
      "bucket",
      "delete",
      "exclude_pattern",
      "gzip_pattern",
      "include_object_properties",
      "interval",
      "prefix",
      "sincedb_path",
      "temporary_directory",
      "watch_for_new_files"
    ],
    "input/snmp": [
      "get",
      "hosts",
      "interval",
      "local_engine_id",
      "poll_hosts_timeout",
      "tables",
      "threads",
      "walk"
    ],
    "input/snmptrap": [
      "community",
      "host",
      "port",
      "supported_transports",
      "supported_versions",
      "threads",
      "yamlmibdir"
    ],
    "input/sqs": [
      "additional_settings",
      "id_field",
      "md5_field",
      "polling_frequency",
      "queue",
      "queue_owner_aws_account_id",
      "sent_timestamp_field"
    ],
    "input/syslog": [
      "facility_labels",
      "grok_pattern",
      "host",
      "locale",
      "port",
      "proxy_protocol",
      "service_type",
      "severity_labels",
      "syslog_field",
      "timezone",
      "use_labels"
    ],
    "input/tcp": [
      "dns_reverse_lookup_enabled",
      "host",
      "mode",
      "port",
      "proxy_protocol",
      "ssl_cert",
      "ssl_certificate",
      "ssl_certificate_authorities",
      "ssl_cipher_suites",
      "ssl_client_authentication",
      "ssl_enable",
      "ssl_enabled",
      "ssl_extra_chain_certs",
      "ssl_key",
      "ssl_key_passphrase",
      "ssl_supported_protocols",
      "ssl_verification_mode",
      "ssl_verify",
      "tcp_keep_alive"
    ],
    "input/twitter": [
      "consumer_key",
      "consumer_secret",
      "follows",
      "full_tweet",
      "ignore_retweets",
      "keywords",
      "languages",
      "locations",
      "oauth_token",
      "oauth_token_secret",
      "proxy_address",
      "proxy_port",
      "rate_limit_reset_in",
      "target",
      "use_proxy",
      "use_samples"
    ],
    "input/udp": [
      "buffer_size",
      "host",
      "port",
      "queue_size",
      "receive_buffer_bytes",
      "source_ip_fieldname",
      "workers"
    ],
    "input/unix": [
      "data_timeout",
      "force_unlink",
      "mode",
      "path",
      "socket_not_present_retry_interval_seconds"
    ],
    "output/cloudwatch": [
      "batch_size",
      "dimensions",
      "field_dimensions",
      "field_metricname",
      "field_namespace",
      "field_unit",
      "field_value",
      "metricname",
      "namespace",
      "queue_size",
      "timeframe",
      "unit",
      "value"
    ],
    "output/csv": [
      "csv_options",
      "fields",
      "spreadsheet_safe"
    ],
    "output/elastic_app_search": [
      "api_key",
      "document_id",
      "engine",
      "timestamp_destination",
      "url"
    ],
    "output/elastic_workplace_search": [
      "access_token",
      "document_id",
      "source",
      "timestamp_destination",
      "url"
    ],
    "output/elasticsearch": [
      "action",
      "api_key",
      "bulk_path",
      "ca_trusted_fingerprint",
      "cacert",
      "cloud_auth",
      "cloud_id",
      "compression_level",
      "custom_headers",
      "dlq_custom_codes",
      "dlq_on_failed_indexname_interpolation",
      "doc_as_upsert",
      "document_id",
      "document_type",
      "failure_type_logging_whitelist",
      "healthcheck_path",
      "hosts",
      "http_compression",
      "ilm_enabled",
      "ilm_pattern",
      "ilm_policy",
      "ilm_rollover_alias",
      "index",
      "join_field",
      "keystore",
      "keystore_password",
      "manage_template",
      "parameters",
      "parent",
      "password",
      "path",
      "pipeline",
      "pool_max",
      "pool_max_per_route",
      "proxy",
      "resurrect_delay",
      "retry_initial_interval",
      "retry_max_interval",
      "retry_on_conflict",
      "routing",
      "script",
      "script_lang",
      "script_type",
      "script_var_name",
      "scripted_upsert",
      "silence_errors_in_log",
      "sniffing",
      "sniffing_delay",
      "sniffing_path",
      "ssl",
      "ssl_certificate",
      "ssl_certificate_authorities",
      "ssl_certificate_verification",
      "ssl_cipher_suites",
      "ssl_enabled",
      "ssl_key",
      "ssl_keystore_password",
      "ssl_keystore_path",
      "ssl_keystore_type",
      "ssl_supported_protocols",
      "ssl_truststore_password",
      "ssl_truststore_path",
      "ssl_truststore_type",
      "ssl_verification_mode",
      "template",
      "template_api",
      "template_name",
      "template_overwrite",
      "timeout",
      "truststore",
      "truststore_password",
      "upsert",
      "user",
      "validate_after_inactivity",
      "version",
      "version_type"
    ],
    "output/email": [
      "address",
      "attachments",
      "authentication",
      "bcc",
      "body",
      "cc",
      "contenttype",
      "debug",
      "domain",
      "from",
      "htmlbody",
      "password",
      "port",
      "replyto",
      "subject",
      "template_file",
      "to",
      "use_tls",
      "username",
      "via"
    ],
    "output/file": [
      "create_if_deleted",
      "dir_mode",
      "file_mode",
      "filename_failure",
      "flush_interval",
      "gzip",
      "path",
      "stale_cleanup_interval",
      "write_behavior"
    ],
    "output/graphite": [
      "exclude_metrics",
      "fields_are_metrics",
      "host",
      "include_metrics",
      "metrics",
      "metrics_format",
      "nested_object_separator",
      "port",
      "reconnect_interval",
      "resend_on_failure",
      "timestamp_field"
    ],
    "output/http": [
      "content_type",
      "format",
      "headers",
      "http_compression",
      "http_method",
      "ignorable_codes",
      "mapping",
      "message",
      "retry_failed",
      "retryable_codes",
      "url"
    ],
    "output/kafka": [
      "acks",
      "batch_size",
      "bootstrap_servers",
      "buffer_memory",
      "client_dns_lookup",
      "client_id",
      "compression_type",
      "jaas_path",
      "kerberos_config",
      "key_serializer",
      "linger_ms",
      "max_request_size",
      "message_headers",
      "message_key",
      "metadata_fetch_timeout_ms",
      "partitioner",
      "receive_buffer_bytes",
      "reconnect_backoff_ms",
      "retries",
      "retry_backoff_ms",
      "sasl_client_callback_handler_class",
      "sasl_jaas_config",
      "sasl_kerberos_service_name",
      "sasl_mechanism",
      "security_protocol",
      "send_buffer_bytes",
      "ssl_endpoint_identification_algorithm",
      "ssl_key_password",
      "ssl_keystore_location",
      "ssl_keystore_password",
      "ssl_keystore_type",
      "ssl_truststore_location",
      "ssl_truststore_password",
      "ssl_truststore_type",
      "topic_id",
      "value_serializer"
    ],
    "output/logstash": [
      "hosts",
      "ssl_enabled",
      "user",
      "username"
    ],
    "output/lumberjack": [
      "flush_size",
      "hosts",
      "idle_flush_time",
      "port",
      "ssl_certificate"
    ],
    "output/nagios": [
      "commandfile",
      "nagios_level"
    ],
    "output/pipe": [
      "command",
      "message_format",
      "ttl"
    ],
    "output/rabbitmq": [
      "durable",
      "exchange",
      "exchange_type",
      "key",
      "message_properties",
      "persistent"
    ],
    "output/redis": [
      "batch",
      "batch_events",
      "batch_timeout",
      "congestion_interval",
      "congestion_threshold",
      "data_type",
      "db",
      "host",
      "key",
      "password",
      "port",
      "reconnect_interval",
      "shuffle_hosts",
      "ssl_certificate",
      "ssl_certificate_authorities",
      "ssl_cipher_suites",
      "ssl_enabled",
      "ssl_key",
      "ssl_key_passphrase",
      "ssl_supported_protocols",
      "ssl_verification_mode",
      "timeout"
    ],
    "output/s3": [
      "additional_settings",
      "bucket",
      "canned_acl",
      "encoding",
      "prefix",
      "restore",
      "retry_count",
      "retry_delay",
      "rotation_strategy",
      "server_side_encryption",
      "server_side_encryption_algorithm",
      "signature_version",
      "size_file",
      "ssekms_key_id",
      "storage_class",
      "tags",
      "temporary_directory",
      "time_file",
      "upload_multipart_threshold",
      "upload_queue_size",
      "upload_workers_count",
      "validate_credentials_on_root_bucket"
    ],
    "output/sns": [
      "arn",
      "publish_boot_message_arn"
    ],
    "output/sqs": [
      "batch_events",
      "message_max_size",
      "queue",
      "queue_owner_aws_account_id"
    ],
    "output/tcp": [
      "host",
      "mode",
      "port",
      "reconnect_interval",
      "ssl_cacert",
      "ssl_cert",
      "ssl_certificate",
      "ssl_certificate_authorities",
      "ssl_cipher_suites",
      "ssl_client_authentication",
      "ssl_enable",
      "ssl_enabled",
      "ssl_key",
      "ssl_key_passphrase",
      "ssl_supported_protocols",
      "ssl_verification_mode",
      "ssl_verify"
    ],
    "output/udp": [
      "host",
      "port",
      "retry_backoff_ms",
      "retry_count"
    ],
    "output/webhdfs": [
      "compression",
      "flush_size",
      "host",
      "idle_flush_time",
      "kerberos_keytab",
      "open_timeout",
      "path",
      "port",
      "read_timeout",
      "retry_interval",
      "retry_known_errors",
      "retry_times",
      "single_file_per_thread",
      "snappy_bufsize",
      "snappy_format",
      "ssl_cert",
      "ssl_key",
      "standby_host",
      "standby_port",
      "use_httpfs",
      "use_kerberos_auth",
      "use_ssl_auth",
      "user"
    ]
  },
  "pluginDocs": {
    "filter/aggregate": {
      "options": {
        "aggregate_maps_path": {
          "type": "string"
        },
        "code": {
          "type": "string",
          "required": true
        },
        "end_of_task": {
          "type": "boolean",
          "default": "false"
        },
        "inactivity_timeout": {
          "type": "number"
        },
        "map_action": {
          "type": "string, one of: create, update, create_or_update",
          "default": "create_or_update"
        },
        "push_map_as_event_on_timeout": {
          "type": "boolean",
          "default": "false"
        },
        "push_previous_map_as_event": {
          "type": "boolean",
          "default": "false"
        },
        "task_id": {
          "type": "string",
          "required": true
        },
        "timeout": {
          "type": "number"
        },
        "timeout_code": {
          "type": "string"
        },
        "timeout_tags": {
          "type": "array",
          "default": "[]"
        },
        "timeout_task_id_field": {
          "type": "string"
        },
        "timeout_timestamp_field": {
          "type": "string"
        }
      }
    },
    "filter/anonymize": {
      "description": "deprecated[3.0.3,We recommend that you use the \u003c\u003cplugins-filters-fingerprint,fingerprint filter plugin\u003e\u003e instead.]",
      "options": {
        "algorithm": {
          "type": "string, one of: SHA1, SHA256, SHA384, SHA512, MD5, MURMUR3, IPV4_NETWORK",
          "required": true,
          "default": "SHA1",
          "description": "digest/hash type"
        },
        "fields": {
          "type": "array",
          "required": true,
          "description": "The fields to be anonymized"
        },
        "key": {
          "type": "string",
          "required": true,
          "description": "Hashing key When using MURMUR3 the key is ignored but must still be set. When using IPV4_NETWORK key is the subnet prefix lentgh"
        }
      }
    },
    "filter/cidr": {
      "options": {
        "address": {
          "type": "array",
          "default": "[]",
          "description": "The IP address(es) to check with. Example:"
        },
        "network": {
          "type": "array",
          "default": "[]",
          "description": "The IP network(s) to check against. Example:"
        },
        "network_path": {
          "type": "path",
          "description": "The full path of the external file containing the IP network(s) to check against. Example:"
        },
        "refresh_interval": {
          "type": "number",
          "default": "600",
          "description": "When using a network list from a file, this setting will indicate how frequently (in seconds) Logstash will check the file for updates."
        },
        "separator": {
          "type": "string",
          "default": "\\n",
          "description": "The separator character used in the encoding of the external file pointed by network_path."
        }
      }
    },
    "filter/clone": {
      "description": "The clone filter is for duplicating events. A clone will be created for each type in the clone list. The original event is left unchanged. Created events are inserted into the pipeline as normal events and will be processed by the remaining pipeline configuration starting from the filter that generated them (i.e. this plugin).",
      "options": {
        "clones": {
          "type": "array",
          "required": true,
          "description": "A new clone will be created with the given type for each type in this list."
        }
      }
    },
    "filter/csv": {
      "description": "The CSV filter takes an event field containing CSV data, parses it, and stores it as individual fields (can optionally specify the names). This filter can also parse data with any separator, not just commas.",
      "options": {
        "autodetect_column_names": {
          "type": "boolean",
          "default": "false",
          "description": "Define whether column names should be auto-detected from the header column or not. Defaults to false."
        },
        "autogenerate_column_names": {
          "type": "boolean",
          "default": "true",
          "description": "Define whether column names should autogenerated or not. Defaults to true. If set to false, columns not having a header specified will not be parsed."
        },
        "columns": {
          "type": "array",
          "default": "[]",
          "description": "Define a list of column names (in the order they appear in the CSV, as if it were a header line). If `columns` is not configured, or there are not enough columns specified, the default column names are \"column1\", \"column2\", etc. In the case that there are more columns in the data than specified in this column list, extra columns will be auto-numbered: (e.g. \"user_defined_1\", \"user_defined_2\", \"column3\", \"column4\", etc.)"
        },
        "convert": {
          "type": "hash",
          "default": "{}",
          "description": "Define a set of datatype conversions to be applied to columns. Possible conversions are integer, float, date, date_time, boolean"
        },
        "quote_char": {
          "type": "string",
          "default": "\"",
          "description": "Define the character used to quote CSV fields. If this is not specified the default is a double quote `\"`. Optional."
        },
        "separator": {
          "type": "string",
          "default": ",",
          "description": "Define the column separator value. If this is not specified, the default is a comma `,`. If you want to define a tabulation as a separator, you need to set the value to the actual tab character and not `\\t`. Optional."
        },
        "skip_empty_columns": {
          "type": "boolean",
          "default": "false",
          "description": "Define whether empty columns should be skipped. Defaults to false. If set to true, columns containing no value will not get set."
        },
        "skip_empty_rows": {
          "type": "boolean",
          "default": "false",
          "description": "Define whether empty rows could potentially be skipped. Defaults to false. If set to true, rows containing no value will be tagged with _csvskippedemptyfield. This tag can referenced by users if they wish to cancel events using an 'if' conditional statement."
        },
        "skip_header": {
          "type": "boolean",
          "default": "false",
          "description": "Define whether the header should be skipped or not Defaults to false, If set to true, the header is dropped"
        },
        "source": {
          "type": "string",
          "default": "message",
          "description": "The CSV data in the value of the `source` field will be expanded into a data structure."
        },
        "target": {
          "type": "field_reference",
          "description": "Define target field for placing the data. Defaults to writing to the root of the event."
        }
      }
    },
    "filter/date": {
      "description": "The date filter is used for parsing dates from fields, and then using that date or timestamp as the logstash timestamp for the event.",
      "options": {
        "locale": {
          "type": "string",
          "description": "Specify a locale to be used for date parsing using either IETF-BCP47 or POSIX language tag. Simple examples are `en`,`en-US` for BCP47 or `en_US` for POSIX."
        },
        "match": {
          "type": "array",
          "default": "[]",
          "description": "An array with field name first, and format patterns following, `[ field, formats... ]`"
        },
        "tag_on_failure": {
          "type": "array",
          "default": "[\"_dateparsefailure\"]",
          "description": "Append values to the `tags` field when there has been no successful match"
        },
        "target": {
          "type": "string",
          "description": "Store the matching timestamp into the given target field.  If not provided, default to updating the `@timestamp` field of the event."
        },
        "timezone": {
          "type": "string",
          "description": "Specify a time zone canonical ID to be used for date parsing. The valid IDs are listed on the Joda.org available time zones page. This is useful in case the time zone cannot be extracted from the value, and is not the platform default. If this is not specified the platform default will be used. Canonical ID is good as it takes care of daylight saving time for you For example, `America/Los_Angeles` or `Europe/Paris` are valid IDs. This field can be dynamic and include parts of the event using the `%{field}` syntax"
        }
      }
    },
    "filter/de_dot": {
      "description": "This filter _appears_ to rename fields by replacing `.` characters with a different separator.  In reality, it's a somewhat expensive filter that has to copy the source field contents to a new destination field (whose name no longer contains dots), and then remove the corresponding source field.",
      "options": {
        "fields": {
          "type": "array",
          "description": "The `fields` array should contain a list of known fields to act on. If undefined, all top-level fields will be checked.  Sub-fields must be manually specified in the array.  For example: `[\"field.suffix\",\"[foo][bar.suffix]\"]` will result in \"field_suffix\" and nested or sub field [\"foo\"][\"bar_suffix\"]"
        },
        "nested": {
          "type": "boolean",
          "default": "false",
          "description": "If `nested` is _true_, then create sub-fields instead of replacing dots with a different separator."
        },
        "recursive": {
          "type": "boolean",
          "default": "false",
          "description": "If `recursive` is _true_, then recursively check sub-fields. It is recommended you only use this when specifying specific fields."
        },
        "separator": {
          "type": "string",
          "default": "_",
          "description": "Replace dots with this value."
        }
      }
    },
    "filter/dissect": {
      "options": {
        "convert_datatype": {
          "type": "hash",
          "default": "{}",
          "description": "With this setting `int` and `float` datatype conversions can be specified. + These will be done after all `mapping` dissections have taken place. + Feel free to use this setting on its own without a `mapping` section. +"
        },
        "mapping": {
          "type": "hash",
          "default": "{}",
          "description": "A hash of dissections of `field =\u003e value` + A later dissection can be done on values from a previous dissection or they can be independent."
        },
        "tag_on_failure": {
          "type": "array",
          "default": "[\"_dissectfailure\"]",
          "description": "Append values to the `tags` field when dissection fails"
        }
      }
    },
    "filter/dns": {
      "description": "The DNS filter performs a lookup (either an A record/CNAME record lookup or a reverse lookup at the PTR record) on records specified under the `reverse` arrays or respectively under the `resolve` arrays.",
      "options": {
        "action": {
          "type": "string, one of: append, replace",
          "default": "append",
          "description": "Determine what action to do: append or replace the values in the fields specified under `reverse` and `resolve`."
        },
        "failed_cache_size": {
          "type": "number",
          "default": "0",
          "description": "cache size for failed requests"
        },
        "failed_cache_ttl": {
          "type": "number",
          "default": "5",
          "description": "how long to cache failed requests (in seconds)"
        },
        "hit_cache_size": {
          "type": "number",
          "default": "0",
          "description": "set the size of cache for successful requests"
        },
        "hit_cache_ttl": {
          "type": "number",
          "default": "60",
          "description": "how long to cache successful requests (in seconds)"
        },
        "hostsfile": {
          "type": "array",
          "description": "Use custom hosts file(s). For example: `[\"/var/db/my_custom_hosts\"]`"
        },
        "max_retries": {
          "type": "number",
          "default": "2",
          "description": "number of times to retry a failed resolve/reverse"
        },
        "nameserver": {
          "type": "array",
          "description": "Use custom nameserver(s). For example:    filter {      dns {         nameserver =\u003e {          address =\u003e [\"8.8.8.8\", \"8.8.4.4\"]          search  =\u003e [\"internal.net\"]        }      }    }"
        },
        "resolve": {
          "type": "array",
          "description": "Forward resolve one or more fields."
        },
        "reverse": {
          "type": "array",
          "description": "Reverse resolve one or more fields."
        },
        "tag_on_timeout": {
          "type": "list of string",
          "default": "[\"_dnstimeout\"]",
          "description": "Tag(s) to apply if a DNS lookup times out. Defaults to `[\"_dnstimeout\"]`."
        },
        "timeout": {
          "type": "number",
          "default": "0.5",
          "description": "`resolv` calls will be wrapped in a timeout instance"
        }
      }
    },
    "filter/drop": {
      "description": "Drop filter.",
      "options": {
        "percentage": {
          "type": "number",
          "default": "100",
          "description": "Drop all the events within a pre-configured percentage."
        }
      }
    },
    "filter/elasticsearch": {
      "options": {
        "aggregation_fields": {
          "type": "hash",
          "default": "{}",
          "description": "Hash of aggregation names to copy from elasticsearch response into Logstash event fields"
        },
        "api_key": {
          "type": "password",
          "description": "Authenticate using Elasticsearch API key. format is id:api_key (as returned by Create API key)"
        },
        "ca_file": {
          "type": "path",
          "description": "SSL Certificate Authority file",
          "deprecated": "Set "
        },
        "cloud_auth": {
          "type": "password",
          "description": "Cloud authentication string (\"\u003cusername\u003e:\u003cpassword\u003e\" format) is an alternative for the `user`/`password` configuration."
        },
        "cloud_id": {
          "type": "string",
          "description": "Cloud ID, from the Elastic Cloud web console. If set `hosts` should not be used."
        },
        "docinfo_fields": {
          "type": "hash",
          "default": "{}",
          "description": "Hash of docinfo fields to copy from old event (found via elasticsearch) into new event"
        },
        "enable_sort": {
          "type": "boolean",
          "default": "true",
          "description": "Whether results should be sorted or not"
        },
        "fields": {
          "type": "array",
          "default": "{}",
          "description": "Array of fields to copy from old event (found via elasticsearch) into new event"
        },
        "hosts": {
          "type": "array",
          "default": "[ 'localhost:9200' ]",
          "description": "List of elasticsearch hosts to use for querying."
        },
        "index": {
          "type": "string",
          "description": "Comma-delimited list of index names to search; use `_all` or empty string to perform the operation on all indices. Field substitution (e.g. `index-name-%{date_field}`) is available"
        },
        "keystore": {
          "type": "path",
          "description": "The keystore used to present a certificate to the server. It can be either .jks or .p12",
          "deprecated": "Use "
        },
        "keystore_password": {
          "type": "password",
          "description": "Set the keystore password",
          "deprecated": "Use "
        },
        "password": {
          "type": "password",
          "description": "Basic Auth - password"
        },
        "proxy": {
          "type": "uri_or_empty",
          "description": "Set the address of a forward HTTP proxy."
        },
        "query": {
          "type": "string",
          "description": "Elasticsearch query string. Read the Elasticsearch query string documentation. for more info at: https://www.elastic.co/guide/en/elasticsearch/reference/master/query-dsl-query-string-query.html#query-string-syntax"
        },
        "query_template": {
          "type": "string",
          "description": "File path to elasticsearch query in DSL format. Read the Elasticsearch query documentation for more info at: https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl.html"
        },
        "result_size": {
          "type": "number",
          "default": "1",
          "description": "How many results to return"
        },
        "retry_on_failure": {
          "type": "number",
          "default": "0",
          "description": "How many times to retry on failure?"
        },
        "retry_on_status": {
          "type": "list of number",
          "default": "[500, 502, 503, 504]",
          "description": "What status codes to retry on?"
        },
        "sort": {
          "type": "string",
          "default": "@timestamp:desc",
          "description": "Comma-delimited list of `\u003cfield\u003e:\u003cdirection\u003e` pairs that define the sort order"
        },
        "ssl": {
          "type": "boolean",
          "default": "false",
          "description": "SSL",
          "deprecated": "Set "
        },
        "ssl_certificate": {
          "type": "path",
          "description": "OpenSSL-style X.509 certificate certificate to authenticate the client"
        },
        "ssl_certificate_authorities": {
          "type": "list of path",
          "description": "SSL Certificate Authority files in PEM encoded format, must also include any chain certificates as necessary"
        },
        "ssl_cipher_suites": {
          "type": "list of string",
          "description": "The list of cipher suites to use, listed by priorities. Supported cipher suites vary depending on which version of Java is used."
        },
        "ssl_enabled": {
          "type": "boolean",
          "description": "SSL"
        },
        "ssl_key": {
          "type": "path",
          "description": "OpenSSL-style RSA private key to authenticate the client"
        },
        "ssl_keystore_password": {
          "type": "password",
          "description": "Set the keystore password"
        },
        "ssl_keystore_path": {
          "type": "path",
          "description": "The keystore used to present a certificate to the server. It can be either .jks or .p12"
        },
        "ssl_keystore_type": {
          "type": "string, one of: pkcs12, jks",
          "description": "The format of the keystore file. It must be either jks or pkcs12"
        },
        "ssl_supported_protocols": {
          "type": "list of string, one of: TLSv1.1, TLSv1.2, TLSv1.3",
          "default": "[]",
          "description": "Supported protocols with versions."
        },
        "ssl_truststore_password": {
          "type": "password",
          "description": "Set the truststore password"
        },
        "ssl_truststore_path": {
          "type": "path",
          "description": "The JKS truststore to validate the server's certificate. Use either `:ssl_truststore_path` or `:ssl_certificate_authorities`"
        },
        "ssl_truststore_type": {
          "type": "string, one of: pkcs12, jks",
          "description": "The format of the truststore file. It must be either jks or pkcs12"
        },
        "ssl_verification_mode": {
          "type": "string, one of: full, none",
          "default": "full",
          "description": "Options to verify the server's certificate. \"full\": validates that the provided certificate has an issue date thats within the not_before and not_after dates; chains to a trusted Certificate Authority (CA); has a hostname or IP address that matches the names within the certificate. \"none\": performs no certificate validation. Disabling this severely compromises security (https://www.cs.utexas.edu/~shmat/shmat_ccs12.pdf)"
        },
        "tag_on_failure": {
          "type": "array",
          "default": "[\"_elasticsearch_lookup_failure\"]",
          "description": "Tags the event on failure to look up geo information. This can be used in later analysis."
        },
        "user": {
          "type": "string",
          "description": "Basic Auth - username"
        }
      }
    },
    "filter/fingerprint": {
      "description": "Create consistent hashes (fingerprints) of one or more fields and store the result in a new field.",
      "options": {
        "base64encode": {
          "type": "boolean",
          "default": "false",
          "description": "When set to `true`, the `SHA1`, `SHA256`, `SHA384`, `SHA512`, `MD5` and `MURMUR3_128` fingerprint methods will produce base64 encoded rather than hex encoded strings."
        },
        "concatenate_all_fields": {
          "type": "boolean",
          "default": "false",
          "description": "When set to `true` and `method` isn't `UUID` or `PUNCTUATION`, the plugin concatenates the names and values of all fields in the event without having to proide the field names in the `source` attribute"
        },
        "concatenate_sources": {
          "type": "boolean",
          "default": "false",
          "description": "When set to `true` and `method` isn't `UUID` or `PUNCTUATION`, the plugin concatenates the names and values of all fields given in the `source` option into one string (like the old checksum filter) before doing the fingerprint computation. If `false` and multiple source fields are given, the target field will be an array with fingerprints of the source fields given."
        },
        "key": {
          "type": "password",
          "description": "When used with the `IPV4_NETWORK` method fill in the subnet prefix length. With other methods, optionally fill in the HMAC key."
        },
        "method": {
          "type": "string, one of: SHA1, SHA256, SHA384, SHA512, MD5, MURMUR3, MURMUR3_128, IPV4_NETWORK, UUID, PUNCTUATION",
          "required": true,
          "default": "SHA1",
          "description": "The fingerprint method to use."
        },
        "source": {
          "type": "array",
          "default": "message",
          "description": "The name(s) of the source field(s) whose contents will be used to create the fingerprint. If an array is given, see the `concatenate_sources` option."
        },
        "target": {
          "type": "string",
          "description": "The name of the field where the generated fingerprint will be stored. Any current contents of that field will be overwritten."
        }
      }
    },
    "filter/geoip": {
      "description": "The GeoIP filter adds information about the geographical location of IP addresses, based on data from the MaxMind GeoLite2 database.",
      "options": {
        "cache_size": {
          "type": "number",
          "default": "1000",
          "description": "GeoIP lookup is surprisingly expensive. This filter uses an cache to take advantage of the fact that IPs agents are often found adjacent to one another in log files and rarely have a random distribution. The higher you set this the more likely an item is to be in the cache and the faster this filter will run. However, if you set this too high you can use more memory than desired. Since the Geoip API upgraded to v2, there is not any eviction policy so far, if cache is full, no more record can be added. Experiment with different values for this option to find the best performance for your dataset."
        },
        "database": {
          "type": "path",
          "description": "The path to the GeoLite2 database file which Logstash should use. City and ASN databases are supported."
        },
        "default_database_type": {
          "type": "string, one of: City, ASN",
          "default": "City",
          "description": "If using the default database, which type should Logstash use.  Valid values are \"City\" and \"ASN\", and case matters."
        },
        "fields": {
          "type": "array",
          "description": "An array of geoip fields to be included in the event."
        },
        "source": {
          "type": "string",
          "required": true,
          "description": "The field containing the IP address or hostname to map via geoip. If this field is an array, only the first value will be used."
        },
        "tag_on_failure": {
          "type": "array",
          "default": "[\"_geoip_lookup_failure\"]",
          "description": "Tags the event on failure to look up geo information. This can be used in later analysis."
        },
        "target": {
          "type": "string",
          "description": "Specify the field into which Logstash should store the geoip data. This can be useful, for example, if you have `src_ip` and `dst_ip` fields and would like the GeoIP information of both IPs."
        }
      }
    },
    "filter/grok": {
      "description": "Parse arbitrary text and structure it.",
      "options": {
        "break_on_match": {
          "type": "boolean",
          "default": "true",
          "description": "Break on first match. The first successful match by grok will result in the filter being finished. If you want grok to try all patterns (maybe you are parsing different things), then set this to false."
        },
        "keep_empty_captures": {
          "type": "boolean",
          "default": "false",
          "description": "If `true`, keep empty captures as event fields."
        },
        "match": {
          "type": "hash",
          "default": "{}"
        },
        "named_captures_only": {
          "type": "boolean",
          "default": "true",
          "description": "If `true`, only store named captures from grok."
        },
        "overwrite": {
          "type": "array",
          "default": "[]",
          "description": "The fields to overwrite."
        },
        "pattern_definitions": {
          "type": "hash",
          "default": "{}",
          "description": "A hash of pattern-name and pattern tuples defining custom patterns to be used by the current filter. Patterns matching existing names will override the pre-existing definition. Think of this as inline patterns available just for this definition of grok"
        },
        "patterns_dir": {
          "type": "array",
          "default": "[]",
          "description": "Logstash ships by default with a bunch of patterns, so you don't necessarily need to define this yourself unless you are adding additional patterns. You can point to multiple pattern directories using this setting. Note that Grok will read all files in the directory matching the patterns_files_glob and assume it's a pattern file (including any tilde backup files)."
        },
        "patterns_files_glob": {
          "type": "string",
          "default": "*",
          "description": "Glob pattern, used to select the pattern files in the directories specified by patterns_dir"
        },
        "tag_on_failure": {
          "type": "array",
          "default": "[\"_grokparsefailure\"]",
          "description": "Append values to the `tags` field when there has been no successful match"
        },
        "tag_on_timeout": {
          "type": "string",
          "default": "_groktimeout",
          "description": "Tag to apply if a grok regexp times out."
        },
        "target": {
          "type": "string",
          "description": "Define the target field for placing the matched captures. If this setting is omitted, data gets stored at the root (top level) of the event."
        },
        "timeout_millis": {
          "type": "number",
          "default": "30000",
          "description": "Attempt to terminate regexps after this amount of time. This applies per pattern if multiple patterns are applied This will never timeout early, but may take a little longer to timeout. Actual timeout is approximate based on a 250ms quantization. Set to 0 to disable timeouts"
        },
        "timeout_scope": {
          "type": "string, one of: pattern, event",
          "default": "pattern",
          "description": "When multiple patterns are provided to `match`, the timeout has historically applied to _each_ pattern, incurring overhead for each and every pattern that is attempted; when the grok filter is configured with `timeout_scope =\u003e 'event'`, the plugin instead enforces a single timeout across all attempted matches on the event, so it can achieve similar safeguard against runaway matchers with significantly less overhead. It's usually better to scope the timeout for the whole event."
        }
      }
    },
    "filter/http": {
      "description": "Logstash HTTP Filter This filter calls a defined URL and saves the answer into a specified field.",
      "options": {
        "body": {},
        "body_format": {
          "type": "string, one of: text, json",
          "default": "text"
        },
        "headers": {
          "type": "hash",
          "default": "{}"
        },
        "query": {
          "type": "hash",
          "default": "{}"
        },
        "tag_on_json_failure": {
          "type": "array",
          "default": "['_jsonparsefailure']"
        },
        "tag_on_request_failure": {
          "type": "array",
          "default": "['_httprequestfailure']",
          "description": "Append values to the `tags` field when there has been no successful match or json parsing error"
        },
        "target_body": {
          "type": "field_reference",
          "description": "default [body] (legacy) required to be specified in ECS mode"
        },
        "target_headers": {
          "type": "field_reference",
          "description": "default [headers] (legacy) or [@metadata][filter][http][response][headers] in ECS mode"
        },
        "url": {
          "type": "string",
          "required": true
        },
        "verb": {
          "default": "GET"
        }
      }
    },
    "filter/jdbc_static": {
      "options": {
        "jdbc_connection_string": {
          "type": "string",
          "required": true,
          "description": "Remote Load DB Jdbc connection string"
        },
        "jdbc_driver_class": {
          "type": "string",
          "required": true,
          "description": "Remote Load DB Jdbc driver class to load, for example \"oracle.jdbc.OracleDriver\" or \"org.apache.derby.jdbc.ClientDriver\""
        },
        "jdbc_driver_library": {
          "type": "string",
          "description": "Remote Load DB Jdbc driver library path to third party driver library. Use comma separated paths in one string if you need more than one library."
        },
        "jdbc_password": {
          "type": "password",
          "description": "Remote Load DB Jdbc password"
        },
        "jdbc_user": {
          "type": "string",
          "description": "Remote Load DB Jdbc user"
        },
        "loader_schedule": {
          "description": "Schedule of when to periodically run loaders, in Cron format for example: \"* * * * *\" (execute query every minute, on the minute)"
        },
        "loaders": {
          "type": "string, one of: LogStash::Filters::Jdbc::Loader",
          "default": "[]",
          "description": "Define the loaders, an Array of Hashes, to fetch remote data and create local tables. the fetched data will be inserted into the local tables. Make sure that the local table name, columns and datatypes correspond to the shape of the remote data being fetched. The default for max_rows is 1 million rows. You may provide an `id` For example: loaders =\u003e [   {     id =\u003e \"country_details\"     query =\u003e \"select code, name from WORLD.COUNTRY\"     max_rows =\u003e 2000     local_table =\u003e \"country\"   },   {     id =\u003e \"servers_load\"     query =\u003e \"select id, ip, name, location from INTERNAL.SERVERS\"     local_table =\u003e \"servers\"   } ] This is optional. You can provide a pre-populated local database server then no initial loaders are needed."
        },
        "local_db_objects": {
          "type": "string, one of: LogStash::Filters::Jdbc::DbObject",
          "default": "[]",
          "description": "Define an array of Database Objects to create when the plugin first starts. These will usually be the definitions to setup the local in-memory tables. For example: local_db_objects =\u003e [   {name =\u003e \"servers\", preserve_existing =\u003e true, index_columns =\u003e [\"ip\"], columns =\u003e [[\"id\", \"INTEGER\"], [\"ip\", \"varchar(64)\"], [\"name\", \"varchar(64)\"], [\"location\", \"varchar(64)\"]]}, ] NOTE: Important! use `preserve_existing =\u003e true` to keep a table created and filled in a previous Logstash session. It will default to false and is unneeded if the database is not persistent. NOTE: Important! Tables created here must have the same names as those used in the `loaders` and `local_lookups` configuration options"
        },
        "local_lookups": {
          "type": "string, one of: LogStash::Filters::Jdbc::LookupProcessor",
          "required": true,
          "description": "Define the list (Array) of enhancement local_lookups to be applied to an event Each entry is a hash of the query string, the target field and value and a parameter hash. Target is overwritten if existing. Target is optional, if omitted the lookup results will be written to the root of the event like this: event.set(\u003ccolumn name (or alias)\u003e, \u003ccolumn value\u003e) Use parameters to have this plugin put values from the event into the query. The parameter maps the symbol used in the query string to the field name in the event. NOTE: when using a query string that includes the LIKE keyword make sure that you provide a Logstash Event sprintf pattern with added wildcards. For example: local_lookups =\u003e [   {     \"query\" =\u003e \"SELECT * FROM country WHERE code = :code\",     \"parameters\" =\u003e {\"code\" =\u003e \"country_code\"}     \"target\" =\u003e \"country_details\"   },   {     \"query\" =\u003e \"SELECT ip, name FROM servers WHERE ip LIKE :ip\",     \"parameters\" =\u003e {\"ip\" =\u003e \"%{[response][ip]}%\"}     \"target\" =\u003e \"servers\"   },   {     \"query\" =\u003e \"SELECT ip, name FROM servers WHERE ip = ?\",     \"prepared_parameters\" =\u003e [\"from_ip\"]     \"target\" =\u003e \"servers\"   } ]"
        },
        "staging_directory": {
          "type": "string",
          "description": "directory for temp files created during bulk loader import."
        },
        "tag_on_default_use": {
          "type": "array",
          "default": "[\"_jdbcstaticdefaultsused\"]",
          "description": "Append values to the `tags` field if no record was found and default values were used"
        },
        "tag_on_failure": {
          "type": "array",
          "default": "[\"_jdbcstaticfailure\"]",
          "description": "Append values to the `tags` field if sql error occured Alternatively, individual `tag_on_failure` arrays can be added to each lookup hash"
        }
      }
    },
    "filter/jdbc_streaming": {
      "options": {
        "cache_expiration": {
          "type": "number",
          "default": "5.0",
          "description": "The minimum number of seconds any entry should remain in the cache, defaults to 5 seconds A numeric value, you can use decimals for example `{ \"cache_expiration\" =\u003e 0.25 }` If there are transient jdbc errors the cache will store empty results for a given parameter set and bypass the jbdc lookup, this merges the default_hash into the event, until the cache entry expires, then the jdbc lookup will be tried again for the same parameters Conversely, while the cache contains valid results any external problem that would cause jdbc errors, will not be noticed for the cache_expiration period."
        },
        "cache_size": {
          "type": "number",
          "default": "500",
          "description": "The maximum number of cache entries are stored, defaults to 500 entries The least recently used entry will be evicted"
        },
        "default_hash": {
          "type": "hash",
          "default": "{}",
          "description": "Define a default object to use when lookup fails to return a matching row. ensure that the key names of this object match the columns from the statement"
        },
        "jdbc_connection_string": {
          "type": "string",
          "required": true,
          "description": "JDBC connection string"
        },
        "jdbc_driver_class": {
          "type": "string",
          "required": true,
          "description": "JDBC driver class to load, for example \"oracle.jdbc.OracleDriver\" or \"org.apache.derby.jdbc.ClientDriver\""
        },
        "jdbc_driver_library": {
          "type": "path",
          "description": "JDBC driver library path to third party driver library."
        },
        "jdbc_password": {
          "type": "password",
          "description": "JDBC password"
        },
        "jdbc_user": {
          "type": "string",
          "description": "JDBC user"
        },
        "jdbc_validate_connection": {
          "type": "boolean",
          "default": "false",
          "description": "Connection pool configuration. Validate connection before use."
        },
        "jdbc_validation_timeout": {
          "type": "number",
          "default": "3600",
          "description": "Connection pool configuration. How often to validate a connection (in seconds)"
        },
        "parameters": {
          "type": "hash",
          "default": "{}",
          "description": "Hash of query parameter, for example `{ \"id\" =\u003e \"id_field\" }`"
        },
        "prepared_statement_bind_values": {
          "type": "array",
          "default": "[]"
        },
        "prepared_statement_name": {
          "type": "string"
        },
        "prepared_statement_warn_on_constant_usage": {
          "type": "boolean",
          "default": "true # deprecate in a future major LS release"
        },
        "sequel_opts": {
          "type": "hash",
          "default": "{}",
          "description": "Options hash to pass to Sequel"
        },
        "statement": {
          "type": "string",
          "required": true,
          "description": "Statement to execute. To use parameters, use named parameter syntax, for example \"SELECT * FROM MYTABLE WHERE ID = :id\""
        },
        "tag_on_default_use": {
          "type": "array",
          "default": "[\"_jdbcstreamingdefaultsused\"]",
          "description": "Append values to the `tags` field if no record was found and default values were used"
        },
        "tag_on_failure": {
          "type": "array",
          "default": "[\"_jdbcstreamingfailure\"]",
          "description": "Append values to the `tags` field if sql error occured"
        },
        "target": {
          "type": "string",
          "required": true,
          "description": "Define the target field to store the extracted result(s) Field is overwritten if exists"
        },
        "use_cache": {
          "type": "boolean",
          "default": "true",
          "description": "Enable or disable caching, boolean true or false, defaults to true"
        },
        "use_prepared_statements": {
          "type": "boolean",
          "default": "false"
        }
      }
    },
    "filter/json": {
      "description": "This is a JSON parsing filter. It takes an existing field which contains JSON and expands it into an actual data structure within the Logstash event.",
      "options": {
        "skip_on_invalid_json": {
          "type": "boolean",
          "default": "false",
          "description": "Allow to skip filter on invalid json (allows to handle json and non-json data without warnings)"
        },
        "source": {
          "type": "string",
          "required": true,
          "description": "The configuration for the JSON filter:"
        },
        "tag_on_failure": {
          "type": "array",
          "default": "[\"_jsonparsefailure\"]",
          "description": "Append values to the `tags` field when there has been no successful match"
        },
        "target": {
          "type": "field_reference",
          "description": "Define the target field for placing the parsed data. If this setting is omitted, the JSON data will be stored at the root (top level) of the event."
        }
      }
    },
    "filter/kv": {
      "description": "This filter helps automatically parse messages (or specific event fields) which are of the `foo=bar` variety.",
      "options": {
        "allow_duplicate_values": {
          "type": "boolean",
          "default": "true",
          "description": "A bool option for removing duplicate key/value pairs. When set to false, only one unique key/value pair will be preserved."
        },
        "allow_empty_values": {
          "type": "boolean",
          "default": "false",
          "description": "A bool option for keeping empty or nil values."
        },
        "default_keys": {
          "type": "hash",
          "default": "{}",
          "description": "A hash specifying the default keys and their values which should be added to the event in case these keys do not exist in the source field being parsed."
        },
        "exclude_keys": {
          "type": "array",
          "default": "[]",
          "description": "An array specifying the parsed keys which should not be added to the event. By default no keys will be excluded."
        },
        "field_split": {
          "type": "string",
          "default": " ",
          "description": "A string of characters to use as single-character field delimiters for parsing out key-value pairs."
        },
        "field_split_pattern": {
          "type": "string",
          "description": "A regex expression to use as field delimiter for parsing out key-value pairs. Useful to define multi-character field delimiters. Setting the field_split_pattern options will take precedence over the field_split option."
        },
        "include_brackets": {
          "type": "boolean",
          "default": "true",
          "description": "A boolean specifying whether to treat square brackets, angle brackets, and parentheses as value \"wrappers\" that should be removed from the value."
        },
        "include_keys": {
          "type": "array",
          "default": "[]",
          "description": "An array specifying the parsed keys which should be added to the event. By default all keys will be added."
        },
        "prefix": {
          "type": "string",
          "description": "A string to prepend to all of the extracted keys."
        },
        "recursive": {
          "type": "boolean",
          "default": "false",
          "description": "A boolean specifying whether to drill down into values and recursively get more key-value pairs from it. The extra key-value pairs will be stored as subkeys of the root key."
        },
        "remove_char_key": {
          "type": "string",
          "description": "A string of characters to remove from the key."
        },
        "remove_char_value": {
          "type": "string",
          "description": "A string of characters to remove from the value."
        },
        "source": {
          "type": "field_reference",
          "default": "message",
          "description": "The field to perform `key=value` searching on"
        },
        "tag_on_failure": {
          "type": "array",
          "default": "['_kv_filter_error']",
          "description": "Tag to apply if kv errors"
        },
        "tag_on_timeout": {
          "type": "string",
          "default": "_kv_filter_timeout",
          "description": "Tag to apply if a kv regexp times out."
        },
        "target": {
          "type": "field_reference",
          "description": "The name of the container to put all of the key-value pairs into."
        },
        "timeout_millis": {
          "type": "number",
          "default": "30_000",
          "description": "Attempt to terminate regexps after this amount of time. This applies per source field value if event has multiple values in the source field. Set to 0 to disable timeouts"
        },
        "transform_key": {
          "type": "string, one of: TRANSFORM_LOWERCASE_KEY, TRANSFORM_UPPERCASE_KEY, TRANSFORM_CAPITALIZE_KEY",
          "description": "Transform keys to lower case, upper case or capitals."
        },
        "transform_value": {
          "type": "string, one of: TRANSFORM_LOWERCASE_KEY, TRANSFORM_UPPERCASE_KEY, TRANSFORM_CAPITALIZE_KEY",
          "description": "Transform values to lower case, upper case or capitals."
        },
        "trim_key": {
          "type": "string",
          "description": "A string of characters to trim from the key. This is useful if your keys are wrapped in brackets or start with space."
        },
        "trim_value": {
          "type": "string",
          "description": "A string of characters to trim from the value. This is useful if your values are wrapped in brackets or are terminated with commas (like postfix logs)."
        },
        "value_split": {
          "type": "string",
          "default": "=",
          "description": "A non-empty string of characters to use as single-character value delimiters for parsing out key-value pairs."
        },
        "value_split_pattern": {
          "type": "string",
          "description": "A regex expression to use as value delimiter for parsing out key-value pairs. Useful to define multi-character value delimiters. Setting the value_split_pattern options will take precedence over the value_split option."
        },
        "whitespace": {
          "type": "string, one of: strict, lenient",
          "default": "lenient",
          "description": "An option specifying whether to be _lenient_ or _strict_ with the acceptance of unnecessary whitespace surrounding the configured value-split sequence."
        }
      }
    },
    "filter/memcached": {
      "description": "This filter provides facilities to interact with Memcached.",
      "options": {
        "get": {
          "type": "hash",
          "description": "GET data from the given memcached keys to inject into the corresponding event fields.  - memcached keys can reference event fields via sprintf  - event fields can be deep references"
        },
        "hosts": {
          "type": "array",
          "default": "[\"localhost\"]",
          "description": "an array of memcached hosts to connect to valid forms: ipv4:   - 127.0.0.1   - 127.0.0.1:11211 ipv6:   - ::1   - [::1]:11211 fqdn:   - your.fqdn.com   - your.fqdn.com:11211"
        },
        "namespace": {
          "type": "string",
          "description": "if specified and non-empty, all keys will be prepended with this string and a colon (`:`)"
        },
        "set": {
          "type": "hash",
          "description": "SET the given fields from the event to the corresponding keys in memcached  - memcached keys can reference event fields via sprintf  - event fields can be deep references"
        },
        "tag_on_failure": {
          "type": "string",
          "default": "_memcached_failure",
          "description": "Tags the event on failure. This can be used in later analysis."
        },
        "ttl": {
          "type": "number",
          "default": "0",
          "description": "if performing a setting operation to memcached, the time-to-live in seconds. NOTE: in Memcached, a value of 0 (default) means \"never expire\""
        }
      }
    },
    "filter/metrics": {
      "description": "The metrics filter is useful for aggregating metrics.",
      "options": {
        "clear_interval": {
          "type": "number",
          "default": "-1",
          "description": "The clear interval, when all counter are reset."
        },
        "flush_interval": {
          "type": "number",
          "default": "5",
          "description": "The flush interval, when the metrics event is created. Must be a multiple of 5s."
        },
        "ignore_older_than": {
          "type": "number",
          "default": "0",
          "description": "Don't track events that have `@timestamp` older than some number of seconds."
        },
        "meter": {
          "type": "array",
          "default": "[]",
          "description": "syntax: `meter =\u003e [ \"name of metric\", \"name of metric\" ]`"
        },
        "percentiles": {
          "type": "array",
          "default": "[1, 5, 10, 90, 95, 99, 100]",
          "description": "The percentiles that should be measured and emitted for timer values."
        },
        "rates": {
          "type": "array",
          "default": "[1, 5, 15]",
          "description": "The rates that should be measured, in minutes. Possible values are 1, 5, and 15."
        },
        "timer": {
          "type": "hash",
          "default": "{}",
          "description": "syntax: `timer =\u003e [ \"name of metric\", \"%{time_value}\" ]`"
        }
      }
    },
    "filter/mutate": {
      "description": "The mutate filter allows you to perform general mutations on fields. You can rename, replace, and modify fields in your events.",
      "options": {
        "capitalize": {
          "type": "array",
          "description": "Convert a string to its capitalized equivalent."
        },
        "coerce": {
          "type": "hash",
          "description": "Sets a default value when the field exists but the value is null."
        },
        "convert": {
          "type": "hash",
          "description": "Convert a field's value to a different type, like turning a string to an integer. If the field value is an array, all members will be converted. If the field is a hash no action will be taken."
        },
        "copy": {
          "type": "hash",
          "description": "Copy an existing field to another field. Existing target field will be overriden."
        },
        "gsub": {
          "type": "array",
          "description": "Match a regular expression against a field value and replace all matches with another string. Only fields that are strings or arrays of strings are supported. For other kinds of fields no action will be taken."
        },
        "join": {
          "type": "hash",
          "description": "Join an array with a separator character. Does nothing on non-array fields."
        },
        "lowercase": {
          "type": "array",
          "description": "Convert a string to its lowercase equivalent."
        },
        "merge": {
          "type": "hash",
          "description": "Merge two fields of arrays or hashes. String fields will be automatically be converted into an array, so:"
        },
        "rename": {
          "type": "hash",
          "description": "Rename one or more fields."
        },
        "replace": {
          "type": "hash",
          "description": "Replace a field with a new value. The new value can include `%{foo}` strings to help you build a new value from other parts of the event."
        },
        "split": {
          "type": "hash",
          "description": "Split a field to an array using a separator character. Only works on string fields."
        },
        "strip": {
          "type": "array",
          "description": "Strip whitespace from field. NOTE: this only works on leading and trailing whitespace."
        },
        "tag_on_failure": {
          "type": "string",
          "default": "_mutate_error",
          "description": "Tag to apply if the operation errors"
        },
        "update": {
          "type": "hash",
          "description": "Update an existing field with a new value. If the field does not exist, then no action will be taken."
        },
        "uppercase": {
          "type": "array",
          "description": "Convert a string to its uppercase equivalent."
        }
      }
    },
    "filter/prune": {
      "description": "The prune filter is for removing fields from events based on whitelists or blacklist of field names or their values (names and values can also be regular expressions).",
      "options": {
        "blacklist_names": {
          "type": "array",
          "default": "[ \"%\\\\{[^}]+\\\\}\" ]",
          "description": "Exclude fields whose names match specified regexps, by default exclude unresolved `%{field}` strings."
        },
        "blacklist_values": {
          "type": "hash",
          "default": "{}",
          "description": "Exclude specified fields if their values match one of the supplied regular expressions. In case field values are arrays, each array item is matched against the regular expressions and matching array items will be excluded."
        },
        "interpolate": {
          "type": "boolean",
          "default": "false",
          "description": "Trigger whether configuration fields and values should be interpolated for dynamic values (when resolving `%{some_field}`). Probably adds some performance overhead. Defaults to false."
        },
        "whitelist_names": {
          "type": "array",
          "default": "[]",
          "description": "Include only fields only if their names match specified regexps, default to empty list which means include everything."
        },
        "whitelist_values": {
          "type": "hash",
          "default": "{}",
          "description": "Include specified fields only if their values match one of the supplied regular expressions. In case field values are arrays, each array item is matched against the regular expressions and only matching array items will be included."
        }
      }
    },
    "filter/ruby": {
      "description": "Execute ruby code.",
      "options": {
        "code": {
          "type": "string",
          "description": "The code to execute for every event. You will have an `event` variable available that is the event itself. See the \u003c\u003cevent-api,Event API\u003e\u003e for more information."
        },
        "init": {
          "type": "string",
          "description": "Any code to execute at logstash startup-time"
        },
        "path": {
          "type": "path",
          "description": "Path to the script"
        },
        "script_params": {
          "default": "{}",
          "description": "Parameters for this specific script"
        },
        "tag_on_exception": {
          "default": "_rubyexception",
          "description": "Tag to add to events that cause an exception in the script filter"
        },
        "tag_with_exception_message": {
          "default": "false",
          "description": "Flag for add exception message to tag_on_exception"
        }
      }
    },
    "filter/sleep": {
      "description": "Sleep a given amount of time. This will cause logstash to stall for the given amount of time. This is useful for rate limiting, etc.",
      "options": {
        "every": {
          "type": "string",
          "default": "1",
          "description": "Sleep on every N'th. This option is ignored in replay mode."
        },
        "replay": {
          "type": "boolean",
          "default": "false",
          "description": "Enable replay mode."
        },
        "time": {
          "type": "string",
          "description": "The length of time to sleep, in seconds, for every event."
        }
      }
    },
    "filter/split": {
      "description": "The split filter clones an event by splitting one of its fields and placing each value resulting from the split into a clone of the original event. The field being split can either be a string or an array.",
      "options": {
        "field": {
          "type": "string",
          "default": "message",
          "description": "The field which value is split by the terminator. Can be a multiline message or the ID of an array. Nested arrays are referenced like: \"[object_id][array_id]\""
        },
        "target": {
          "type": "string",
          "description": "The field within the new event which the value is split into. If not set, the target field defaults to split field name."
        },
        "terminator": {
          "type": "string",
          "default": "\\n",
          "description": "The string to split on. This is usually a line terminator, but can be any string. If you are splitting a JSON array into multiple events, you can ignore this field."
        }
      }
    },
    "filter/syslog_pri": {
      "description": "Filter plugin for logstash to parse the `PRI` field from the front of a Syslog (RFC3164) message.  If no priority is set, it will default to 13 (per RFC).",
      "options": {
        "facility_labels": {
          "type": "array",
          "default": "[",
          "description": "Labels for facility levels. This comes from RFC3164."
        },
        "severity_labels": {
          "type": "array",
          "default": "[",
          "description": "Labels for severity levels. This comes from RFC3164."
        },
        "syslog_pri_field_name": {
          "type": "string",
          "description": "Name of field which passes in the extracted PRI part of the syslog message default: 'syslog_pri' or '[log][syslog][priority]' with ECS"
        },
        "use_labels": {
          "type": "boolean",
          "default": "true",
          "description": "Add human-readable names after parsing severity and facility from PRI"
        }
      }
    },
    "filter/throttle": {
      "options": {
        "after_count": {
          "type": "number",
          "default": "-1",
          "description": "Events greater than this count will be throttled.  Setting this value to -1, the default, will cause no events to be throttled based on the upper bound."
        },
        "before_count": {
          "type": "number",
          "default": "-1",
          "description": "Events less than this count will be throttled.  Setting this value to -1, the default, will cause no events to be throttled based on the lower bound."
        },
        "key": {
          "type": "string",
          "required": true,
          "description": "The key used to identify events.  Events with the same key are grouped together. Field substitutions are allowed, so you can combine multiple fields."
        },
        "max_age": {
          "type": "number",
          "default": "3600",
          "description": "The maximum age of a timeslot.  Higher values allow better tracking of an asynchronous flow of events, but require more memory.  As a rule of thumb you should set this value to at least twice the period.  Or set this value to period + maximum time offset between unordered events with the same key.  Values below the specified period give unexpected results if unordered events are processed simultaneously."
        },
        "max_counters": {
          "type": "number",
          "default": "100000",
          "description": "The maximum number of counters to store before decreasing the maximum age of a timeslot. Setting this value to -1 will prevent an upper bound with no constraint on the number of counters.  This configuration value should only be used as a memory control mechanism and can cause early counter expiration if the value is reached. It is recommended to leave the default value and ensure that your key is selected such that it limits the number of counters required (i.e. don't use UUID as the key)."
        },
        "period": {
          "type": "string",
          "default": "60",
          "description": "The period in seconds after the first occurrence of an event until a new timeslot is created.  This period is tracked per unique key and per timeslot. Field substitutions are allowed in this value.  This allows you to specify that certain kinds of events throttle for a specific period of time."
        },
        "periodic_flush": {
          "type": "boolean",
          "default": "true",
          "description": "Call the filter flush method at regular interval.  It is used by the memory control mechanism.  Set to false if you like your VM to go (B)OOM."
        }
      }
    },
    "filter/translate": {
      "options": {
        "destination": {
          "type": "string",
          "default": "translation\" (legacy)",
          "deprecated": "Use `target` option instead."
        },
        "dictionary": {
          "type": "hash",
          "default": "{}",
          "description": "The dictionary to use for translation, when specified in the logstash filter configuration item (i.e. do not use the `@dictionary_path` file)."
        },
        "dictionary_path": {
          "type": "path",
          "description": "The full path of the external dictionary file. The format of the table should be a standard YAML, JSON or CSV with filenames ending in `.yaml`, `.yml`, `.json` or `.csv` to be read. Make sure you specify any integer-based keys in quotes. For example, the YAML file (`.yaml` or `.yml` should look something like this:"
        },
        "exact": {
          "type": "boolean",
          "default": "true",
          "description": "If `exact =\u003e false`, and logstash receives the same event, the destination field will be also set to `bar`. However, if logstash receives an event with the `data` field set to `foofing`, the destination field will be set to `barfing`."
        },
        "fallback": {
          "type": "string",
          "description": "In case no translation occurs in the event (no matches), this will add a default translation string, which will always populate `field`, if the match failed."
        },
        "field": {
          "type": "string",
          "description": "due compatibility w `field =\u003e ...` (non ECS mode) we can not mark it as required",
          "deprecated": "Use `source` option instead."
        },
        "iterate_on": {
          "type": "string",
          "description": "When the value that you need to perform enrichment on is a  variable sized array then specify the field name in this setting. This setting introduces two modes, 1) when the value is an array of strings and 2) when the value is an array of objects (as in JSON object). In the first mode, you should have the same field name in both `field` and `iterate_on`, the result will be an array added to the field specified in the `destination` setting. This array will have the looked up value (or the `fallback` value or nil) in same ordinal position as each sought value. In the second mode, specify the field that has the array of objects then specify the field in each object that provides the sought value with `field` and the field to write the looked up value (or the `fallback` value) to with `destination`"
        },
        "override": {
          "type": "boolean",
          "default": "false unless field == target",
          "description": "If the destination (or target) field already exists, this configuration item specifies whether the filter should skip translation (default) or overwrite the target field value with the new translation value."
        },
        "refresh_behaviour": {
          "type": "string, one of: merge, replace",
          "default": "merge",
          "description": "When using a dictionary file, this setting indicates how the update will be executed. Setting this to `merge` leads to entries removed from the dictionary file being kept; `replace` deletes old entries on update."
        },
        "refresh_interval": {
          "type": "number",
          "default": "300",
          "description": "When using a dictionary file, this setting will indicate how frequently (in seconds) logstash will check the dictionary file for updates."
        },
        "regex": {
          "type": "boolean",
          "default": "false",
          "description": "If you'd like to treat dictionary keys as regular expressions, set `regex =\u003e true`. Note: this is activated only when `exact =\u003e true`."
        },
        "source": {
          "type": "field_reference",
          "required": true,
          "description": "The name of the logstash event field containing the value to be compared for a match by the translate filter (e.g. `message`, `host`, `response_code`)."
        },
        "target": {
          "type": "field_reference",
          "description": "The target field you wish to populate with the translation. When ECS Compatibility is enabled, the default is an in-place translation that will replace the value of the source field. When ECS Compatibility is disabled, this option falls through to the deprecated `destination` field."
        },
        "yaml_dictionary_code_point_limit": {
          "type": "number",
          "description": "The max amount of code points in the YAML file in `dictionary_path`. Please be aware that byte limit depends on the encoding. Snakeyaml 1.33 has a default limit 3MB. YAML file over the limit throws exception. JSON and CSV currently do not have such limit. The limit could be too small in some use cases. Setting a bigger number in `yaml_dictionary_code_point_limit` to relax the restriction. The default value is 128MB for code points of size 1 byte"
        }
      }
    },
    "filter/truncate": {
      "description": "Allows you to truncate fields longer than a given length.",
      "options": {
        "fields": {
          "type": "list of string",
          "description": "A list of fieldrefs to truncate if they are too long."
        },
        "length_bytes": {
          "type": "number",
          "required": true,
          "description": "Fields over this length will be truncated to this length."
        }
      }
    },
    "filter/urldecode": {
      "description": "The urldecode filter is for decoding fields that are urlencoded.",
      "options": {
        "all_fields": {
          "type": "boolean",
          "default": "false",
          "description": "Urldecode all fields"
        },
        "charset": {
          "default": "UTF-8",
          "description": "Thel character encoding used in this filter. Examples include `UTF-8` and `cp1252`"
        },
        "field": {
          "type": "string",
          "default": "message",
          "description": "The field which value is urldecoded"
        },
        "tag_on_failure": {
          "type": "array",
          "default": "[\"_urldecodefailure\"]",
          "description": "Append values to the `tags` field when an exception is thrown"
        }
      }
    },
    "filter/useragent": {
      "description": "Parse user agent strings into structured data based on BrowserScope data",
      "options": {
        "lru_cache_size": {
          "type": "number",
          "default": "100_000",
          "description": "UA parsing is surprisingly expensive. This filter uses an LRU cache to take advantage of the fact that user agents are often found adjacent to one another in log files and rarely have a random distribution. The higher you set this the more likely an item is to be in the cache and the faster this filter will run. However, if you set this too high you can use more memory than desired."
        },
        "prefix": {
          "type": "string",
          "default": "' # not supported in ECS mode",
          "description": "A string to prepend to all of the extracted keys"
        },
        "regexes": {
          "type": "string",
          "description": "`regexes.yaml` file to use"
        },
        "source": {
          "type": "string",
          "required": true,
          "description": "The field containing the user agent string. If this field is an array, only the first value will be used."
        },
        "target": {
          "type": "string",
          "description": "The name of the field to assign user agent data into."
        }
      }
    },
    "filter/uuid": {
      "description": "The uuid filter allows you to generate a UUID and add it as a field to each processed event.",
      "options": {
        "overwrite": {
          "type": "boolean",
          "default": "false",
          "description": "If the value in the field currently (if any) should be overridden by the generated UUID. Defaults to `false` (i.e. if the field is present, with ANY value, it won't be overridden)"
        },
        "target": {
          "type": "string",
          "required": true,
          "description": "Select the name of the field where the generated UUID should be stored."
        }
      }
    },
    "filter/xml": {
      "description": "XML filter. Takes a field that contains XML and expands it into an actual datastructure.",
      "options": {
        "force_array": {
          "type": "boolean",
          "default": "true",
          "description": "By default the filter will force single elements to be arrays. Setting this to false will prevent storing single elements in arrays."
        },
        "force_content": {
          "type": "boolean",
          "default": "false",
          "description": "By default the filter will expand attributes differently from content inside of tags. This option allows you to force text content and attributes to always parse to a hash value."
        },
        "namespaces": {
          "type": "hash",
          "default": "{}",
          "description": "By default only namespaces declarations on the root element are considered. This allows to configure all namespace declarations to parse the XML document."
        },
        "parse_options": {
          "type": "string",
          "description": "Supported XML parsing options are 'strict', 'no_error' and 'no_warning'. - strict mode turns on strict parsing rules (non-compliant xml will fail) - no_error and no_warning can be used to suppress errors/warnings"
        },
        "remove_namespaces": {
          "type": "boolean",
          "default": "false",
          "description": "Remove all namespaces from all nodes in the document. Of course, if the document had nodes with the same names but different namespaces, they will now be ambiguous."
        },
        "source": {
          "type": "string",
          "required": true,
          "description": "Config for xml to hash is:"
        },
        "store_xml": {
          "type": "boolean",
          "default": "true",
          "description": "By default the filter will store the whole parsed XML in the destination field as described above. Setting this to false will prevent that."
        },
        "suppress_empty": {
          "type": "boolean",
          "default": "true",
          "description": "By default, output nothing if the element is empty. If set to `false`, empty element will result in an empty hash object."
        },
        "target": {
          "type": "string",
          "description": "Define target for placing the data"
        },
        "xpath": {
          "type": "hash",
          "default": "{}",
          "description": "xpath will additionally select string values (non-strings will be converted to strings with Ruby's `to_s` function) from parsed XML (using each source field defined using the method above) and place those values in the destination fields. Configuration:"
        }
      }
    },
    "input/azure_event_hubs": {
      "options": {
        "checkpoint_interval": {
          "type": "number",
          "default": "5",
          "description": "The interval in seconds between writing checkpoint while processing a batch. Default 5 seconds. Checkpoints can slow down processing, but are needed to know where to start after a restart. Note - checkpoints happen after every batch, so this configuration is only applicable while processing a single batch. Value is expressed in seconds, set to zero to disable basic Example: azure_event_hubs {    config_mode =\u003e \"basic\"    event_hub_connections =\u003e [\"Endpoint=sb://example1...;EntityPath=event_hub_name1\"]    checkpoint_interval =\u003e 5 } advanced example: azure_event_hubs {   config_mode =\u003e \"advanced\"   event_hubs =\u003e [       { \"event_hub_name1\" =\u003e {           event_hub_connection =\u003e \"Endpoint=sb://example1...\"           checkpoint_interval =\u003e 5       }}    ] }"
        },
        "config_mode": {
          "type": "string, one of: basic, advanced",
          "default": "basic",
          "description": "This plugin supports two styles of configuration basic - You supply a list of Event Hub connection strings complete with the 'EntityPath' that defines the Event Hub name. All other configuration is shared. advanced - You supply a list of Event Hub names, and under each name provide that Event Hub's configuration. Most all of the configuration options are identical as the basic model, except they are configured per Event Hub. Defaults to basic Example: azure_event_hubs {    config_mode =\u003e \"basic\"    event_hub_connections =\u003e [\"Endpoint=sb://example1...;EntityPath=event_hub_name1\"  , \"Endpoint=sb://example2...;EntityPath=event_hub_name2\"  ] }"
        },
        "consumer_group": {
          "type": "string",
          "default": "$Default",
          "description": "Consumer group used to read the Event Hub(s). It is recommended to change from the $Default to a consumer group specifically for Logstash, and ensure that all instances of Logstash use that consumer group. basic Example: azure_event_hubs {    config_mode =\u003e \"basic\"    event_hub_connections =\u003e [\"Endpoint=sb://example1...;EntityPath=event_hub_name1\"]    consumer_group =\u003e \"logstash\" } advanced example: azure_event_hubs {   config_mode =\u003e \"advanced\"   event_hubs =\u003e [       { \"event_hub_name1\" =\u003e {           event_hub_connection =\u003e \"Endpoint=sb://example1...\"           consumer_group =\u003e \"logstash\"       }}    ] }"
        },
        "decorate_events": {
          "type": "boolean",
          "default": "false",
          "description": "Adds meta data to the event. [@metadata][azure_event_hubs][name] - the name of hte event host [@metadata][azure_event_hubs][consumer_group] - the consumer group that consumed this event [@metadata][azure_event_hubs][processor_host] - the unique identifier that identifies which host processed this event. Note - there can be multiple processor hosts on a single instance of Logstash. [@metadata][azure_event_hubs][partition] - the partition from which event came from [@metadata][azure_event_hubs][offset] - the event hub offset for this event [@metadata][azure_event_hubs][sequence] - the event hub sequence for this event [@metadata][azure_event_hubs][timestamp] - the enqueued time of the event [@metadata][azure_event_hubs][event_size] - the size of the event basic Example: azure_event_hubs {    config_mode =\u003e \"basic\"    event_hub_connections =\u003e [\"Endpoint=sb://example1...;EntityPath=event_hub_name1\"]    decorate_events =\u003e true } advanced example: azure_event_hubs {   config_mode =\u003e \"advanced\"   event_hubs =\u003e [       { \"event_hub_name1\" =\u003e {           event_hub_connection =\u003e \"Endpoint=sb://example1...\"           decorate_events =\u003e true       }}    ] }"
        },
        "event_hub_connections": {
          "type": "array",
          "required": true,
          "description": "basic MODE ONLY - The Event Hubs to read from. This is a list of Event Hub connection strings that includes the 'EntityPath'. All other configuration options will be shared between Event Hubs. Example: azure_event_hubs {    config_mode =\u003e \"basic\"    event_hub_connections =\u003e [\"Endpoint=sb://example1...;EntityPath=event_hub_name1\"  , \"Endpoint=sb://example2...;EntityPath=event_hub_name2\"  ] }"
        },
        "event_hubs": {
          "type": "array",
          "required": true,
          "description": "advanced MODE ONLY - The event hubs to read from. This is a array of hashes, where the each entry of the array is a hash of the event_hub_name =\u003e {configuration}. Note - most basic configuration options are supported under the Event Hub names, and examples proved where applicable Note - while in advanced mode, if any basic options are defined at the top level they will be used if not already defined under the Event Hub name.  e.g. you may define shared configuration at the top level Note - the required event_hub_connection is named 'event_hub_connection' (singular) which differs from the basic configuration option 'event_hub_connections' (plural) Note - the 'event_hub_connection' may contain the 'EntityPath', but only if it matches the Event Hub name. Note - the same Event Hub name is allowed under different configurations (and is why the config is array of Hashes) Example: azure_event_hubs {   config_mode =\u003e \"advanced\"   event_hubs =\u003e [       { \"event_hub_name1\" =\u003e {           event_hub_connection =\u003e \"Endpoint=sb://example1...\"       }},       { \"event_hub_name2\" =\u003e {           event_hub_connection =\u003e \"Endpoint=sb://example2...\"           storage_connection =\u003e \"DefaultEndpointsProtocol=https;AccountName=example....\"           storage_container =\u003e \"my_container\"      }}    ]    consumer_group =\u003e \"logstash\" # shared across all Event Hubs }"
        },
        "initial_position": {
          "type": "string, one of: beginning, end, look_back",
          "default": "beginning",
          "description": "When first reading from an event hub, start from this position. beginning - reads ALL pre-existing events in the event hub end - reads NO pre-existing events in the event hub look_back - reads end minus N seconds worth of pre-existing events Note - If the storage_connection is set, this configuration is only applicable for the very first time Logstash reads from the event hub. basic Example: azure_event_hubs {    config_mode =\u003e \"basic\"    event_hub_connections =\u003e [\"Endpoint=sb://example1...;EntityPath=event_hub_name1\"]    initial_position =\u003e \"beginning\" } advanced example: azure_event_hubs {   config_mode =\u003e \"advanced\"   event_hubs =\u003e [       { \"event_hub_name1\" =\u003e {           event_hub_connection =\u003e \"Endpoint=sb://example1...\"           initial_position =\u003e \"beginning\"       }}    ] }"
        },
        "initial_position_look_back": {
          "type": "number",
          "default": "86400",
          "description": "The number of seconds to look back for pre-existing events to determine the initial position. Note - If the storage_connection is set, this configuration is only applicable for the very first time Logstash reads from the event hub. Note - this options is only used when initial_position =\u003e \"look_back\" Value is expressed in seconds, default is 1 day basic Example: azure_event_hubs {    config_mode =\u003e \"basic\"    event_hub_connections =\u003e [\"Endpoint=sb://example1...;EntityPath=event_hub_name1\"]    initial_position =\u003e \"look_back\"    initial_position_look_back =\u003e 86400 } advanced example: azure_event_hubs {   config_mode =\u003e \"advanced\"   event_hubs =\u003e [       { \"event_hub_name1\" =\u003e {           event_hub_connection =\u003e \"Endpoint=sb://example1...\"           initial_position =\u003e \"look_back\"           initial_position_look_back =\u003e 86400       }}    ] }"
        },
        "max_batch_size": {
          "type": "number",
          "default": "125",
          "description": "The max size of events are processed together. A checkpoint is created after each batch. Increasing this value may help with performance, but requires more memory. Defaults to 50 basic Example: azure_event_hubs {    config_mode =\u003e \"basic\"    event_hub_connections =\u003e [\"Endpoint=sb://example1...;EntityPath=event_hub_name1\"]    max_batch_size =\u003e 125 } advanced example: azure_event_hubs {   config_mode =\u003e \"advanced\"   event_hubs =\u003e [       { \"event_hub_name1\" =\u003e {           event_hub_connection =\u003e \"Endpoint=sb://example1...\"           max_batch_size =\u003e 125       }}    ] }"
        },
        "prefetch_count": {
          "type": "number",
          "default": "300",
          "description": "The max size of events that are retrieved prior to processing. Increasing this value may help with performance, but requires more memory. Defaults to 300 basic Example: azure_event_hubs {    config_mode =\u003e \"basic\"    event_hub_connections =\u003e [\"Endpoint=sb://example1...;EntityPath=event_hub_name1\"]    prefetch_count =\u003e 300 } advanced example: azure_event_hubs {   config_mode =\u003e \"advanced\"   event_hubs =\u003e [       { \"event_hub_name1\" =\u003e {           event_hub_connection =\u003e \"Endpoint=sb://example1...\"           prefetch_count =\u003e 300       }}    ] } NOTE - This option is intentionally not part of the public documentation. This is a very low level configuration that shouldn't need to be changed by anyone other then an Event Hub expert."
        },
        "receive_timeout": {
          "type": "number",
          "default": "60",
          "description": "The max time allowed receive events without a timeout. Value is expressed in seconds, default 60 basic Example: azure_event_hubs {    config_mode =\u003e \"basic\"    event_hub_connections =\u003e [\"Endpoint=sb://example1...;EntityPath=event_hub_name1\"]    receive_timeout =\u003e 60 } advanced example: azure_event_hubs {   config_mode =\u003e \"advanced\"   event_hubs =\u003e [       { \"event_hub_name1\" =\u003e {           event_hub_connection =\u003e \"Endpoint=sb://example1...\"           receive_timeout =\u003e 300       }}    ] } NOTE - This option is intentionally not part of the public documentation. This is a very low level configuration that shouldn't need to be changed by anyone other then an Event Hub expert."
        },
        "storage_connection": {
          "type": "password",
          "description": "Used to persists the offsets between restarts and ensure that multiple instances of Logstash process different partitions This is *stongly* encouraged to be set for production environments. When this value is set, restarts will pick up from where it left off. Without this value set the initial_position is *always* used. basic Example: azure_event_hubs {    config_mode =\u003e \"basic\"    event_hub_connections =\u003e [\"Endpoint=sb://example1...;EntityPath=event_hub_name1\"]    storage_connection =\u003e \"DefaultEndpointsProtocol=https;AccountName=example....\" } advanced example: azure_event_hubs {   config_mode =\u003e \"advanced\"   event_hubs =\u003e [       { \"event_hub_name1\" =\u003e {           event_hub_connection =\u003e \"Endpoint=sb://example1...\"           storage_connection =\u003e \"DefaultEndpointsProtocol=https;AccountName=example....\"       }}    ] }"
        },
        "storage_container": {
          "type": "string",
          "description": "The storage container to persist the offsets. Note - don't allow multiple Event Hubs to write to the same container with the same consumer group, else the offsets will be persisted incorrectly. Note - this will default to the event hub name if not defined basic Example: azure_event_hubs {    config_mode =\u003e \"basic\"    event_hub_connections =\u003e [\"Endpoint=sb://example1...;EntityPath=event_hub_name1\"]    storage_connection =\u003e \"DefaultEndpointsProtocol=https;AccountName=example....\"    storage_container =\u003e \"my_container\" } advanced example: azure_event_hubs {   config_mode =\u003e \"advanced\"   event_hubs =\u003e [       { \"event_hub_name1\" =\u003e {           event_hub_connection =\u003e \"Endpoint=sb://example1...\"           storage_connection =\u003e \"DefaultEndpointsProtocol=https;AccountName=example....\"           storage_container =\u003e \"my_container\"       }}    ] }"
        },
        "threads": {
          "type": "number",
          "default": "16",
          "description": "Total threads used process events. Requires at minimum 2 threads. This option can not be set per Event Hub. azure_event_hubs {    threads =\u003e 16 }"
        }
      }
    },
    "input/beats": {
      "description": "This input plugin enables Logstash to receive events from the Elastic Beats framework.",
      "options": {
        "add_hostname": {
          "type": "boolean",
          "default": "false",
          "description": "Flag to determine whether to add host information (provided by the beat in the 'hostname' field) to the event",
          "deprecated": "This option will be removed in the future as beats determine the event schema"
        },
        "cipher_suites": {
          "type": "array",
          "default": "[]",
          "description": "The list of ciphers suite to use, listed by priorities.",
          "deprecated": "Set "
        },
        "client_inactivity_timeout": {
          "type": "number",
          "default": "60",
          "description": "Close Idle clients after X seconds of inactivity."
        },
        "enrich": {},
        "event_loop_threads": {
          "type": "number",
          "default": "0",
          "description": "Expert only setting which set's Netty Event Loop Group thread count defaults to zero where Netty's DEFAULT_EVENT_LOOP_THREADS (NettyRuntime.availableProcessors() * 2) will be applied"
        },
        "executor_threads": {
          "type": "number",
          "description": "Beats handler executor thread"
        },
        "host": {
          "type": "string",
          "default": "0.0.0.0",
          "description": "The IP address to listen on."
        },
        "include_codec_tag": {
          "type": "boolean",
          "default": "true",
          "deprecated": "use `enrich` option to configure which enrichments to perform"
        },
        "port": {
          "type": "number",
          "required": true,
          "description": "The port to listen on."
        },
        "ssl": {
          "type": "boolean",
          "default": "false",
          "description": "Events are by default sent in plain text. You can enable encryption by setting `ssl` to true and configuring the `ssl_certificate` and `ssl_key` options.",
          "deprecated": "Use "
        },
        "ssl_certificate": {
          "type": "path",
          "description": "SSL certificate to use."
        },
        "ssl_certificate_authorities": {
          "type": "array",
          "default": "[]",
          "description": "Validate client certificates against these authorities. You can define multiple files or paths. All the certificates will be read and added to the trust store. You need to configure the `ssl_verify_mode` to `peer` or `force_peer` to enable the verification."
        },
        "ssl_cipher_suites": {
          "default": "SslContextBuilder.getDefaultCiphers"
        },
        "ssl_client_authentication": {
          "type": "string, one of: none, optional, required",
          "default": "none",
          "description": "Controls the servers behavior in regard to requesting a certificate from client connections. `none`: No client authentication `optional`: Requests a client certificate but the client is not required to present one. `required`: Forces a client to present a certificate."
        },
        "ssl_enabled": {
          "type": "boolean",
          "default": "false",
          "description": "Events are by default sent in plain text. You can enable encryption by setting `ssl_enabled` to true and configuring the `ssl_certificate` and `ssl_key` options."
        },
        "ssl_handshake_timeout": {
          "type": "number",
          "default": "10000",
          "description": "Time in milliseconds for an incomplete ssl handshake to timeout"
        },
        "ssl_key": {
          "type": "path",
          "description": "SSL key to use. NOTE: This key need to be in the PKCS8 format, you can convert it with OpenSSL for more information."
        },
        "ssl_key_passphrase": {
          "type": "password",
          "description": "SSL key passphrase to use."
        },
        "ssl_peer_metadata": {
          "type": "boolean",
          "default": "false",
          "description": "Enables storing client certificate information in event's metadata. You need to configure the `ssl_verify_mode` to `peer` or `force_peer` to enable this.",
          "deprecated": "use `enrich` option to configure which enrichments to perform"
        },
        "ssl_supported_protocols": {
          "type": "list of string, one of: TLSv1.1, TLSv1.2, TLSv1.3",
          "default": "['TLSv1.2', 'TLSv1.3']"
        },
        "ssl_verify_mode": {
          "type": "string, one of: none, peer, force_peer",
          "default": "none",
          "description": "By default the server doesn't do any client verification.",
          "deprecated": "Set "
        },
        "tls_max_version": {
          "type": "number",
          "default": "TLS.max.version",
          "description": "The maximum TLS version allowed for the encrypted connections. The value must be the one of the following: 1.0 for TLS 1.0, 1.1 for TLS 1.1, 1.2 for TLS 1.2",
          "deprecated": "Set "
        },
        "tls_min_version": {
          "type": "number",
          "default": "TLS.min.version",
          "description": "The minimum TLS version allowed for the encrypted connections. The value must be one of the following: 1.0 for TLS 1.0, 1.1 for TLS 1.1, 1.2 for TLS 1.2",
          "deprecated": "Set "
        }
      }
    },
    "input/cloudwatch": {
      "description": "Pull events from the Amazon Web Services CloudWatch API.",
      "options": {
        "combined": {
          "type": "boolean",
          "default": "false",
          "description": "Use this for namespaces that need to combine the dimensions like S3 and SNS."
        },
        "filters": {
          "type": "array",
          "description": "Specify the filters to apply when fetching resources:"
        },
        "interval": {
          "type": "number",
          "default": "(60 * 15)",
          "description": "Set how frequently CloudWatch should be queried"
        },
        "metrics": {
          "type": "array",
          "default": "[ 'CPUUtilization', 'DiskReadOps', 'DiskWriteOps', 'NetworkIn', 'NetworkOut' ]",
          "description": "Specify the metrics to fetch for the namespace. The defaults are AWS/EC2 specific."
        },
        "namespace": {
          "type": "string",
          "default": "AWS/EC2",
          "description": "The service namespace of the metrics to fetch."
        },
        "period": {
          "type": "number",
          "default": "(60 * 5)",
          "description": "Set the granularity of the returned datapoints."
        },
        "statistics": {
          "type": "array",
          "default": "[ 'SampleCount', 'Average', 'Minimum', 'Maximum', 'Sum' ]",
          "description": "Specify the statistics to fetch for each namespace"
        }
      }
    },
    "input/couchdb_changes": {
      "description": "This CouchDB input allows you to automatically stream events from the CouchDB _changes URI. Moreover, any \"future\" changes will automatically be streamed as well making it easy to synchronize your CouchDB data with any target destination",
      "options": {
        "always_reconnect": {
          "type": "boolean",
          "default": "true",
          "description": "Reconnect flag.  When true, always try to reconnect after a failure"
        },
        "ca_file": {
          "type": "path",
          "description": "Path to a CA certificate file, used to validate certificates"
        },
        "db": {
          "type": "string",
          "required": true,
          "description": "The CouchDB db to connect to. Required parameter."
        },
        "heartbeat": {
          "type": "number",
          "default": "1000",
          "description": "Logstash connects to CouchDB's _changes with feed=continuous The heartbeat is how often (in milliseconds) Logstash will ping CouchDB to ensure the connection is maintained.  Changing this setting is not recommended unless you know what you are doing."
        },
        "host": {
          "type": "string",
          "default": "localhost",
          "description": "IP or hostname of your CouchDB instance"
        },
        "ignore_attachments": {
          "type": "boolean",
          "default": "true",
          "description": "Future feature! Until implemented, changing this from the default will not do anything."
        },
        "initial_sequence": {
          "type": "number",
          "description": "If unspecified, Logstash will attempt to read the last sequence number from the `sequence_path` file.  If that is empty or non-existent, it will begin with 0 (the beginning)."
        },
        "keep_id": {
          "type": "boolean",
          "default": "false",
          "description": "Preserve the CouchDB document id \"_id\" value in the output."
        },
        "keep_revision": {
          "type": "boolean",
          "default": "false",
          "description": "Preserve the CouchDB document revision \"_rev\" value in the output."
        },
        "password": {
          "type": "password",
          "default": "nil",
          "description": "Password, if authentication is needed to connect to CouchDB"
        },
        "port": {
          "type": "number",
          "default": "5984",
          "description": "Port of your CouchDB instance."
        },
        "reconnect_delay": {
          "type": "number",
          "default": "10",
          "description": "Reconnect delay: time between reconnect attempts, in seconds."
        },
        "secure": {
          "type": "boolean",
          "default": "false",
          "description": "Connect to CouchDB's _changes feed securely (via https) Default: false (via http)"
        },
        "sequence_path": {
          "type": "string",
          "description": "File path where the last sequence number in the _changes stream is stored. If unset it will write to `$HOME/.couchdb_seq`"
        },
        "timeout": {
          "type": "number",
          "description": "Timeout: Number of milliseconds to wait for new data before terminating the connection.  If a timeout is set it will disable the heartbeat configuration option."
        },
        "username": {
          "type": "string",
          "default": "nil",
          "description": "Username, if authentication is needed to connect to CouchDB"
        }
      }
    },
    "input/dead_letter_queue": {
      "description": "Logstash input to read events from Logstash's dead letter queue",
      "options": {
        "clean_consumed": {
          "type": "boolean",
          "default": "false",
          "description": "If true deletes the DLQ segments that has been processed. Supported only Logstash \u003e= 8.4.0 If this setting is `true` and Logstash version doesn't provides this feature then result in a configuration error. This feature implicitly requires that the `commit_offsets` option is set to `true`. Iif it's not then you'll get a configuration error."
        },
        "commit_offsets": {
          "type": "boolean",
          "default": "true",
          "description": "Should this input commit offsets as it processes the events. `false` value is typically used when you want to iterate multiple times over the events in the dead letter queue, but don't want to save state. This is when you are exploring the events in the dead letter queue."
        },
        "path": {
          "type": "path",
          "required": true,
          "description": "Path to the dead letter queue directory which was created by a Logstash instance. This is the path from where \"dead\" events are read from and is typically configured in the original Logstash instance with the setting path.dead_letter_queue."
        },
        "pipeline_id": {
          "type": "string",
          "default": "main",
          "description": "ID of the pipeline whose events you want to read from."
        },
        "sincedb_path": {
          "type": "string",
          "description": "Path of the sincedb database file (keeps track of the current position of dead letter queue) that will be written to disk. The default will write sincedb files to `\u003cpath.data\u003e/plugins/inputs/dead_letter_queue` NOTE: it must be a file path and not a directory path"
        },
        "start_timestamp": {
          "type": "string",
          "description": "Timestamp in ISO8601 format from when you want to start processing the events from. For example, 2017-04-04T23:40:37"
        }
      }
    },
    "input/elastic_serverless_forwarder": {
      "options": {
        "auth_basic_password": {
          "type": "password"
        },
        "auth_basic_username": {
          "type": "string",
          "description": "optional http basic auth"
        },
        "host": {
          "type": "string",
          "default": "0.0.0.0",
          "description": "bind address"
        },
        "port": {
          "type": "number",
          "required": true
        },
        "ssl": {
          "type": "boolean",
          "default": "true",
          "description": "ssl-config",
          "deprecated": "Use "
        },
        "ssl_certificate": {
          "type": "path",
          "description": "ssl-identity"
        },
        "ssl_certificate_authorities": {
          "type": "list of path",
          "description": "ssl-trust"
        },
        "ssl_cipher_suites": {
          "type": "list of string",
          "description": "ssl-expert-mode"
        },
        "ssl_client_authentication": {
          "type": "string, one of: none, optional, required",
          "default": "none"
        },
        "ssl_enabled": {
          "type": "boolean",
          "default": "true"
        },
        "ssl_handshake_timeout": {
          "type": "number",
          "default": "10_000"
        },
        "ssl_key": {
          "type": "path"
        },
        "ssl_key_passphrase": {
          "type": "password"
        },
        "ssl_supported_protocols": {
          "type": "list of string"
        },
        "ssl_verification_mode": {
          "type": "string, one of: certificate",
          "default": "certificate"
        }
      }
    },
    "input/elasticsearch": {
      "options": {
        "api_key": {
          "type": "password",
          "description": "Authenticate using Elasticsearch API key. format is id:api_key (as returned by Create API key)"
        },
        "ca_file": {
          "type": "path",
          "description": "SSL Certificate Authority file in PEM encoded format, must also include any chain certificates as necessary",
          "deprecated": "Set "
        },
        "cloud_auth": {
          "type": "password",
          "description": "Cloud authentication string (\"\u003cusername\u003e:\u003cpassword\u003e\" format) is an alternative for the `user`/`password` configuration."
        },
        "cloud_id": {
          "type": "string",
          "description": "Cloud ID, from the Elastic Cloud web console. If set `hosts` should not be used."
        },
        "connect_timeout_seconds": {
          "type": "positive_whole_number",
          "default": "10",
          "description": "Connection Timeout, in Seconds"
        },
        "docinfo": {
          "type": "boolean",
          "default": "false",
          "description": "If set, include Elasticsearch document information such as index, type, and the id in the event."
        },
        "docinfo_fields": {
          "type": "array",
          "default": "['_index', '_type', '_id']",
          "description": "List of document metadata to move to the `docinfo_target` field. To learn more about Elasticsearch metadata fields read http://www.elasticsearch.org/guide/en/elasticsearch/guide/current/_document_metadata.html"
        },
        "docinfo_target": {
          "type": "field_reference",
          "description": "Where to move the Elasticsearch document information. default: [@metadata][input][elasticsearch] in ECS mode, @metadata field otherwise"
        },
        "hosts": {
          "type": "array",
          "description": "List of elasticsearch hosts to use for querying. Each host can be either IP, HOST, IP:port or HOST:port. Port defaults to 9200"
        },
        "index": {
          "type": "string",
          "default": "logstash-*",
          "description": "The index or alias to search."
        },
        "password": {
          "type": "password",
          "description": "Basic Auth - password"
        },
        "proxy": {
          "type": "uri_or_empty",
          "description": "Set the address of a forward HTTP proxy."
        },
        "query": {
          "type": "string",
          "default": "{ \"sort\": [ \"_doc\" ] }",
          "description": "The query to be executed. Read the Elasticsearch query DSL documentation for more info https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl.html"
        },
        "request_timeout_seconds": {
          "type": "positive_whole_number",
          "default": "60",
          "description": "Request Timeout, in Seconds"
        },
        "response_type": {
          "type": "string, one of: hits, aggregations",
          "default": "hits",
          "description": "This allows you to speccify the response type: either hits or aggregations where hits: normal search request       aggregations: aggregation request"
        },
        "retries": {
          "type": "number",
          "default": "0",
          "description": "The number of retries to run the query. If the query fails after all retries, it logs an error message."
        },
        "schedule": {
          "type": "string",
          "description": "Schedule of when to periodically run statement, in Cron format for example: \"* * * * *\" (execute query every minute, on the minute)"
        },
        "scroll": {
          "type": "string",
          "default": "1m",
          "description": "This parameter controls the keepalive time in seconds of the scrolling request and initiates the scrolling process. The timeout applies per round trip (i.e. between the previous scroll request, to the next)."
        },
        "search_api": {
          "type": "string, one of: auto, search_after, scroll",
          "default": "auto",
          "description": "Default `auto` will use `search_after` api for Elasticsearch 8 and use `scroll` api for 7 Set to scroll to fallback to previous version"
        },
        "size": {
          "type": "number",
          "default": "1000",
          "description": "This allows you to set the maximum number of hits returned per scroll."
        },
        "slices": {
          "type": "number",
          "description": "This parameter controls the number of parallel slices to be consumed simultaneously by this pipeline input."
        },
        "socket_timeout_seconds": {
          "type": "positive_whole_number",
          "default": "60",
          "description": "Socket Timeout, in Seconds"
        },
        "ssl": {
          "type": "boolean",
          "default": "false",
          "description": "SSL",
          "deprecated": "Set "
        },
        "ssl_certificate": {
          "type": "path",
          "description": "OpenSSL-style X.509 certificate certificate to authenticate the client"
        },
        "ssl_certificate_authorities": {
          "type": "list of path",
          "description": "SSL Certificate Authority files in PEM encoded format, must also include any chain certificates as necessary"
        },
        "ssl_certificate_verification": {
          "type": "boolean",
          "default": "true",
          "description": "Option to validate the server's certificate. Disabling this severely compromises security. For more information on the importance of certificate verification please read https://www.cs.utexas.edu/~shmat/shmat_ccs12.pdf",
          "deprecated": "Set "
        },
        "ssl_cipher_suites": {
          "type": "list of string",
          "description": "The list of cipher suites to use, listed by priorities. Supported cipher suites vary depending on which version of Java is used."
        },
        "ssl_enabled": {
          "type": "boolean",
          "description": "SSL"
        },
        "ssl_key": {
          "type": "path",
          "description": "OpenSSL-style RSA private key to authenticate the client"
        },
        "ssl_keystore_password": {
          "type": "password",
          "description": "Set the keystore password"
        },
        "ssl_keystore_path": {
          "type": "path",
          "description": "The keystore used to present a certificate to the server. It can be either .jks or .p12"
        },
        "ssl_keystore_type": {
          "type": "string, one of: pkcs12, jks",
          "description": "The format of the keystore file. It must be either jks or pkcs12"
        },
        "ssl_supported_protocols": {
          "type": "list of string, one of: TLSv1.1, TLSv1.2, TLSv1.3",
          "default": "[]",
          "description": "Supported protocols with versions."
        },
        "ssl_truststore_password": {
          "type": "password",
          "description": "Set the truststore password"
        },
        "ssl_truststore_path": {
          "type": "path",
          "description": "The JKS truststore to validate the server's certificate. Use either `:ssl_truststore_path` or `:ssl_certificate_authorities`"
        },
        "ssl_truststore_type": {
          "type": "string, one of: pkcs12, jks",
          "description": "The format of the truststore file. It must be either jks or pkcs12"
        },
        "ssl_verification_mode": {
          "type": "string, one of: full, none",
          "default": "full",
          "description": "Options to verify the server's certificate. \"full\": validates that the provided certificate has an issue date thats within the not_before and not_after dates; chains to a trusted Certificate Authority (CA); has a hostname or IP address that matches the names within the certificate. \"none\": performs no certificate validation. Disabling this severely compromises security (https://www.cs.utexas.edu/~shmat/shmat_ccs12.pdf)"
        },
        "target": {
          "type": "field_reference",
          "description": "If set, the _source of each hit will be added nested under the target instead of at the top-level"
        },
        "user": {
          "type": "string",
          "description": "Basic Auth - username"
        }
      }
    },
    "input/exec": {
      "description": "Periodically run a shell command and capture the whole output as an event.",
      "options": {
        "command": {
          "type": "string",
          "required": true,
          "description": "Command to run. For example : `uptime`"
        },
        "interval": {
          "type": "number",
          "description": "Interval to run the command. Value is in seconds. Either `interval` or `schedule` option must be defined."
        },
        "schedule": {
          "type": "string",
          "description": "Schedule of when to periodically run command, in Cron format For example: \"* * * * *\" (execute command every minute, on the minute) Either `interval` or `schedule` option must be defined."
        }
      }
    },
    "input/file": {
      "options": {
        "check_archive_validity": {
          "type": "boolean",
          "default": "false",
          "description": "Before start read a compressed file, checks for its validity. This request a full read of the archive, so potentially could cost time. If not specified to true, and the file is corrupted, could end in cyclic processing of the broken file."
        },
        "close_older": {
          "type": "string, one of: FriendlyDurations, seconds",
          "default": "1 hour",
          "description": "The file input closes any files that were last read the specified timespan in seconds ago. If tailing, and there is a large time gap in incoming data the file can be closed (allowing other files to be opened) but will be queued for reopening when new data is detected. If reading, the file will be closed after closed_older seconds from when the last bytes were read. The default is 1 hour"
        },
        "delimiter": {
          "type": "string",
          "default": "\\n",
          "description": "set the new line delimiter, defaults to \"\\n\""
        },
        "discover_interval": {
          "type": "number",
          "default": "15",
          "description": "How often (in seconds) we expand the filename patterns in the `path` option to discover new files to watch."
        },
        "exclude": {
          "type": "array",
          "description": "Exclusions (matched against the filename, not full path). Filename patterns are valid here, too. For example, if you have"
        },
        "exit_after_read": {
          "type": "boolean",
          "default": "false",
          "description": "When in 'read' mode - this option is closing all file watchers when EOF is hit This option also disables discovery of new/changes files. It works only on files found at the beginning Sincedb still works, if you run LS once again after doing some changes - only new values will be read"
        },
        "file_chunk_count": {
          "type": "number",
          "description": "When combined with the `file_chunk_size`, this option sets how many chunks are read from each file before moving to the next active file. e.g. a `chunk_count` of 32 with the default `file_chunk_size` will process 1MB from each active file. See the option `max_open_files` for more info. The default set internally is very large, 4611686018427387903. By default the file is read to the end before moving to the next active file."
        },
        "file_chunk_size": {
          "type": "number",
          "description": "File content is read off disk in blocks or chunks, then using whatever the set delimiter is, lines are extracted from the chunk. Specify the size in bytes of each chunk. See `file_chunk_count` to see why and when to change this from the default. The default set internally is 32768 (32KB)"
        },
        "file_completed_action": {
          "type": "string, one of: delete, log, log_and_delete",
          "default": "delete",
          "description": "When in 'read' mode, what action should be carried out when a file is done with. If 'delete' is specified then the file will be deleted. If 'log' is specified then the full path of the file is logged to the file specified in the `file_completed_log_path` setting."
        },
        "file_completed_log_path": {
          "type": "string",
          "description": "Which file should the completely read file paths be appended to. Only specify this path to a file when `file_completed_action` is 'log' or 'log_and_delete'. IMPORTANT: this file is appended to only - it could become very large. You are responsible for file rotation."
        },
        "file_sort_by": {
          "type": "string, one of: last_modified, path",
          "default": "last_modified",
          "description": "Which attribute of a discovered file should be used to sort the discovered files. Files can be sort by modified date or full path alphabetic. The default is `last_modified` Previously the processing order of the discovered files was OS dependent."
        },
        "file_sort_direction": {
          "type": "string, one of: asc, desc",
          "default": "asc",
          "description": "Choose between ascending and descending order when also choosing between `last_modified` and `path` file_sort_by options. If ingesting the newest data first is important then opt for last_modified + desc If ingesting the oldest data first is important then opt for last_modified + asc If you use a special naming convention for the file full paths then perhaps path + asc will help to achieve the goal of controlling the order of file ingestion"
        },
        "ignore_older": {
          "type": "string, one of: FriendlyDurations, seconds",
          "description": "When the file input discovers a file that was last modified before the specified timespan in seconds, the file is ignored. After its discovery, if an ignored file is modified it is no longer ignored and any new data is read. By default, this option is disabled. Note this unit is in seconds."
        },
        "max_open_files": {
          "type": "number",
          "description": "What is the maximum number of file_handles that this input consumes at any one time. Use close_older to close some files if you need to process more files than this number. This should not be set to the maximum the OS can do because file handles are needed for other LS plugins and OS processes. The default of 4095 is set in filewatch."
        },
        "mode": {
          "type": "string, one of: tail, read",
          "default": "tail",
          "description": "What mode do you want the file input to operate in. Tail a few files or read many content-complete files The default is tail If \"read\" is specified then the following other settings are ignored   `start_position` (files are always read from the beginning)   `delimiter` (files are assumed to use \\n or \\r (or both) as line endings)   `close_older` (files are automatically 'closed' when EOF is reached) If \"read\" is specified then the following settings are heeded   `ignore_older` (older files are not processed) \"read\" mode now supports gzip file processing"
        },
        "path": {
          "type": "array",
          "required": true,
          "description": "The path(s) to the file(s) to use as an input. You can use filename patterns here, such as `/var/log/*.log`. If you use a pattern like `/var/log/**/*.log`, a recursive search of `/var/log` will be done for all `*.log` files. Paths must be absolute and cannot be relative."
        },
        "sincedb_clean_after": {
          "type": "string, one of: FriendlyDurations, days",
          "default": "14 days\" # days",
          "description": "The sincedb entry now has a last active timestamp associated with it. If no changes are detected in tracked files in the last N days their sincedb tracking record will expire and not be persisted. This option protects against the well known inode recycling problem. (add reference)"
        },
        "sincedb_path": {
          "type": "string",
          "description": "Path of the sincedb database file (keeps track of the current position of monitored log files) that will be written to disk. The default will write sincedb files to `\u003cpath.data\u003e/plugins/inputs/file` NOTE: it must be a file path and not a directory path"
        },
        "sincedb_write_interval": {
          "type": "string, one of: FriendlyDurations, seconds",
          "default": "15",
          "description": "How often (in seconds) to write a since database with the current position of monitored log files."
        },
        "start_position": {
          "type": "string, one of: beginning, end",
          "default": "end",
          "description": "Choose where Logstash starts initially reading files: at the beginning or at the end. The default behavior treats files like live streams and thus starts at the end. If you have old data you want to import, set this to 'beginning'."
        },
        "stat_interval": {
          "type": "string, one of: FriendlyDurations, seconds",
          "default": "1",
          "description": "How often (in seconds) we stat files to see if they have been modified. Increasing this interval will decrease the number of system calls we make, but increase the time to detect new log lines."
        }
      }
    },
    "input/ganglia": {
      "description": "Read ganglia packets from the network via udp",
      "options": {
        "host": {
          "type": "string",
          "default": "0.0.0.0",
          "description": "The address to listen on"
        },
        "port": {
          "type": "number",
          "default": "8649",
          "description": "The port to listen on. Remember that ports less than 1024 (privileged ports) may require root to use."
        }
      }
    },
    "input/gelf": {
      "description": "This input will read GELF messages as events over the network, making it a good choice if you already use Graylog2 today.",
      "options": {
        "host": {
          "type": "string",
          "default": "0.0.0.0",
          "description": "The IP address or hostname to listen on."
        },
        "port": {
          "type": "number",
          "default": "12201",
          "description": "The ports to listen on. Remember that ports less than 1024 (privileged ports) may require root to use. port_tcp and port_udp can be used to have a different port for udp than the tcp port."
        },
        "port_tcp": {
          "type": "number"
        },
        "port_udp": {
          "type": "number"
        },
        "remap": {
          "type": "boolean",
          "default": "true",
          "description": "Whether or not to remap the GELF message fields to Logstash event fields or leave them intact."
        },
        "strip_leading_underscore": {
          "type": "boolean",
          "default": "true",
          "description": "Whether or not to remove the leading `\\_` in GELF fields or leave them in place. (Logstash \u003c 1.2 did not remove them by default.). Note that GELF version 1.1 format now requires all non-standard fields to be added as an \"additional\" field, beginning with an underscore."
        },
        "use_tcp": {
          "type": "boolean",
          "default": "false",
          "description": "Whether or not to use TCP or/and UDP"
        },
        "use_udp": {
          "type": "boolean",
          "default": "true"
        }
      }
    },
    "input/generator": {
      "description": "Generate random log events.",
      "options": {
        "count": {
          "type": "number",
          "default": "0",
          "description": "Set how many messages should be generated."
        },
        "lines": {
          "type": "array",
          "description": "The lines to emit, in order. This option cannot be used with the 'message' setting."
        },
        "message": {
          "type": "string",
          "default": "Hello world!",
          "description": "The message string to use in the event."
        }
      }
    },
    "input/graphite": {
      "description": "Receive graphite metrics. This plugin understands the text-based graphite carbon protocol. Both `N` and `specific-timestamp` forms are supported, example:"
    },
    "input/heartbeat": {
      "description": "Generate heartbeat messages.",
      "options": {
        "count": {
          "type": "number",
          "default": "-1",
          "description": "How many times to iterate. This is typically used only for testing purposes."
        },
        "interval": {
          "type": "number",
          "default": "60",
          "description": "Set how frequently messages should be sent."
        },
        "message": {
          "type": "string",
          "default": "ok",
          "description": "The message string to use in the event."
        },
        "sequence": {
          "type": "string, one of: none, epoch, sequence",
          "description": "Select the type of sequence, deprecating the 'epoch' and 'sequence' values in 'message'."
        }
      }
    },
    "input/http": {
      "description": "Using this input you can receive single or multiline events over http(s). Applications can send a HTTP POST request with a body to the endpoint started by this input and Logstash will convert it into an event for subsequent processing. Users can pass plain text, JSON, or any formatted data and use a corresponding codec with this input. For Content-Type `application/json` the `json` codec is used, but for all other data formats, `plain` codec is used.",
      "options": {
        "additional_codecs": {
          "type": "hash",
          "default": "{ \"application/json\" =\u003e \"json\" }",
          "description": "Apply specific codecs for specific content types. The default codec will be applied only after this list is checked and no codec for the request's content-type is found"
        },
        "cipher_suites": {
          "type": "array",
          "default": "[]",
          "deprecated": "Set "
        },
        "host": {
          "type": "string",
          "default": "0.0.0.0",
          "description": "The host or ip to bind"
        },
        "keystore": {
          "type": "path",
          "description": "The JKS keystore to validate the client's certificates",
          "deprecated": "Set "
        },
        "keystore_password": {
          "type": "password",
          "description": "The JKS keystore password",
          "deprecated": "Set "
        },
        "max_content_length": {
          "type": "number",
          "default": "100 * 1024 * 1024"
        },
        "max_pending_requests": {
          "type": "number",
          "default": "200"
        },
        "password": {
          "type": "password",
          "description": "Password for basic authorization"
        },
        "port": {
          "type": "number",
          "default": "8080",
          "description": "The TCP port to bind to"
        },
        "remote_host_target_field": {
          "type": "string",
          "description": "target field for the client host of the http request"
        },
        "request_headers_target_field": {
          "type": "string",
          "description": "target field for the client host of the http request"
        },
        "response_code": {
          "type": "string, one of: 200, 201, 202, 204",
          "default": "200"
        },
        "response_headers": {
          "type": "hash",
          "default": "{ 'Content-Type' =\u003e 'text/plain' }",
          "description": "specify a custom set of response headers"
        },
        "ssl": {
          "type": "boolean",
          "default": "false",
          "description": "Events are by default sent in plain text. You can enable encryption by setting `ssl` to true and configuring the `ssl_certificate` and `ssl_key` options.",
          "deprecated": "Set "
        },
        "ssl_certificate": {
          "type": "path",
          "description": "SSL certificate to use."
        },
        "ssl_certificate_authorities": {
          "type": "array",
          "default": "[]",
          "description": "Validate client certificates against these authorities. You can define multiple files or paths. All the certificates will be read and added to the trust store. You need to configure the `ssl_client_authentication` to `optional` or `required` to enable the verification."
        },
        "ssl_cipher_suites": {
          "default": "SslSimpleBuilder.getDefaultCiphers",
          "description": "The list of ciphers suite to use, listed by priorities."
        },
        "ssl_client_authentication": {
          "type": "string, one of: none, optional, required",
          "default": "none",
          "description": "Controls the servers behavior in regard to requesting a certificate from client connections. `none`: No client authentication `optional`: Requests a client certificate but the client is not required to present one. `required`: Forces a client to present a certificate."
        },
        "ssl_enabled": {
          "type": "boolean",
          "default": "false",
          "description": "Events are by default sent in plain text. You can enable encryption by setting `ssl` to true and configuring the `ssl_certificate` and `ssl_key` options."
        },
        "ssl_handshake_timeout": {
          "type": "number",
          "default": "10000",
          "description": "Time in milliseconds for an incomplete ssl handshake to timeout"
        },
        "ssl_key": {
          "type": "path",
          "description": "SSL key to use. NOTE: This key need to be in the PKCS8 format, you can convert it with OpenSSL for more information."
        },
        "ssl_key_passphrase": {
          "type": "password",
          "description": "SSL key passphrase to use."
        },
        "ssl_keystore_password": {
          "type": "password",
          "description": "The JKS keystore password"
        },
        "ssl_keystore_path": {
          "type": "path",
          "description": "The path for the keystore file that contains a private key and certificate"
        },
        "ssl_keystore_type": {
          "type": "string, one of: pkcs12, jks",
          "description": "The format of the keystore file. It must be either jks or pkcs12"
        },
        "ssl_supported_protocols": {
          "type": "list of string, one of: TLSv1.1, TLSv1.2, TLSv1.3",
          "default": "['TLSv1.2', 'TLSv1.3']"
        },
        "ssl_truststore_password": {
          "type": "password",
          "description": "Set the truststore password"
        },
        "ssl_truststore_path": {
          "type": "path",
          "description": "The path for the keystore that contains the certificates to trust. It must be either a Java keystore (jks) or a PKCS#12 file"
        },
        "ssl_truststore_type": {
          "type": "string, one of: pkcs12, jks",
          "description": "The format of the truststore file. It must be either jks or pkcs12"
        },
        "ssl_verify_mode": {
          "type": "string, one of: none, peer, force_peer",
          "default": "none",
          "description": "By default the server doesn't do any client verification.",
          "deprecated": "Set "
        },
        "threads": {
          "type": "number"
        },
        "tls_max_version": {
          "type": "number",
          "default": "TLS.max.version",
          "description": "The maximum TLS version allowed for the encrypted connections. The value must be the one of the following: 1.0 for TLS 1.0, 1.1 for TLS 1.1, 1.2 for TLS 1.2, 1.3 for TLS 1.3",
          "deprecated": "Set "
        },
        "tls_min_version": {
          "type": "number",
          "default": "TLS.min.version",
          "description": "The minimum TLS version allowed for the encrypted connections. The value must be one of the following: 1.0 for TLS 1.0, 1.1 for TLS 1.1, 1.2 for TLS 1.2, 1.3 for TLS 1.3",
          "deprecated": "Set "
        },
        "user": {
          "type": "string",
          "description": "Username for basic authorization"
        },
        "verify_mode": {
          "type": "string, one of: none, peer, force_peer",
          "default": "none",
          "deprecated": "Set "
        }
      }
    },
    "input/http_poller": {
      "options": {
        "metadata_target": {
          "type": "string",
          "default": "@metadata",
          "description": "If you'd like to work with the request/response metadata. Set this value to the name of the field you'd like to store a nested hash of metadata."
        },
        "schedule": {
          "type": "hash",
          "required": true,
          "description": "Schedule of when to periodically poll from the urls Format: A hash with   + key: \"cron\" | \"every\" | \"in\" | \"at\"   + value: string Examples:   a) { \"every\" =\u003e \"1h\" }   b) { \"cron\" =\u003e \"* * * * * UTC\" } See: rufus/scheduler for details about different schedule options and value string format"
        },
        "target": {
          "type": "field_reference",
          "description": "Define the target field for placing the received data. If this setting is omitted, the data will be stored at the root (top level) of the event."
        },
        "urls": {
          "type": "hash",
          "required": true,
          "description": "A Hash of urls in this format : `\"name\" =\u003e \"url\"`. The name and the url will be passed in the outputed event"
        }
      }
    },
    "input/jdbc": {
      "options": {
        "charset": {
          "type": "string",
          "description": "The character encoding of all columns, leave empty if the columns are already properly UTF-8 encoded. Specific columns charsets using :columns_charset can override this setting."
        },
        "clean_run": {
          "type": "boolean",
          "default": "false",
          "description": "Whether the previous run state should be preserved"
        },
        "columns_charset": {
          "type": "hash",
          "default": "{}",
          "description": "The character encoding for specific columns. This option will override the `:charset` option for the specified columns."
        },
        "connection_retry_attempts": {
          "type": "number",
          "default": "1",
          "description": "Maximum number of times to try connecting to database"
        },
        "connection_retry_attempts_wait_time": {
          "type": "number",
          "default": "0.5",
          "description": "Number of seconds to sleep between connection attempts"
        },
        "jdbc_connection_string": {
          "type": "string",
          "required": true,
          "description": "JDBC connection string"
        },
        "jdbc_default_timezone": {
          "type": "jdbc_timezone_spec",
          "description": "Timezone conversion. SQL does not allow for timezone data in timestamp fields.  This plugin will automatically convert your SQL timestamp fields to Logstash timestamps, in relative UTC time in ISO8601 format."
        },
        "jdbc_driver_class": {
          "type": "string",
          "required": true,
          "description": "JDBC driver class to load, for exmaple, \"org.apache.derby.jdbc.ClientDriver\" NB per https://github.com/logstash-plugins/logstash-input-jdbc/issues/43 if you are using the Oracle JDBC driver (ojdbc6.jar) the correct `jdbc_driver_class` is `\"Java::oracle.jdbc.driver.OracleDriver\"`"
        },
        "jdbc_driver_library": {
          "type": "string",
          "description": "JDBC driver library path to third party driver library. In case of multiple libraries being required you can pass them separated by a comma."
        },
        "jdbc_fetch_size": {
          "type": "number",
          "description": "JDBC fetch size. if not provided, respective driver's default will be used"
        },
        "jdbc_page_size": {
          "type": "number",
          "default": "100000",
          "description": "JDBC page size"
        },
        "jdbc_paging_enabled": {
          "type": "boolean",
          "default": "false",
          "description": "JDBC enable paging"
        },
        "jdbc_paging_mode": {
          "type": "string, one of: auto, explicit",
          "default": "auto",
          "description": "Which pagination mode to use, automatic pagination or explicitly defined in the query."
        },
        "jdbc_password": {
          "type": "password",
          "description": "JDBC password"
        },
        "jdbc_password_filepath": {
          "type": "path",
          "description": "JDBC password filename"
        },
        "jdbc_pool_timeout": {
          "type": "number",
          "default": "5",
          "description": "Connection pool configuration. The amount of seconds to wait to acquire a connection before raising a PoolTimeoutError (default 5)"
        },
        "jdbc_user": {
          "type": "string",
          "required": true,
          "description": "JDBC user"
        },
        "jdbc_validate_connection": {
          "type": "boolean",
          "default": "false",
          "description": "Connection pool configuration. Validate connection before use."
        },
        "jdbc_validation_timeout": {
          "type": "number",
          "default": "3600",
          "description": "Connection pool configuration. How often to validate a connection (in seconds)"
        },
        "last_run_metadata_path": {
          "type": "string",
          "description": "Path to file with last run time. The default will write file to `\u003cpath.data\u003e/plugins/inputs/jdbc/logstash_jdbc_last_run` NOTE: it must be a file path and not a directory path"
        },
        "lowercase_column_names": {
          "type": "boolean",
          "default": "true",
          "description": "Whether to force the lowercasing of identifier fields"
        },
        "parameters": {
          "type": "hash",
          "default": "{}",
          "description": "Hash of query parameter, for example `{ \"target_id\" =\u003e \"321\" }`"
        },
        "plugin_timezone": {
          "type": "string, one of: local, utc",
          "default": "utc",
          "description": "give users the ability to force Sequel application side into using local timezone"
        },
        "prepared_statement_bind_values": {
          "type": "array",
          "default": "[]"
        },
        "prepared_statement_name": {
          "type": "string"
        },
        "record_last_run": {
          "type": "boolean",
          "default": "true",
          "description": "Whether to save state or not in last_run_metadata_path"
        },
        "schedule": {
          "type": "string",
          "description": "Schedule of when to periodically run statement, in Cron format for example: \"* * * * *\" (execute query every minute, on the minute)"
        },
        "sequel_opts": {
          "type": "hash",
          "default": "{}",
          "description": "General/Vendor-specific Sequel configuration options."
        },
        "sql_log_level": {
          "type": "string, one of: fatal, error, warn, info, debug",
          "default": "info",
          "description": "Log level at which to log SQL queries, the accepted values are the common ones fatal, error, warn, info and debug. The default value is info."
        },
        "statement": {
          "type": "string",
          "description": "Statement to execute"
        },
        "statement_filepath": {
          "type": "path",
          "description": "Path of file containing statement to execute"
        },
        "statement_retry_attempts": {
          "type": "number",
          "default": "1",
          "description": "Maximum number of times to try running statement"
        },
        "statement_retry_attempts_wait_time": {
          "type": "number",
          "default": "0.5",
          "description": "Number of seconds to sleep between statement execution"
        },
        "target": {
          "type": "field_reference",
          "description": "Define the target field to store the loaded columns"
        },
        "tracking_column": {
          "type": "string",
          "description": "If tracking column value rather than timestamp, the column whose value is to be tracked"
        },
        "tracking_column_type": {
          "type": "string, one of: numeric, timestamp",
          "default": "numeric",
          "description": "Type of tracking column. Currently only \"numeric\" and \"timestamp\""
        },
        "use_column_value": {
          "type": "boolean",
          "default": "false",
          "description": "Use an incremental column value rather than a timestamp"
        },
        "use_prepared_statements": {
          "type": "boolean",
          "default": "false"
        }
      }
    },
    "input/jms": {
      "description": "Read events from a Jms Broker. Supports both Jms Queues and Topics.",
      "options": {
        "broker_url": {
          "type": "string",
          "description": "Url to use when connecting to the JMS provider"
        },
        "destination": {
          "type": "string",
          "required": true,
          "description": "Name of the destination queue or topic to use."
        },
        "durable_subscriber": {
          "type": "boolean",
          "default": "false",
          "description": "Durable message_consumer settings. By default the `durable_subscriber_name` will be set to the topic, and `durable_subscriber_client_id` will be set to 'Logstash'"
        },
        "durable_subscriber_client_id": {
          "type": "string"
        },
        "durable_subscriber_name": {
          "type": "string"
        },
        "factory": {
          "type": "string",
          "description": "Name of JMS Provider Factory class"
        },
        "factory_settings": {
          "type": "hash",
          "description": "Factory settings"
        },
        "headers_target": {
          "type": "field_reference"
        },
        "include_body": {
          "type": "boolean",
          "default": "true"
        },
        "include_header": {
          "type": "boolean",
          "description": "A JMS message has three parts :  Message Headers (required)  Message Properties (optional)  Message Bodies (optional) You can tell the input plugin which parts should be included in the event produced by Logstash",
          "deprecated": "Set "
        },
        "include_headers": {
          "type": "boolean",
          "default": "true"
        },
        "include_properties": {
          "type": "boolean",
          "default": "true",
          "description": "Include JMS Message Properties Field values in the event"
        },
        "interval": {
          "type": "number",
          "default": "10",
          "description": "Polling interval in seconds."
        },
        "jndi_context": {
          "type": "hash",
          "description": "Mandatory if jndi lookup is being used, contains details on how to connect to JNDI server"
        },
        "jndi_name": {
          "type": "string",
          "description": "Name of JNDI entry at which the Factory can be found"
        },
        "keystore": {
          "type": "path"
        },
        "keystore_password": {
          "type": "password"
        },
        "oracle_aq_buffered_messages": {
          "type": "boolean",
          "default": "false",
          "description": "Receive Oracle AQ buffered messages. In this mode persistent Oracle AQ JMS messages will not be received."
        },
        "password": {
          "type": "password",
          "description": "Password to use when connecting to the JMS provider"
        },
        "properties_target": {
          "type": "field_reference"
        },
        "pub_sub": {
          "type": "boolean",
          "default": "false",
          "description": "If pub-sub (topic) style should be used."
        },
        "require_jars": {
          "type": "array",
          "description": "An optional array of Jar file names to load for the specified JMS provider. By using this option it is not necessary to put all the JMS Provider specific jar files into the java CLASSPATH prior to starting Logstash."
        },
        "runner": {
          "description": "Choose an implementation of the run block. Value can be either consumer, async or thread"
        },
        "selector": {
          "type": "string",
          "description": "Set the selector to use to get messages off the queue or topic"
        },
        "skip_headers": {
          "type": "array",
          "default": "[]",
          "description": "List of headers to skip from the event if headers are included"
        },
        "skip_properties": {
          "type": "array",
          "default": "[]",
          "description": "List of properties to skip from the event if properties are included"
        },
        "system_properties": {
          "type": "hash",
          "description": "System properties"
        },
        "target": {
          "type": "field_reference",
          "description": "Defines a target field for placing fields. If this setting is omitted, data gets stored at the root (top level) of the event. The target is only relevant while decoding data into a new event."
        },
        "timeout": {
          "type": "number",
          "default": "60",
          "description": "Initial connection timeout in seconds."
        },
        "truststore": {
          "type": "path"
        },
        "truststore_password": {
          "type": "password"
        },
        "use_jms_timestamp": {
          "type": "boolean",
          "default": "false",
          "description": "Convert the JMSTimestamp header field to the @timestamp value of the event"
        },
        "username": {
          "type": "string",
          "description": "Username to connect to JMS provider with"
        },
        "yaml_file": {
          "type": "string",
          "description": "Yaml config file"
        },
        "yaml_section": {
          "type": "string",
          "description": "Yaml config file section name For some known examples, see: [Example jms.yml](https://github.com/reidmorrison/jruby-jms/blob/master/examples/jms.yml)"
        }
      }
    },
    "input/kafka": {
      "description": "This input will read events from a Kafka topic. It uses the 0.10 version of the consumer API provided by Kafka to read messages from the broker.",
      "options": {
        "auto_commit_interval_ms": {
          "type": "number",
          "default": "5000 # Kafka default",
          "description": "The frequency in milliseconds that the consumer offsets are committed to Kafka."
        },
        "auto_create_topics": {
          "type": "boolean",
          "default": "true",
          "description": "Option to add Kafka metadata like topic, message size and header key values to the event. With `basic` this will add a field named `kafka` to the logstash event containing the following attributes:   `topic`: The topic this message is associated with   `consumer_group`: The consumer group used to read in this event   `partition`: The partition this message is associated with   `offset`: The offset from the partition this message is associated with   `key`: A ByteBuffer containing the message key   `timestamp`: The timestamp of this message While with `extended` it adds also all the key values present in the Kafka header if the key is valid UTF-8 else silently skip it."
        },
        "auto_offset_reset": {
          "type": "string",
          "description": "What to do when there is no initial offset in Kafka or if an offset is out of range:"
        },
        "bootstrap_servers": {
          "type": "string",
          "default": "localhost:9092",
          "description": "A list of URLs of Kafka instances to use for establishing the initial connection to the cluster. This list should be in the form of `host1:port1,host2:port2` These urls are just used for the initial connection to discover the full cluster membership (which may change dynamically) so this list need not contain the full set of servers (you may want more than one, though, in case a server is down)."
        },
        "check_crcs": {
          "type": "boolean",
          "default": "true",
          "description": "Automatically check the CRC32 of the records consumed. This ensures no on-the-wire or on-disk corruption to the messages occurred. This check adds some overhead, so it may be disabled in cases seeking extreme performance."
        },
        "client_dns_lookup": {
          "type": "string, one of: default, use_all_dns_ips, resolve_canonical_bootstrap_servers_only",
          "default": "use_all_dns_ips",
          "description": "How DNS lookups should be done. If set to `use_all_dns_ips`, when the lookup returns multiple IP addresses for a hostname, they will all be attempted to connect to before failing the connection. If the value is `resolve_canonical_bootstrap_servers_only` each entry will be resolved and expanded into a list of canonical names. Starting from Kafka 3 `default` value for `client.dns.lookup` value has been removed. If explicitly configured it fallbacks to `use_all_dns_ips`."
        },
        "client_id": {
          "type": "string",
          "default": "logstash",
          "description": "The id string to pass to the server when making requests. The purpose of this is to be able to track the source of requests beyond just ip/port by allowing a logical application name to be included."
        },
        "client_rack": {
          "type": "string",
          "description": "The rack id string to pass to the server when making requests. This is used as a selector for a rack, region, or datacenter. Corresponds to the broker.rack parameter in the broker configuration. Only has an effect in combination with brokers with Kafka 2.4+ with the broker.rack setting. Ignored otherwise."
        },
        "codec": {
          "type": "codec",
          "description": "default :codec, 'plain' or 'json' depending whether schema registry is used"
        },
        "consumer_threads": {
          "type": "number",
          "default": "1",
          "description": "Ideally you should have as many threads as the number of partitions for a perfect balancemore threads than partitions means that some threads will be idle"
        },
        "decorate_events": {
          "type": "string, one of: none, basic, extended, false, true",
          "default": "none"
        },
        "enable_auto_commit": {
          "type": "boolean",
          "default": "true",
          "description": "If true, periodically commit to Kafka the offsets of messages already returned by the consumer. This committed offset will be used when the process fails as the position from which the consumption will begin."
        },
        "exclude_internal_topics": {
          "type": "string",
          "description": "Whether records from internal topics (such as offsets) should be exposed to the consumer. If set to true the only way to receive records from an internal topic is subscribing to it."
        },
        "fetch_max_bytes": {
          "type": "number",
          "default": "52_428_800 # (50MB) Kafka default",
          "description": "The maximum amount of data the server should return for a fetch request. This is not an absolute maximum, if the first message in the first non-empty partition of the fetch is larger than this value, the message will still be returned to ensure that the consumer can make progress."
        },
        "fetch_max_wait_ms": {
          "type": "number",
          "default": "500 # Kafka default",
          "description": "The maximum amount of time the server will block before answering the fetch request if there isn't sufficient data to immediately satisfy `fetch_min_bytes`. This should be less than or equal to the timeout used in `poll_timeout_ms`"
        },
        "fetch_min_bytes": {
          "type": "number",
          "description": "The minimum amount of data the server should return for a fetch request. If insufficient data is available the request will wait for that much data to accumulate before answering the request."
        },
        "group_id": {
          "type": "string",
          "default": "logstash",
          "description": "The identifier of the group this consumer belongs to. Consumer group is a single logical subscriber that happens to be made up of multiple processors. Messages in a topic will be distributed to all Logstash instances with the same `group_id`"
        },
        "group_instance_id": {
          "type": "string",
          "description": "Set a static group instance id used in static membership feature to avoid rebalancing when a consumer goes offline. If set and `consumer_threads` is greater than 1 then for each consumer crated by each thread an artificial suffix is appended to the user provided `group_instance_id` to avoid clashing."
        },
        "heartbeat_interval_ms": {
          "type": "number",
          "default": "3000 # Kafka default",
          "description": "The expected time between heartbeats to the consumer coordinator. Heartbeats are used to ensure that the consumer's session stays active and to facilitate rebalancing when new consumers join or leave the group. The value must be set lower than `session.timeout.ms`, but typically should be set no higher than 1/3 of that value. It can be adjusted even lower to control the expected time for normal rebalances."
        },
        "isolation_level": {
          "type": "string, one of: read_uncommitted, read_committed",
          "default": "read_uncommitted\" # Kafka default",
          "description": "Controls how to read messages written transactionally. If set to read_committed, consumer.poll() will only return transactional messages which have been committed. If set to read_uncommitted' (the default), consumer.poll() will return all messages, even transactional messages which have been aborted. Non-transactional messages will be returned unconditionally in either mode."
        },
        "jaas_path": {
          "type": "path",
          "description": "The Java Authentication and Authorization Service (JAAS) API supplies user authentication and authorization services for Kafka. This setting provides the path to the JAAS file. Sample JAAS file for Kafka client:"
        },
        "kerberos_config": {
          "type": "path",
          "description": "Optional path to kerberos config file. This is krb5.conf style as detailed in https://web.mit.edu/kerberos/krb5-1.12/doc/admin/conf_files/krb5_conf.html"
        },
        "key_deserializer_class": {
          "type": "string",
          "default": "DEFAULT_DESERIALIZER_CLASS",
          "description": "Java Class used to deserialize the record's key"
        },
        "max_partition_fetch_bytes": {
          "type": "number",
          "default": "1_048_576 # (1MB) Kafka default",
          "description": "The maximum amount of data per-partition the server will return. The maximum total memory used for a request will be \u003ccode\u003e#partitions * max.partition.fetch.bytes\u003c/code\u003e. This size must be at least as large as the maximum message size the server allows or else it is possible for the producer to send messages larger than the consumer can fetch. If that happens, the consumer can get stuck trying to fetch a large message on a certain partition."
        },
        "max_poll_interval_ms": {
          "type": "number",
          "default": "300_000 # (5m) Kafka default",
          "description": "The maximum delay between invocations of poll() when using consumer group management. This places an upper bound on the amount of time that the consumer can be idle before fetching more records. If poll() is not called before expiration of this timeout, then the consumer is considered failed and the group will rebalance in order to reassign the partitions to another member."
        },
        "max_poll_records": {
          "type": "number",
          "default": "500 # Kafka default",
          "description": "The maximum number of records returned in a single call to poll()."
        },
        "partition_assignment_strategy": {
          "type": "string",
          "description": "The name of the partition assignment strategy that the client uses to distribute partition ownership amongst consumer instances, supported options are `range`, `round_robin`, `sticky` and `cooperative_sticky` (for backwards compatibility setting the class name directly is supported)."
        },
        "poll_timeout_ms": {
          "type": "number",
          "default": "100",
          "description": "Time kafka consumer will wait to receive new messages from topics"
        },
        "receive_buffer_bytes": {
          "type": "number",
          "default": "32_768 # (32KB) Kafka default",
          "description": "The size of the TCP receive buffer (SO_RCVBUF) to use when reading data. If the value is `-1`, the OS default will be used."
        },
        "reconnect_backoff_ms": {
          "type": "number",
          "default": "50 # Kafka default",
          "description": "The base amount of time to wait before attempting to reconnect to a given host. This avoids repeatedly connecting to a host in a tight loop. This backoff applies to all connection attempts by the client to a broker."
        },
        "retry_backoff_ms": {
          "type": "number",
          "default": "100 # Kafka default",
          "description": "The amount of time to wait before attempting to retry a failed fetch request to a given topic partition. This avoids repeated fetching-and-failing in a tight loop."
        },
        "sasl_client_callback_handler_class": {
          "type": "string",
          "description": "SASL client callback handler class"
        },
        "sasl_jaas_config": {
          "type": "string",
          "description": "JAAS configuration settings. This allows JAAS config to be a part of the plugin configuration and allows for different JAAS configuration per each plugin config."
        },
        "sasl_kerberos_service_name": {
          "type": "string",
          "description": "The Kerberos principal name that Kafka broker runs as. This can be defined either in Kafka's JAAS config or in Kafka's config."
        },
        "sasl_mechanism": {
          "type": "string",
          "default": "GSSAPI",
          "description": "SASL mechanism used for client connections. This may be any mechanism for which a security provider is available. GSSAPI is the default mechanism."
        },
        "schema_registry_key": {
          "type": "string",
          "description": "Option to set key to access Schema Registry."
        },
        "schema_registry_proxy": {
          "type": "uri",
          "description": "Option to set the proxy of the Schema Registry. This option permits to define a proxy to be used to reach the schema registry service instance."
        },
        "schema_registry_secret": {
          "type": "password",
          "description": "Option to set secret to access Schema Registry."
        },
        "schema_registry_ssl_keystore_location": {
          "type": "string",
          "description": "If schema registry client authentication is required, this setting stores the keystore path."
        },
        "schema_registry_ssl_keystore_password": {
          "type": "password",
          "description": "The keystore password."
        },
        "schema_registry_ssl_keystore_type": {
          "type": "string, one of: jks, PKCS12",
          "default": "jks",
          "description": "The keystore type"
        },
        "schema_registry_ssl_truststore_location": {
          "type": "string",
          "description": "The JKS truststore path to validate the Schema Registry's certificate."
        },
        "schema_registry_ssl_truststore_password": {
          "type": "password",
          "description": "The truststore password."
        },
        "schema_registry_ssl_truststore_type": {
          "type": "string, one of: jks, PKCS12",
          "default": "jks",
          "description": "The truststore type"
        },
        "schema_registry_url": {
          "type": "uri",
          "description": "Option to set the endpoint of the Schema Registry. This option permit the usage of Avro Kafka deserializer which retrieve the schema of the Avro message from an instance of schema registry. If this option has value `value_deserializer_class` nor `topics_pattern` could be valued"
        },
        "schema_registry_validation": {
          "type": "string, one of: auto, skip",
          "default": "auto",
          "description": "Option to skip validating the schema registry during registration. This can be useful when using certificate based auth"
        },
        "security_protocol": {
          "type": "string, one of: PLAINTEXT, SSL, SASL_PLAINTEXT, SASL_SSL",
          "default": "PLAINTEXT",
          "description": "Security protocol to use, which can be either of PLAINTEXT,SSL,SASL_PLAINTEXT,SASL_SSL"
        },
        "send_buffer_bytes": {
          "type": "number",
          "default": "131_072 # (128KB) Kafka default",
          "description": "The size of the TCP send buffer (SO_SNDBUF) to use when sending data. If the value is -1, the OS default will be used."
        },
        "session_timeout_ms": {
          "type": "number",
          "default": "10_000 # (10s) Kafka default",
          "description": "The timeout after which, if the `poll_timeout_ms` is not invoked, the consumer is marked dead and a rebalance operation is triggered for the group identified by `group_id`"
        },
        "ssl_endpoint_identification_algorithm": {
          "type": "string",
          "default": "https",
          "description": "Algorithm to use when verifying host. Set to \"\" to disable"
        },
        "ssl_key_password": {
          "type": "password",
          "description": "The password of the private key in the key store file."
        },
        "ssl_keystore_location": {
          "type": "path",
          "description": "If client authentication is required, this setting stores the keystore path."
        },
        "ssl_keystore_password": {
          "type": "password",
          "description": "If client authentication is required, this setting stores the keystore password"
        },
        "ssl_keystore_type": {
          "type": "string",
          "description": "The keystore type."
        },
        "ssl_truststore_location": {
          "type": "path",
          "description": "The JKS truststore path to validate the Kafka broker's certificate."
        },
        "ssl_truststore_password": {
          "type": "password",
          "description": "The truststore password"
        },
        "ssl_truststore_type": {
          "type": "string",
          "description": "The truststore type."
        },
        "topics": {
          "type": "array",
          "default": "[\"logstash\"]",
          "description": "A list of topics to subscribe to, defaults to [\"logstash\"]."
        },
        "topics_pattern": {
          "type": "string",
          "description": "A topic regex pattern to subscribe to. The topics configuration will be ignored when using this configuration."
        },
        "value_deserializer_class": {
          "type": "string",
          "default": "DEFAULT_DESERIALIZER_CLASS",
          "description": "Java Class used to deserialize the record's value"
        }
      }
    },
    "input/logstash": {
      "options": {
        "host": {
          "type": "string",
          "default": "0.0.0.0"
        },
        "password": {
          "type": "password"
        },
        "port": {
          "type": "number",
          "default": "9800"
        },
        "ssl_certificate": {
          "type": "path",
          "description": "SSL:IDENTITY:SOURCE cert/key pair"
        },
        "ssl_certificate_authorities": {
          "type": "list of path",
          "description": "SSL:TRUST:SOURCE ca file"
        },
        "ssl_cipher_suites": {
          "type": "list of string"
        },
        "ssl_client_authentication": {
          "type": "string, one of: none, optional, required",
          "default": "none",
          "description": "SSL:TRUST:CONFIG"
        },
        "ssl_enabled": {
          "type": "boolean",
          "default": "true"
        },
        "ssl_handshake_timeout": {
          "type": "number",
          "description": "SSL:TUNING"
        },
        "ssl_key": {
          "type": "path"
        },
        "ssl_key_passphrase": {
          "type": "password"
        },
        "ssl_keystore_password": {
          "type": "password"
        },
        "ssl_keystore_path": {
          "type": "path",
          "description": "SSL:IDENTITY:SOURCE keystore"
        },
        "ssl_supported_protocols": {
          "type": "list of string"
        },
        "username": {
          "type": "string",
          "description": "optional username/password credentials"
        }
      }
    },
    "input/pipe": {
      "description": "Stream events from a long running command pipe.",
      "options": {
        "command": {
          "type": "string",
          "required": true,
          "description": "Command to run and read events from, one line at a time."
        }
      }
    },
    "input/rabbitmq": {
      "options": {
        "ack": {
          "type": "boolean",
          "default": "true",
          "description": "Enable message acknowledgements. With acknowledgements messages fetched by Logstash but not yet sent into the Logstash pipeline will be requeued by the server if Logstash shuts down. Acknowledgements will however hurt the message throughput."
        },
        "arguments": {
          "type": "array",
          "default": "{}"
        },
        "auto_delete": {
          "type": "boolean",
          "default": "false",
          "description": "Should the queue be deleted on the broker when the last consumer disconnects? Set this option to `false` if you want the queue to remain on the broker, queueing up messages until a consumer comes along to consume them."
        },
        "durable": {
          "type": "boolean",
          "default": "false",
          "description": "Is this queue durable? (aka; Should it survive a broker restart?)"
        },
        "exchange": {
          "type": "string",
          "description": "The name of the exchange to bind the queue to. Specify `exchange_type` as well to declare the exchange if it does not exist"
        },
        "exchange_type": {
          "type": "string",
          "description": "The type of the exchange to bind to. Specifying this will cause this plugin to declare the exchange if it does not exist."
        },
        "exclusive": {
          "type": "boolean",
          "default": "false",
          "description": "Is the queue exclusive? Exclusive queues can only be used by the connection that declared them and will be deleted when it is closed (e.g. due to a Logstash restart)."
        },
        "key": {
          "type": "string",
          "default": "logstash",
          "description": "The routing key to use when binding a queue to the exchange. This is only relevant for direct or topic exchanges."
        },
        "metadata_enabled": {
          "type": "string, one of: none, basic, extended, false, true",
          "default": "none",
          "description": "Enable the storage of message headers and properties in `@metadata`. This may impact performance"
        },
        "passive": {
          "type": "boolean",
          "default": "false",
          "description": "If true the queue will be passively declared, meaning it must already exist on the server. To have Logstash create the queue if necessary leave this option as false. If actively declaring a queue that already exists, the queue options for this plugin (durable etc) must match those of the existing queue."
        },
        "prefetch_count": {
          "type": "number",
          "default": "256",
          "description": "Prefetch count. If acknowledgements are enabled with the `ack` option, specifies the number of outstanding unacknowledged messages allowed."
        },
        "queue": {
          "type": "string",
          "description": "The name of the queue Logstash will consume events from. If left empty, a transient queue with an randomly chosen name will be created."
        },
        "subscription_retry_interval_seconds": {
          "type": "number",
          "required": true,
          "default": "5",
          "description": "Amount of time in seconds to wait after a failed subscription request before retrying. Subscribes can fail if the server goes away and then comes back."
        }
      }
    },
    "input/redis": {
      "options": {
        "batch_count": {
          "type": "number",
          "default": "125",
          "description": "The number of events to return from Redis using EVAL."
        },
        "command_map": {
          "type": "hash",
          "default": "{}",
          "description": "Redefined Redis commands to be passed to the Redis client."
        },
        "data_type": {
          "type": "string, one of: list, channel, pattern_channel",
          "required": true,
          "description": "Specify either list or channel.  If `data_type` is `list`, then we will BLPOP the key.  If `data_type` is `channel`, then we will SUBSCRIBE to the key. If `data_type` is `pattern_channel`, then we will PSUBSCRIBE to the key."
        },
        "db": {
          "type": "number",
          "default": "0",
          "description": "The Redis database number."
        },
        "host": {
          "type": "string",
          "default": "127.0.0.1",
          "description": "The hostname of your Redis server."
        },
        "key": {
          "type": "string",
          "required": true,
          "description": "The name of a Redis list or channel."
        },
        "password": {
          "type": "password",
          "description": "Password to authenticate with. There is no authentication by default."
        },
        "path": {
          "type": "string",
          "description": "The unix socket path to connect on. Will override host and port if defined. There is no unix socket path by default."
        },
        "port": {
          "type": "number",
          "default": "6379",
          "description": "The port to connect on."
        },
        "ssl": {
          "type": "boolean",
          "default": "false",
          "description": "SSL"
        },
        "timeout": {
          "type": "number",
          "default": "5",
          "description": "Initial connection timeout in seconds."
        }
      }
    },
    "input/s3": {
      "description": "Stream events from files from a S3 bucket.",
      "options": {
        "additional_settings": {
          "type": "hash",
          "default": "{}"
        },
        "backup_add_prefix": {
          "type": "string",
          "default": "nil",
          "description": "Append a prefix to the key (full path including file name in s3) after processing. If backing up to another (or the same) bucket, this effectively lets you choose a new 'folder' to place the files in"
        },
        "backup_to_bucket": {
          "type": "string",
          "default": "nil",
          "description": "Name of a S3 bucket to backup processed files to."
        },
        "backup_to_dir": {
          "type": "string",
          "default": "nil",
          "description": "Path of a local directory to backup processed files to."
        },
        "bucket": {
          "type": "string",
          "required": true,
          "description": "The name of the S3 bucket."
        },
        "delete": {
          "type": "boolean",
          "default": "false",
          "description": "Whether to delete processed files from the original bucket."
        },
        "exclude_pattern": {
          "type": "string",
          "default": "nil",
          "description": "Ruby style regexp of keys to exclude from the bucket"
        },
        "gzip_pattern": {
          "type": "string",
          "default": "\\.gz(ip)?$",
          "description": "Regular expression used to determine whether an input file is in gzip format. default to an expression that matches *.gz and *.gzip file extensions"
        },
        "include_object_properties": {
          "type": "boolean",
          "default": "false",
          "description": "Whether or not to include the S3 object's properties (last_modified, content_type, metadata) into each Event at [@metadata][s3]. Regardless of this setting, [@metdata][s3][key] will always be present."
        },
        "interval": {
          "type": "number",
          "default": "60",
          "description": "Interval to wait between to check the file list again after a run is finished. Value is in seconds."
        },
        "prefix": {
          "type": "string",
          "default": "nil",
          "description": "If specified, the prefix of filenames in the bucket must match (not a regexp)"
        },
        "sincedb_path": {
          "type": "string",
          "default": "nil",
          "description": "The path to use for writing state. The state stored by this plugin is a memory of files already processed by this plugin."
        },
        "temporary_directory": {
          "type": "string",
          "default": "File.join(Dir.tmpdir, \"logstash\")",
          "description": "Set the directory where logstash will store the tmp files before processing them. default to the current OS temporary directory in linux /tmp/logstash"
        },
        "watch_for_new_files": {
          "type": "boolean",
          "default": "true",
          "description": "Whether to watch for new files with the interval. If false, overrides any interval and only lists the s3 bucket once."
        }
      }
    },
    "input/snmp": {
      "description": "The SNMP input plugin polls network devices using Simple Network Management Protocol (SNMP) to gather information related to the current state of the devices operation.",
      "options": {
        "get": {
          "type": "array",
          "description": "List of OIDs for which we want to retrieve the scalar value"
        },
        "hosts": {
          "type": "array",
          "description": "List of hosts to query the configured `get` and `walk` options."
        },
        "interval": {
          "type": "number",
          "default": "30",
          "description": "Set polling interval in seconds"
        },
        "local_engine_id": {
          "type": "string",
          "description": "The optional SNMPv3 engine's administratively-unique identifier. Its length must be greater or equal than 5 and less or equal than 32."
        },
        "poll_hosts_timeout": {
          "type": "number",
          "description": "Timeout in milliseconds to execute all hosts configured operations (get, walk, table)."
        },
        "tables": {
          "type": "array",
          "description": "List of tables to walk"
        },
        "threads": {
          "type": "number",
          "required": true,
          "description": "Number of threads to use for concurrently executing the hosts SNMP requests."
        },
        "walk": {
          "type": "array",
          "description": "List of OIDs for which we want to retrieve the subtree of information"
        }
      }
    },
    "input/snmptrap": {
      "description": "Read snmp trap messages as events",
      "options": {
        "community": {
          "type": "array",
          "default": "public",
          "description": "SNMP Community String to listen for."
        },
        "host": {
          "type": "string",
          "required": true,
          "default": "0.0.0.0",
          "description": "The address to listen on"
        },
        "port": {
          "type": "number",
          "required": true,
          "default": "1062",
          "description": "The port to listen on. Remember that ports less than 1024 (privileged ports) may require root to use. hence the default of 1062."
        },
        "supported_transports": {
          "type": "list of string, one of: tcp, udp",
          "required": true,
          "default": "%w[udp]",
          "description": "The supported transport protocols to listen on."
        },
        "supported_versions": {
          "type": "list of string, one of: 1, 2c, 3",
          "required": true,
          "description": "The supported SNMP versions to listen on"
        },
        "threads": {
          "type": "number",
          "required": true,
          "description": "Number of threads to use for processing the received SNMP messages. By default, SNMP4J uses a single listener thread, which delegates all received data to the message dispatcher. This setting, configures the number of threads to be used by the message dispatcher, so it won't block the listener thread."
        },
        "yamlmibdir": {
          "type": "string",
          "description": "directory of YAML MIB maps  (same format ruby-snmp uses)",
          "deprecated": "Use `mib_paths` instead."
        }
      }
    },
    "input/sqs": {
      "description": "Pull events from an Amazon Web Services Simple Queue Service (SQS) queue.",
      "options": {
        "additional_settings": {
          "type": "hash",
          "default": "{}"
        },
        "id_field": {
          "type": "string",
          "description": "Name of the event field in which to store the SQS message ID"
        },
        "md5_field": {
          "type": "string",
          "description": "Name of the event field in which to store the SQS message MD5 checksum"
        },
        "polling_frequency": {
          "type": "number",
          "default": "DEFAULT_POLLING_FREQUENCY",
          "description": "Polling frequency, default is 20 seconds"
        },
        "queue": {
          "type": "string",
          "required": true,
          "description": "Name of the SQS Queue name to pull messages from. Note that this is just the name of the queue, not the URL or ARN."
        },
        "queue_owner_aws_account_id": {
          "type": "string",
          "description": "Account ID of the AWS account which owns the queue."
        },
        "sent_timestamp_field": {
          "type": "string",
          "description": "Name of the event field in which to store the SQS message Sent Timestamp"
        }
      }
    },
    "input/stdin": {
      "description": "Read events from standard input."
    },
    "input/syslog": {
      "description": "Read syslog messages as events over the network.",
      "options": {
        "facility_labels": {
          "type": "array",
          "description": "Labels for facility levels. These are defined in RFC3164."
        },
        "grok_pattern": {
          "type": "string",
          "description": "Set custom grok pattern to parse the syslog, in case the format differs from the defined standard.  This is common in security and other appliances"
        },
        "host": {
          "type": "string",
          "default": "0.0.0.0",
          "description": "The address to listen on."
        },
        "locale": {
          "type": "string",
          "description": "Specify a locale to be used for date parsing using either IETF-BCP47 or POSIX language tag. Simple examples are `en`,`en-US` for BCP47 or `en_US` for POSIX. If not specified, the platform default will be used."
        },
        "port": {
          "type": "number",
          "default": "514",
          "description": "The port to listen on. Remember that ports less than 1024 (privileged ports) may require root to use."
        },
        "proxy_protocol": {
          "type": "boolean",
          "default": "false",
          "description": "Proxy protocol support, only v1 is supported at this time http://www.haproxy.org/download/1.5/doc/proxy-protocol.txt"
        },
        "service_type": {
          "type": "string",
          "default": "system",
          "description": "ECS only option to configure [service][type] value in produced events."
        },
        "severity_labels": {
          "type": "array",
          "description": "Labels for severity levels. These are defined in RFC3164."
        },
        "syslog_field": {
          "type": "string",
          "default": "message",
          "description": "Use custom post-codec processing field (e.g. syslog, after cef codec processing) instead of the default `message` field"
        },
        "timezone": {
          "type": "string",
          "description": "Specify a time zone canonical ID to be used for date parsing. The valid IDs are listed on the [Joda.org available time zones page](http://joda-time.sourceforge.net/timezones.html). This is useful in case the time zone cannot be extracted from the value, and is not the platform default. If this is not specified the platform default will be used. Canonical ID is good as it takes care of daylight saving time for you For example, `America/Los_Angeles` or `Europe/France` are valid IDs."
        },
        "use_labels": {
          "type": "boolean",
          "default": "true",
          "description": "Use label parsing for severity and facility levels."
        }
      }
    },
    "input/tcp": {
      "description": "Read events over a TCP socket.",
      "options": {
        "dns_reverse_lookup_enabled": {
          "type": "boolean",
          "default": "true",
          "description": "Option to allow users to avoid DNS Reverse Lookup."
        },
        "host": {
          "type": "string",
          "default": "0.0.0.0",
          "description": "When mode is `server`, the address to listen on. When mode is `client`, the address to connect to."
        },
        "mode": {
          "type": "string, one of: server, client",
          "default": "server",
          "description": "Mode to operate in. `server` listens for client connections, `client` connects to a server."
        },
        "port": {
          "type": "number",
          "required": true,
          "description": "When mode is `server`, the port to listen on. When mode is `client`, the port to connect to."
        },
        "proxy_protocol": {
          "type": "boolean",
          "default": "false",
          "description": "Proxy protocol support, only v1 is supported at this time http://www.haproxy.org/download/1.5/doc/proxy-protocol.txt"
        },
        "ssl_cert": {
          "type": "path",
          "description": "SSL certificate path",
          "deprecated": "Use "
        },
        "ssl_certificate": {
          "type": "path",
          "description": "SSL certificate path"
        },
        "ssl_certificate_authorities": {
          "type": "array",
          "default": "[]",
          "description": "Validate client certificates against these authorities. You can define multiple files or paths. All the certificates will be read and added to the trust store."
        },
        "ssl_cipher_suites": {
          "default": "[]",
          "description": "The list of ciphers suite to use, listed by priorities. NOTE: the default setting [] uses Java SSL defaults."
        },
        "ssl_client_authentication": {
          "type": "string, one of: none, optional, required",
          "default": "required",
          "description": "Controls the servers behavior in regard to requesting a certificate from client connections. `none`: No client authentication `optional`: Requests a client certificate but the client is not required to present one. `required`: Forces a client to present a certificate. This option needs to be used with `ssl_certificate_authorities` and a defined list of CAs."
        },
        "ssl_enable": {
          "type": "boolean",
          "default": "false",
          "description": "Enable SSL (must be set for other `ssl_` options to take effect).",
          "deprecated": "Use "
        },
        "ssl_enabled": {
          "type": "boolean",
          "default": "false",
          "description": "Enable SSL (must be set for other `ssl_` options to take effect)."
        },
        "ssl_extra_chain_certs": {
          "type": "array",
          "default": "[]",
          "description": "An Array of extra X509 certificates to be added to the certificate chain. Useful when the CA chain is not necessary in the system store."
        },
        "ssl_key": {
          "type": "path",
          "description": "SSL key path"
        },
        "ssl_key_passphrase": {
          "type": "password",
          "default": "nil",
          "description": "SSL key passphrase"
        },
        "ssl_supported_protocols": {
          "type": "list of string, one of: TLSv1.1, TLSv1.2, TLSv1.3",
          "default": "[]",
          "description": "NOTE: the default setting [] uses Java SSL engine defaults."
        },
        "ssl_verification_mode": {
          "type": "string, one of: full, none",
          "default": "full",
          "description": "Options to verify the server's certificate. \"full\": validates that the provided certificate has an issue date thats within the not_before and not_after dates; chains to a trusted Certificate Authority (CA); has a hostname or IP address that matches the names within the certificate. \"certificate\": Validates the provided certificate and verifies that its signed by a trusted authority (CA), but doest check the certificate hostname. \"none\": performs no certificate validation. Disabling this severely compromises security (https://www.cs.utexas.edu/~shmat/shmat_ccs12.pdf)"
        },
        "ssl_verify": {
          "type": "boolean",
          "default": "true",
          "description": "Verify the identity of the other end of the SSL connection against the CA. For input, sets the field `sslsubject` to that of the client certificate.",
          "deprecated": "Use "
        },
        "tcp_keep_alive": {
          "type": "boolean",
          "default": "false",
          "description": "Instruct the socket to use TCP keep alives. Uses OS defaults for keep alive settings."
        }
      }
    },
    "input/twitter": {
      "description": "Ingest events from the Twitter Streaming API.",
      "options": {
        "consumer_key": {
          "type": "string",
          "required": true,
          "description": "Your Twitter App's consumer key"
        },
        "consumer_secret": {
          "type": "password",
          "required": true,
          "description": "Your Twitter App's consumer secret"
        },
        "follows": {
          "type": "array",
          "description": "A comma separated list of user IDs, indicating the users to return statuses for in the Twitter stream. See https://dev.twitter.com/streaming/overview/request-parameters#follow for more details."
        },
        "full_tweet": {
          "type": "boolean",
          "default": "false",
          "description": "Record full tweet object as given to us by the Twitter Streaming API."
        },
        "ignore_retweets": {
          "type": "boolean",
          "default": "false",
          "description": "Lets you ingore the retweets coming out of the Twitter API. Default =\u003e false"
        },
        "keywords": {
          "type": "array",
          "description": "Any keywords to track in the Twitter stream. For multiple keywords, use the syntax [\"foo\", \"bar\"]. There's a logical OR between each keyword string listed and a logical AND between words separated by spaces per keyword string. See https://dev.twitter.com/streaming/overview/request-parameters#track for more details."
        },
        "languages": {
          "type": "array",
          "description": "A list of BCP 47 language identifiers corresponding to any of the languages listed on Twitters advanced search page will only return tweets that have been detected as being written in the specified languages."
        },
        "locations": {
          "type": "string",
          "description": "A comma-separated list of longitude, latitude pairs specifying a set of bounding boxes to filter tweets by. See https://dev.twitter.com/streaming/overview/request-parameters#locations for more details."
        },
        "oauth_token": {
          "type": "string",
          "required": true,
          "description": "Your oauth token."
        },
        "oauth_token_secret": {
          "type": "password",
          "required": true,
          "description": "Your oauth token secret."
        },
        "proxy_address": {
          "type": "string",
          "default": "127.0.0.1",
          "description": "Location of the proxy, by default the same machine as the one running this LS instance"
        },
        "proxy_port": {
          "type": "number",
          "default": "3128",
          "description": "Port where the proxy is listening, by default 3128 (squid)"
        },
        "rate_limit_reset_in": {
          "type": "number",
          "default": "300",
          "description": "Duration in seconds to wait before retrying a connection when twitter responds with a 429 TooManyRequests In some cases the 'x-rate-limit-reset' header is not set in the response and \u003cerror\u003e.rate_limit.reset_in is nil. If this occurs then we use the integer specified here. The default is 5 minutes."
        },
        "target": {
          "type": "field_reference",
          "description": "Defines a target field for placing fields. If this setting is omitted, data gets stored at the root (top level) of the event. The target is only relevant while decoding data into a new event."
        },
        "use_proxy": {
          "type": "boolean",
          "default": "false",
          "description": "When to use a proxy to handle the connections"
        },
        "use_samples": {
          "type": "boolean",
          "default": "false",
          "description": "Returns a small random sample of all public statuses. The tweets returned by the default access level are the same, so if two different clients connect to this endpoint, they will see the same tweets. If set to true, the keywords, follows, locations, and languages options will be ignored. Default =\u003e false"
        }
      }
    },
    "input/udp": {
      "description": "Read messages as events over the network via udp. The only required configuration item is `port`, which specifies the udp port logstash will listen on for event streams.",
      "options": {
        "buffer_size": {
          "type": "number",
          "default": "65536",
          "description": "The maximum packet size to read from the network"
        },
        "host": {
          "type": "string",
          "default": "0.0.0.0",
          "description": "The address which logstash will listen on."
        },
        "port": {
          "type": "number",
          "required": true,
          "description": "The port which logstash will listen on. Remember that ports less than 1024 (privileged ports) may require root or elevated privileges to use."
        },
        "queue_size": {
          "type": "number",
          "default": "2000",
          "description": "This is the number of unprocessed UDP packets you can hold in memory before packets will start dropping."
        },
        "receive_buffer_bytes": {
          "type": "number",
          "description": "The socket receive buffer size in bytes. If option is not set, the operating system default is used. The operating system will use the max allowed value if receive_buffer_bytes is larger than allowed. Consult your operating system documentation if you need to increase this max allowed value."
        },
        "source_ip_fieldname": {
          "type": "string",
          "description": "The name of the field where the source IP address will be stored"
        },
        "workers": {
          "type": "number",
          "default": "2",
          "description": "Number of threads processing packets"
        }
      }
    },
    "input/unix": {
      "description": "Read events over a UNIX socket.",
      "options": {
        "data_timeout": {
          "type": "number",
          "default": "-1",
          "description": "The 'read' timeout in seconds. If a particular connection is idle for more than this timeout period, we will assume it is dead and close it."
        },
        "force_unlink": {
          "type": "boolean",
          "default": "false",
          "description": "Remove socket file in case of EADDRINUSE failure"
        },
        "mode": {
          "type": "string, one of: server, client",
          "default": "server",
          "description": "Mode to operate in. `server` listens for client connections, `client` connects to a server."
        },
        "path": {
          "type": "string",
          "required": true,
          "description": "When mode is `server`, the path to listen on. When mode is `client`, the path to connect to."
        },
        "socket_not_present_retry_interval_seconds": {
          "type": "number",
          "required": true,
          "default": "5",
          "description": "Amount of time in seconds to wait if the socket file is not present, before retrying. Only positive values are allowed."
        }
      }
    },
    "output/cloudwatch": {
      "description": "This output lets you aggregate and send metric data to AWS CloudWatch",
      "options": {
        "batch_size": {
          "type": "number",
          "default": "20",
          "description": "How many data points can be given in one call to the CloudWatch API"
        },
        "dimensions": {
          "type": "hash",
          "description": "The default dimensions [ name, value, ... ] to use for events which do not have a `CW_dimensions` field"
        },
        "field_dimensions": {
          "type": "string",
          "default": "CW_dimensions",
          "description": "The name of the field used to set the dimensions on an event metric The field named here, if present in an event, must have an array of one or more key \u0026 value pairs, for example...     `add_field =\u003e [ \"CW_dimensions\", \"Environment\", \"CW_dimensions\", \"prod\" ]` or, equivalently...     `add_field =\u003e [ \"CW_dimensions\", \"Environment\" ]`     `add_field =\u003e [ \"CW_dimensions\", \"prod\" ]`"
        },
        "field_metricname": {
          "type": "string",
          "default": "CW_metricname",
          "description": "The name of the field used to set the metric name on an event The author of this plugin recommends adding this field to events in inputs \u0026 filters rather than using the per-output default setting so that one output plugin on your logstash indexer can serve all events (which of course had fields set on your logstash shippers.)"
        },
        "field_namespace": {
          "type": "string",
          "default": "CW_namespace",
          "description": "The name of the field used to set a different namespace per event Note: Only one namespace can be sent to CloudWatch per API call so setting different namespaces will increase the number of API calls and those cost money."
        },
        "field_unit": {
          "type": "string",
          "default": "CW_unit",
          "description": "The name of the field used to set the unit on an event metric"
        },
        "field_value": {
          "type": "string",
          "default": "CW_value",
          "description": "The name of the field used to set the value (float) on an event metric"
        },
        "metricname": {
          "type": "string",
          "description": "The default metric name to use for events which do not have a `CW_metricname` field. Beware: If this is provided then all events which pass through this output will be aggregated and sent to CloudWatch, so use this carefully.  Furthermore, when providing this option, you will probably want to also restrict events from passing through this output using event type, tag, and field matching"
        },
        "namespace": {
          "type": "string",
          "default": "Logstash",
          "description": "The default namespace to use for events which do not have a `CW_namespace` field"
        },
        "queue_size": {
          "type": "number",
          "default": "10000",
          "description": "How many events to queue before forcing a call to the CloudWatch API ahead of `timeframe` schedule Set this to the number of events-per-timeframe you will be sending to CloudWatch to avoid extra API calls"
        },
        "timeframe": {
          "type": "string",
          "default": "1m",
          "description": "How often to send data to CloudWatch This does not affect the event timestamps, events will always have their actual timestamp (to-the-minute) sent to CloudWatch."
        },
        "unit": {
          "default": "COUNT_UNIT",
          "description": "The default unit to use for events which do not have a `CW_unit` field If you set this option you should probably set the \"value\" option along with it"
        },
        "value": {
          "type": "string",
          "default": "1",
          "description": "The default value to use for events which do not have a `CW_value` field If provided, this must be a string which can be converted to a float, for example...     \"1\", \"2.34\", \".5\", and \"0.67\" If you set this option you should probably set the `unit` option along with it"
        }
      }
    },
    "output/csv": {
      "description": "CSV output.",
      "options": {
        "csv_options": {
          "type": "hash",
          "description": "Options for CSV output. This is passed directly to the Ruby stdlib to_csv function. Full documentation is available on the Ruby CSV documentation page. A typical use case would be to use alternative column or row seperators eg: `csv_options =\u003e {\"col_sep\" =\u003e \"\\t\" \"row_sep\" =\u003e \"\\r\\n\"}` gives tab seperated data with windows line endings"
        },
        "fields": {
          "type": "array",
          "required": true,
          "description": "The field names from the event that should be written to the CSV file. Fields are written to the CSV in the same order as the array. If a field does not exist on the event, an empty string will be written. Supports field reference syntax eg: `fields =\u003e [\"field1\", \"[nested][field]\"]`."
        },
        "spreadsheet_safe": {
          "type": "boolean",
          "default": "true",
          "description": "Option to not escape/munge string values. Please note turning off this option may not make the values safe in your spreadsheet application"
        }
      }
    },
    "output/elastic_app_search": {
      "options": {
        "api_key": {
          "type": "password",
          "required": true,
          "description": "The private API Key with write permissions. https://www.elastic.co/guide/en/app-search/current/authentication.html#authentication-api-keys"
        },
        "document_id": {
          "type": "string",
          "description": "The id for app search documents. This can be an interpolated value like `myapp-%{sequence_id}`. Reusing ids will cause documents to be rewritten."
        },
        "engine": {
          "type": "string",
          "required": true,
          "description": "The name of the search engine you created in App Search, an information repository that includes the indexed document records. The `engine` field supports {logstash-ref}/event-dependent-configuration.html#sprintf[sprintf format] to allow the engine name to be derived from a field value from each event, for example `engine-%{engine_name}`."
        },
        "timestamp_destination": {
          "type": "string",
          "description": "Where to move the value from the `@timestamp` field."
        },
        "url": {
          "type": "string",
          "required": true,
          "description": "The value of the API endpoint in the form of a URL."
        }
      }
    },
    "output/elastic_workplace_search": {
      "options": {
        "access_token": {
          "type": "password",
          "required": true,
          "description": "The source access token. Visit the source overview page in the Workplace Search dashboard to find the token associated with your source."
        },
        "document_id": {
          "type": "string",
          "description": "The id for workplace search documents. This can be an interpolated value like `myapp-%{sequence_id}`. Reusing ids will cause documents to be rewritten."
        },
        "source": {
          "type": "string",
          "required": true,
          "description": "The ID of the source you created in Workplace Search. The `source` field supports {logstash-ref}/event-dependent-configuration.html#sprintf[sprintf format] to allow the source ID to be derived from a field value from each event, for example `%{source_id}`."
        },
        "timestamp_destination": {
          "type": "string",
          "description": "Where to move the value from the `@timestamp` field."
        },
        "url": {
          "type": "string",
          "required": true,
          "description": "The value of the API endpoint in the form of a URL."
        }
      }
    },
    "output/elasticsearch": {
      "options": {
        "action": {
          "type": "string",
          "default": "index\" unless data_stream",
          "description": "The Elasticsearch action to perform. Valid actions are:"
        },
        "api_key": {
          "type": "password",
          "description": "Authenticate using Elasticsearch API key. format is id:api_key (as returned by Create API key)"
        },
        "bulk_path": {
          "type": "string",
          "description": "HTTP Path to perform the _bulk requests to this defaults to a concatenation of the path parameter and \"_bulk\""
        },
        "ca_trusted_fingerprint": {
          "description": "One or more hex-encoded SHA256 fingerprints to trust as Certificate Authorities"
        },
        "cacert": {
          "type": "path",
          "description": "The .cer or .pem file to validate the server's certificate",
          "deprecated": "Set "
        },
        "cloud_auth": {
          "type": "password",
          "description": "Cloud authentication string (\"\u003cusername\u003e:\u003cpassword\u003e\" format) is an alternative for the `user`/`password` configuration."
        },
        "cloud_id": {
          "type": "string",
          "description": "Cloud ID, from the Elastic Cloud web console. If set `hosts` should not be used."
        },
        "compression_level": {
          "type": "string, one of: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9",
          "default": "DEFAULT_ZIP_LEVEL }",
          "description": "Number `1` ~ `9` are the gzip compression level Set `0` to disable compression Set `1` (best speed) to `9` (best compression) to use compression"
        },
        "custom_headers": {
          "type": "hash",
          "default": "{} }",
          "description": "Custom Headers to send on each request to elasticsearch nodes"
        },
        "dlq_custom_codes": {
          "type": "list of number",
          "default": "[] }",
          "description": "List extra HTTP's error codes that are considered valid to move the events into the dead letter queue. It's considered a configuration error to re-use the same predefined codes for success, DLQ or conflict. The option accepts a list of natural numbers corresponding to HTTP errors codes."
        },
        "dlq_on_failed_indexname_interpolation": {
          "type": "boolean",
          "default": "true }",
          "description": "if enabled, failed index name interpolation events go into dead letter queue."
        },
        "doc_as_upsert": {
          "type": "boolean",
          "default": "false",
          "description": "Enable `doc_as_upsert` for update mode. Create a new document with source if `document_id` doesn't exist in Elasticsearch"
        },
        "document_id": {
          "type": "string",
          "description": "The document ID for the index. Useful for overwriting existing entries in Elasticsearch with the same ID."
        },
        "document_type": {
          "type": "string",
          "deprecated": "Document types are being deprecated in Elasticsearch 6.0, and removed entirely in 7.0. You should avoid this feature"
        },
        "failure_type_logging_whitelist": {
          "type": "array",
          "default": "[] }",
          "description": "Deprecated, refer to `silence_errors_in_log`."
        },
        "healthcheck_path": {
          "type": "string",
          "description": "HTTP Path where a HEAD request is sent when a backend is marked down the request is sent in the background to see if it has come back again before it is once again eligible to service requests. If you have custom firewall rules you may need to change this"
        },
        "hosts": {
          "type": "list of uri",
          "default": "[ DEFAULT_HOST ]",
          "description": "Sets the host(s) of the remote instance. If given an array it will load balance requests across the hosts specified in the `hosts` parameter. Remember the `http` protocol uses the http address (eg. 9200, not 9300).     `\"127.0.0.1\"`     `[\"127.0.0.1:9200\",\"127.0.0.2:9200\"]`     `[\"\"https://127.0.0.1:9200\"`     `[\"dedicated master nodes from the `hosts` list to prevent LS from sending bulk requests to the master nodes.  So this parameter should only reference either data or client nodes in Elasticsearch."
        },
        "http_compression": {
          "type": "boolean",
          "default": "true",
          "description": "Enable gzip compression on requests. Note that response compression is on by default for Elasticsearch v5.0 and beyond Set `true` to enable compression with level 1 Set `false` to disable compression with level 0",
          "deprecated": "Set "
        },
        "ilm_enabled": {
          "type": "string, one of: true, false, true, false, auto",
          "default": "auto"
        },
        "ilm_pattern": {
          "type": "string",
          "default": "{now/d}-000001",
          "description": "appends {now/d}-000001 by default for new index creation, subsequent rollover indices will increment based on this pattern i.e. 000002 {now/d} is date math, and will insert the appropriate value automatically."
        },
        "ilm_policy": {
          "type": "string",
          "default": "DEFAULT_POLICY",
          "description": "ILM policy to use, if undefined the default policy will be used."
        },
        "ilm_rollover_alias": {
          "type": "string",
          "description": "Rollover alias used for indexing data. If rollover alias doesn't exist, Logstash will create it and map it to the relevant index"
        },
        "index": {
          "type": "string",
          "description": "The index to write events to. This can be dynamic using the `%{foo}` syntax. The default value will partition your indices by day so you can more easily delete old data or only search specific date ranges. Indexes may not contain uppercase characters. For weekly indexes ISO 8601 format is recommended, eg. logstash-%{+xxxx.ww}. LS uses Joda to format the index pattern from event timestamp. Joda formats are defined here."
        },
        "join_field": {
          "type": "string",
          "default": "nil",
          "description": "For child documents, name of the join field"
        },
        "keystore": {
          "type": "path",
          "description": "The keystore used to present a certificate to the server. It can be either .jks or .p12",
          "deprecated": "Set "
        },
        "keystore_password": {
          "type": "password",
          "description": "Set the keystore password",
          "deprecated": "Set "
        },
        "manage_template": {
          "type": "boolean",
          "default": "true",
          "description": "From Logstash 1.3 onwards, a template is applied to Elasticsearch during Logstash's startup if one with the name `template_name` does not already exist. By default, the contents of this template is the default template for `logstash-%{+YYYY.MM.dd}` which always matches indices based on the pattern `logstash-*`.  Should you require support for other index names, or would like to change the mappings in the template in general, a custom template can be specified by setting `template` to the path of a template file."
        },
        "parameters": {
          "type": "hash",
          "description": "Pass a set of key value pairs as the URL query string. This query string is added to every host listed in the 'hosts' configuration. If the 'hosts' list contains urls that already have query strings, the one specified here will be appended."
        },
        "parent": {
          "type": "string",
          "default": "nil",
          "description": "For child documents, ID of the associated parent. This can be dynamic using the `%{foo}` syntax."
        },
        "password": {
          "type": "password",
          "description": "Password to authenticate to a secure Elasticsearch cluster"
        },
        "path": {
          "type": "string",
          "description": "HTTP Path at which the Elasticsearch server lives. Use this if you must run Elasticsearch behind a proxy that remaps the root path for the Elasticsearch HTTP API lives. Note that if you use paths as components of URLs in the 'hosts' field you may not also set this field. That will raise an error at startup"
        },
        "pipeline": {
          "type": "string",
          "default": "nil",
          "description": "Set which ingest pipeline you wish to execute for an event. You can also use event dependent configuration here like `pipeline =\u003e \"%{INGEST_PIPELINE}\"`"
        },
        "pool_max": {
          "type": "number",
          "default": "1000 }",
          "description": "While the output tries to reuse connections efficiently we have a maximum. This sets the maximum number of open connections the output will create. Setting this too low may mean frequently closing / opening connections which is bad."
        },
        "pool_max_per_route": {
          "type": "number",
          "default": "100 }",
          "description": "While the output tries to reuse connections efficiently we have a maximum per endpoint. This sets the maximum number of open connections per endpoint the output will create. Setting this too low may mean frequently closing / opening connections which is bad."
        },
        "proxy": {
          "type": "uri",
          "description": "Set the address of a forward HTTP proxy. This used to accept hashes as arguments but now only accepts arguments of the URI type to prevent leaking credentials."
        },
        "resurrect_delay": {
          "type": "number",
          "default": "5 }",
          "description": "How frequently, in seconds, to wait between resurrection attempts. Resurrection is the process by which backend endpoints marked 'down' are checked to see if they have come back to life"
        },
        "retry_initial_interval": {
          "type": "number",
          "default": "2 }",
          "description": "Set initial interval in seconds between bulk retries. Doubled on each retry up to `retry_max_interval`"
        },
        "retry_max_interval": {
          "type": "number",
          "default": "64 }",
          "description": "Set max interval in seconds between bulk retries."
        },
        "retry_on_conflict": {
          "type": "number",
          "default": "1",
          "description": "The number of times Elasticsearch should internally retry an update/upserted document See the partial updates for more info"
        },
        "routing": {
          "type": "string",
          "description": "A routing override to be applied to all processed events. This can be dynamic using the `%{foo}` syntax."
        },
        "script": {
          "type": "string",
          "description": "Set script name for scripted update mode"
        },
        "script_lang": {
          "type": "string",
          "default": "painless",
          "description": "Set the language of the used script. If not set, this defaults to painless in ES 5.0"
        },
        "script_type": {
          "type": "string, one of: inline, indexed, file",
          "default": "[\"inline\"]",
          "description": "Define the type of script referenced by \"script\" variable  inline : \"script\" contains inline script  indexed : \"script\" contains the name of script directly indexed in elasticsearch  file    : \"script\" contains the name of script stored in elasticseach's config directory"
        },
        "script_var_name": {
          "type": "string",
          "default": "event",
          "description": "Set variable name passed to script (scripted update)"
        },
        "scripted_upsert": {
          "type": "boolean",
          "default": "false",
          "description": "if enabled, script is in charge of creating non-existent document (scripted update)"
        },
        "silence_errors_in_log": {
          "type": "array",
          "default": "[] }",
          "description": "Defines the list of Elasticsearch errors that you don't want to log. A useful example is when you want to skip all 409 errors which are `version_conflict_engine_exception`. Deprecates `failure_type_logging_whitelist`."
        },
        "sniffing": {
          "type": "boolean",
          "default": "false }",
          "description": "This setting asks Elasticsearch for the list of all cluster nodes and adds them to the hosts list. Note: This will return ALL nodes with HTTP enabled (including master nodes!). If you use this with master nodes, you probably want to disable HTTP on them by setting `http.enabled` to false in their elasticsearch.yml. You can either use the `sniffing` option or manually enter multiple Elasticsearch hosts using the `hosts` parameter."
        },
        "sniffing_delay": {
          "type": "number",
          "default": "5 }",
          "description": "How long to wait, in seconds, between sniffing attempts"
        },
        "sniffing_path": {
          "type": "string",
          "description": "HTTP Path to be used for the sniffing requests the default value is computed by concatenating the path value and \"_nodes/http\" if sniffing_path is set it will be used as an absolute path do not use full URL here, only paths, e.g. \"/sniff/_nodes/http\""
        },
        "ssl": {
          "type": "boolean",
          "description": "Enable SSL/TLS secured communication to Elasticsearch cluster. Leaving this unspecified will use whatever scheme is specified in the URLs listed in 'hosts'. If no explicit protocol is specified plain HTTP will be used. If SSL is explicitly disabled here the plugin will refuse to start if an HTTPS URL is given in 'hosts'",
          "deprecated": "Set "
        },
        "ssl_certificate": {
          "type": "path",
          "description": "OpenSSL-style X.509 certificate certificate to authenticate the client"
        },
        "ssl_certificate_authorities": {
          "type": "list of path",
          "description": "The .cer or .pem files to validate the server's certificate"
        },
        "ssl_certificate_verification": {
          "type": "boolean",
          "default": "true",
          "description": "Option to validate the server's certificate. Disabling this severely compromises security. For more information on disabling certificate verification please read https://www.cs.utexas.edu/~shmat/shmat_ccs12.pdf",
          "deprecated": "Set "
        },
        "ssl_cipher_suites": {
          "type": "list of string",
          "description": "The list of cipher suites to use, listed by priorities. Supported cipher suites vary depending on which version of Java is used."
        },
        "ssl_enabled": {
          "type": "boolean",
          "description": "Enable SSL/TLS secured communication to Elasticsearch cluster. Leaving this unspecified will use whatever scheme is specified in the URLs listed in 'hosts'. If no explicit protocol is specified plain HTTP will be used. If SSL is explicitly disabled here the plugin will refuse to start if an HTTPS URL is given in 'hosts'"
        },
        "ssl_key": {
          "type": "path",
          "description": "OpenSSL-style RSA private key to authenticate the client"
        },
        "ssl_keystore_password": {
          "type": "password",
          "description": "Set the keystore password"
        },
        "ssl_keystore_path": {
          "type": "path",
          "description": "The keystore used to present a certificate to the server. It can be either .jks or .p12"
        },
        "ssl_keystore_type": {
          "type": "string, one of: pkcs12, jks",
          "description": "The format of the keystore file. It must be either jks or pkcs12"
        },
        "ssl_supported_protocols": {
          "type": "list of string, one of: TLSv1.1, TLSv1.2, TLSv1.3",
          "default": "[]"
        },
        "ssl_truststore_password": {
          "type": "password",
          "description": "Set the truststore password"
        },
        "ssl_truststore_path": {
          "type": "path",
          "description": "The JKS truststore to validate the server's certificate. Use either `:ssl_truststore_path` or `:ssl_certificate_authorities`"
        },
        "ssl_truststore_type": {
          "type": "string, one of: pkcs12, jks",
          "description": "The format of the truststore file. It must be either jks or pkcs12"
        },
        "ssl_verification_mode": {
          "type": "string, one of: full, none",
          "default": "full' }",
          "description": "Options to verify the server's certificate. \"full\": validates that the provided certificate has an issue date thats within the not_before and not_after dates; chains to a trusted Certificate Authority (CA); has a hostname or IP address that matches the names within the certificate. \"none\": performs no certificate validation. Disabling this severely compromises security (https://www.cs.utexas.edu/~shmat/shmat_ccs12.pdf)"
        },
        "template": {
          "type": "path",
          "description": "You can set the path to your own template here, if you so desire. If not set, the included template will be used."
        },
        "template_api": {
          "type": "string, one of: auto, legacy, composable",
          "default": "auto",
          "description": "Flag for enabling legacy template api for Elasticsearch 8 Default auto will use index template api for Elasticsearch 8 and use legacy api for 7 Set to legacy to use legacy template api"
        },
        "template_name": {
          "type": "string",
          "description": "This configuration option defines how the template is named inside Elasticsearch. Note that if you have used the template management features and subsequently change this, you will need to prune the old template manually, e.g."
        },
        "template_overwrite": {
          "type": "boolean",
          "default": "false",
          "description": "The template_overwrite option will always overwrite the indicated template in Elasticsearch with either the one indicated by template or the included one. This option is set to false by default. If you always want to stay up to date with the template provided by Logstash, this option could be very useful to you. Likewise, if you have your own template file managed by puppet, for example, and you wanted to be able to update it regularly, this option could help there as well."
        },
        "timeout": {
          "type": "number",
          "default": "60 }",
          "description": "Set the timeout, in seconds, for network operations and requests sent Elasticsearch. If a timeout occurs, the request will be retried."
        },
        "truststore": {
          "type": "path",
          "description": "The JKS truststore to validate the server's certificate. Use either `:truststore` or `:cacert`",
          "deprecated": "Set "
        },
        "truststore_password": {
          "type": "password",
          "description": "Set the truststore password",
          "deprecated": "Use "
        },
        "upsert": {
          "type": "string",
          "description": "Set upsert content for update mode.s Create a new document with this parameter as json string if `document_id` doesn't exists"
        },
        "user": {
          "type": "string",
          "description": "Username to authenticate to a secure Elasticsearch cluster"
        },
        "validate_after_inactivity": {
          "type": "number",
          "default": "10000 }",
          "description": "How long to wait before checking if the connection is stale before executing a request on a connection using keepalive. You may want to set this lower, if you get connection errors regularly Quoting the Apache commons docs (this client is based Apache Commmons): 'Defines period of inactivity in milliseconds after which persistent connections must be re-validated prior to being leased to the consumer. Non-positive value passed to this method disables connection validation. This check helps detect connections that have become stale (half-closed) while kept inactive in the pool.' See these docs for more info"
        },
        "version": {
          "type": "string",
          "description": "The version to use for indexing. Use sprintf syntax like `%{my_version}` to use a field value here. See https://www.elastic.co/blog/elasticsearch-versioning-support."
        },
        "version_type": {
          "type": "string, one of: internal, external, external_gt, external_gte, force",
          "description": "The version_type to use for indexing. See https://www.elastic.co/blog/elasticsearch-versioning-support. See also https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-index_.html#_version_types"
        }
      }
    },
    "output/email": {
      "description": "Send email when an output is received. Alternatively, you may include or exclude the email output execution using conditionals.",
      "options": {
        "address": {
          "type": "string",
          "default": "localhost",
          "description": "The address used to connect to the mail server"
        },
        "attachments": {
          "type": "array",
          "default": "[]",
          "description": "Attachments - specify the name(s) and location(s) of the files."
        },
        "authentication": {
          "type": "string",
          "description": "Authentication method used when identifying with the server"
        },
        "bcc": {
          "type": "string",
          "description": "Same as cc but in blind carbon"
        },
        "body": {
          "type": "string",
          "description": "Body for the email - plain text only."
        },
        "cc": {
          "type": "string",
          "description": "The fully-qualified email address(es) to include as cc: address(es)."
        },
        "contenttype": {
          "type": "string",
          "default": "text/html; charset=UTF-8",
          "description": "contenttype : for multipart messages, set the content-type and/or charset of the HTML part. NOTE: this may not be functional (KH)"
        },
        "debug": {
          "type": "boolean",
          "default": "false",
          "description": "Run the mail relay in debug mode"
        },
        "domain": {
          "type": "string",
          "default": "localhost",
          "description": "HELO/EHLO domain name"
        },
        "from": {
          "type": "string",
          "default": "logstash.alert@example.com",
          "description": "The fully-qualified email address for the From: field in the email."
        },
        "htmlbody": {
          "type": "string",
          "description": "HTML Body for the email, which may contain HTML markup."
        },
        "password": {
          "type": "password",
          "description": "Password to authenticate with the server"
        },
        "port": {
          "type": "number",
          "default": "25",
          "description": "Port used to communicate with the mail server"
        },
        "replyto": {
          "type": "string",
          "description": "The fully qualified email address for the Reply-To: field."
        },
        "subject": {
          "type": "string",
          "description": "Subject: for the email."
        },
        "template_file": {
          "type": "path",
          "description": "Email template file to be used - as mustache template."
        },
        "to": {
          "type": "string",
          "required": true,
          "description": "The fully-qualified email address to send the email to."
        },
        "use_tls": {
          "type": "boolean",
          "default": "false",
          "description": "Enables TLS when communicating with the server"
        },
        "username": {
          "type": "string",
          "description": "Username to authenticate with the server"
        },
        "via": {
          "type": "string",
          "default": "smtp",
          "description": "How Logstash should send the email, either via SMTP or by invoking sendmail."
        }
      }
    },
    "output/file": {
      "description": "This output writes events to files on disk. You can use fields from the event as parts of the filename and/or path.",
      "options": {
        "create_if_deleted": {
          "type": "boolean",
          "default": "true",
          "description": "If the configured file is deleted, but an event is handled by the plugin, the plugin will recreate the file. Default =\u003e true"
        },
        "dir_mode": {
          "type": "number",
          "default": "-1",
          "description": "Dir access mode to use. Note that due to the bug in jruby system umask is ignored on linux: https://github.com/jruby/jruby/issues/3426 Setting it to -1 uses default OS value. Example: `\"dir_mode\" =\u003e 0750`"
        },
        "file_mode": {
          "type": "number",
          "default": "-1",
          "description": "File access mode to use. Note that due to the bug in jruby system umask is ignored on linux: https://github.com/jruby/jruby/issues/3426 Setting it to -1 uses default OS value. Example: `\"file_mode\" =\u003e 0640`"
        },
        "filename_failure": {
          "type": "string",
          "default": "_filepath_failures",
          "description": "If the generated path is invalid, the events will be saved into this file and inside the defined path."
        },
        "flush_interval": {
          "type": "number",
          "default": "2",
          "description": "Flush interval (in seconds) for flushing writes to log files. 0 will flush on every message."
        },
        "gzip": {
          "type": "boolean",
          "default": "false",
          "description": "Gzip the output stream before writing to disk."
        },
        "path": {
          "type": "string",
          "required": true,
          "description": "The path to the file to write. Event fields can be used here, like `/var/log/logstash/%{host}/%{application}` One may also utilize the path option for date-based log rotation via the joda time format. This will use the event timestamp. E.g.: `path =\u003e \"./test-%{+YYYY-MM-dd}.txt\"` to create `./test-2013-05-29.txt`"
        },
        "stale_cleanup_interval": {
          "type": "number",
          "default": "10",
          "description": "How often should the stale files cleanup cycle run (in seconds). The stale files cleanup cycle closes inactive files (i.e files not written to since the last cycle)."
        },
        "write_behavior": {
          "type": "string, one of: overwrite, append",
          "default": "append",
          "description": "How should the file be written?"
        }
      }
    },
    "output/graphite": {
      "description": "This output allows you to pull metrics from your logs and ship them to Graphite. Graphite is an open source tool for storing and graphing metrics.",
      "options": {
        "exclude_metrics": {
          "type": "array",
          "default": "[ \"%\\{[^}]+\\}\" ]",
          "description": "Exclude regex matched metric names, by default exclude unresolved %{field} strings."
        },
        "fields_are_metrics": {
          "type": "boolean",
          "default": "false",
          "description": "An array indicating that these event fields should be treated as metrics and will be sent verbatim to Graphite. You may use either `fields_are_metrics` or `metrics`, but not both."
        },
        "host": {
          "type": "string",
          "default": "localhost",
          "description": "The hostname or IP address of the Graphite server."
        },
        "include_metrics": {
          "type": "array",
          "default": "[ \".*\" ]",
          "description": "Include only regex matched metric names."
        },
        "metrics": {
          "type": "hash",
          "default": "{}",
          "description": "The metric(s) to use. This supports dynamic strings like %{host} for metric names and also for values. This is a hash field with key being the metric name, value being the metric value. Example:"
        },
        "metrics_format": {
          "type": "string",
          "default": "DEFAULT_METRICS_FORMAT",
          "description": "Defines the format of the metric string. The placeholder '*' will be replaced with the name of the actual metric."
        },
        "nested_object_separator": {
          "type": "string",
          "default": ".",
          "description": "When hashes are passed in as values they are broken out into a dotted notation For instance if you configure this plugin with # [source,ruby]     metrics =\u003e \"mymetrics\""
        },
        "port": {
          "type": "number",
          "default": "2003",
          "description": "The port to connect to on the Graphite server."
        },
        "reconnect_interval": {
          "type": "number",
          "default": "2",
          "description": "Interval between reconnect attempts to Carbon."
        },
        "resend_on_failure": {
          "type": "boolean",
          "default": "false",
          "description": "Should metrics be resent on failure?"
        },
        "timestamp_field": {
          "type": "string",
          "default": "@timestamp",
          "description": "Use this field for the timestamp instead of '@timestamp' which is the default. Useful when backfilling or just getting more accurate data into graphite since you probably have a cache layer infront of Logstash."
        }
      }
    },
    "output/http": {
      "options": {
        "content_type": {
          "type": "string",
          "description": "Content type"
        },
        "format": {
          "type": "string, one of: json, json_batch, form, message",
          "default": "json",
          "description": "Set the format of the http body."
        },
        "headers": {
          "type": "hash",
          "default": "{}",
          "description": "Custom headers to use format is `headers =\u003e [\"X-My-Header\", \"%{host}\"]`"
        },
        "http_compression": {
          "type": "boolean",
          "default": "false",
          "description": "Set this to true if you want to enable gzip compression for your http requests"
        },
        "http_method": {
          "description": "The HTTP Verb. One of \"put\", \"post\", \"patch\", \"delete\", \"get\", \"head\""
        },
        "ignorable_codes": {
          "type": "list of number",
          "description": "If you would like to consider some non-2xx codes to be successes enumerate them here. Responses returning these codes will be considered successes"
        },
        "mapping": {
          "type": "hash",
          "description": "This lets you choose the structure and parts of the event that are sent."
        },
        "message": {
          "type": "string"
        },
        "retry_failed": {
          "type": "boolean",
          "default": "true",
          "description": "Set this to false if you don't want this output to retry failed requests"
        },
        "retryable_codes": {
          "type": "list of number",
          "default": "[429, 500, 502, 503, 504]",
          "description": "If encountered as response codes this plugin will retry these requests"
        },
        "url": {
          "type": "string",
          "description": "URL to use"
        }
      }
    },
    "output/kafka": {
      "description": "Write events to a Kafka topic. This uses the Kafka Producer API to write messages to a topic on the broker.",
      "options": {
        "acks": {
          "type": "string, one of: 0, 1, all",
          "default": "1",
          "description": "The number of acknowledgments the producer requires the leader to have received before considering a request complete."
        },
        "batch_size": {
          "type": "number",
          "default": "16_384 # Kafka default",
          "description": "The producer will attempt to batch records together into fewer requests whenever multiple records are being sent to the same partition. This helps performance on both the client and the server. This configuration controls the default batch size in bytes."
        },
        "bootstrap_servers": {
          "type": "string",
          "default": "localhost:9092",
          "description": "This is for bootstrapping and the producer will only use it for getting metadata (topics, partitions and replicas). The socket connections for sending the actual data will be established based on the broker information returned in the metadata. The format is `host1:port1,host2:port2`, and the list can be a subset of brokers or a VIP pointing to a subset of brokers."
        },
        "buffer_memory": {
          "type": "number",
          "default": "33_554_432 # (32M) Kafka default",
          "description": "The total bytes of memory the producer can use to buffer records waiting to be sent to the server."
        },
        "client_dns_lookup": {
          "type": "string, one of: default, use_all_dns_ips, resolve_canonical_bootstrap_servers_only",
          "default": "use_all_dns_ips",
          "description": "How DNS lookups should be done. If set to `use_all_dns_ips`, when the lookup returns multiple IP addresses for a hostname, they will all be attempted to connect to before failing the connection. If the value is `resolve_canonical_bootstrap_servers_only` each entry will be resolved and expanded into a list of canonical names. Starting from Kafka 3 `default` value for `client.dns.lookup` value has been removed. If explicitly configured it fallbacks to `use_all_dns_ips`."
        },
        "client_id": {
          "type": "string",
          "default": "logstash",
          "description": "The id string to pass to the server when making requests. The purpose of this is to be able to track the source of requests beyond just ip/port by allowing a logical application name to be included with the request"
        },
        "compression_type": {
          "type": "string, one of: none, gzip, snappy, lz4, zstd",
          "default": "none",
          "description": "The compression type for all data generated by the producer. The default is none (i.e. no compression). Valid values are none, gzip, snappy, lz4 or zstd."
        },
        "jaas_path": {
          "type": "path",
          "description": "The Java Authentication and Authorization Service (JAAS) API supplies user authentication and authorization services for Kafka. This setting provides the path to the JAAS file. Sample JAAS file for Kafka client:"
        },
        "kerberos_config": {
          "type": "path",
          "description": "Optional path to kerberos config file. This is krb5.conf style as detailed in https://web.mit.edu/kerberos/krb5-1.12/doc/admin/conf_files/krb5_conf.html"
        },
        "key_serializer": {
          "type": "string",
          "default": "org.apache.kafka.common.serialization.StringSerializer",
          "description": "Serializer class for the key of the message"
        },
        "linger_ms": {
          "type": "number",
          "default": "0 # Kafka default",
          "description": "The producer groups together any records that arrive in between request transmissions into a single batched request. Normally this occurs only under load when records arrive faster than they can be sent out. However in some circumstances the client may want to reduce the number of requests even under moderate load. This setting accomplishes this by adding a small amount of artificial delaythat is, rather than immediately sending out a record the producer will wait for up to the given delay to allow other records to be sent so that the sends can be batched together."
        },
        "max_request_size": {
          "type": "number",
          "default": "1_048_576 # (1MB) Kafka default",
          "description": "The maximum size of a request"
        },
        "message_headers": {
          "type": "hash",
          "default": "{}",
          "description": "Headers added to kafka message in the form of key-value pairs"
        },
        "message_key": {
          "type": "string",
          "description": "The key for the message"
        },
        "metadata_fetch_timeout_ms": {
          "type": "number",
          "default": "60_000",
          "description": "the timeout setting for initial metadata request to fetch topic metadata."
        },
        "partitioner": {
          "type": "string",
          "description": "Partitioner to use - can be `default`, `uniform_sticky`, `round_robin` or a fully qualified class name of a custom partitioner."
        },
        "receive_buffer_bytes": {
          "type": "number",
          "default": "32_768 # (32KB) Kafka default",
          "description": "The size of the TCP receive buffer to use when reading data"
        },
        "reconnect_backoff_ms": {
          "type": "number",
          "default": "50 # Kafka default",
          "description": "The amount of time to wait before attempting to reconnect to a given host when a connection fails."
        },
        "retries": {
          "type": "number",
          "description": "The default retry behavior is to retry until successful. To prevent data loss, the use of this setting is discouraged."
        },
        "retry_backoff_ms": {
          "type": "number",
          "default": "100 # Kafka default",
          "description": "The amount of time to wait before attempting to retry a failed produce request to a given topic partition."
        },
        "sasl_client_callback_handler_class": {
          "type": "string",
          "description": "SASL client callback handler class"
        },
        "sasl_jaas_config": {
          "type": "string",
          "description": "JAAS configuration settings. This allows JAAS config to be a part of the plugin configuration and allows for different JAAS configuration per each plugin config."
        },
        "sasl_kerberos_service_name": {
          "type": "string",
          "description": "The Kerberos principal name that Kafka broker runs as. This can be defined either in Kafka's JAAS config or in Kafka's config."
        },
        "sasl_mechanism": {
          "type": "string",
          "default": "GSSAPI",
          "description": "SASL mechanism used for client connections. This may be any mechanism for which a security provider is available. GSSAPI is the default mechanism."
        },
        "security_protocol": {
          "type": "string, one of: PLAINTEXT, SSL, SASL_PLAINTEXT, SASL_SSL",
          "default": "PLAINTEXT",
          "description": "Security protocol to use, which can be either of PLAINTEXT,SSL,SASL_PLAINTEXT,SASL_SSL"
        },
        "send_buffer_bytes": {
          "type": "number",
          "default": "131_072 # (128KB) Kafka default",
          "description": "The size of the TCP send buffer to use when sending data."
        },
        "ssl_endpoint_identification_algorithm": {
          "type": "string",
          "default": "https",
          "description": "Algorithm to use when verifying host. Set to \"\" to disable"
        },
        "ssl_key_password": {
          "type": "password",
          "description": "The password of the private key in the key store file."
        },
        "ssl_keystore_location": {
          "type": "path",
          "description": "If client authentication is required, this setting stores the keystore path."
        },
        "ssl_keystore_password": {
          "type": "password",
          "description": "If client authentication is required, this setting stores the keystore password"
        },
        "ssl_keystore_type": {
          "type": "string",
          "description": "The keystore type."
        },
        "ssl_truststore_location": {
          "type": "path",
          "description": "The JKS truststore path to validate the Kafka broker's certificate."
        },
        "ssl_truststore_password": {
          "type": "password",
          "description": "The truststore password"
        },
        "ssl_truststore_type": {
          "type": "string",
          "description": "The truststore type."
        },
        "topic_id": {
          "type": "string",
          "required": true,
          "description": "The topic to produce messages to"
        },
        "value_serializer": {
          "type": "string",
          "default": "org.apache.kafka.common.serialization.StringSerializer",
          "description": "Serializer class for the value of the message"
        }
      }
    },
    "output/logstash": {
      "options": {
        "hosts": {
          "type": "list of required_host_optional_port",
          "required": true,
          "description": "Sets the host of the downstream Logstash instance. Host can be any of IPv4, IPv6 (requires to be in enclosed bracket) or host name, the forms:     `\"127.0.0.1\"` or `[\"127.0.0.1\"]` if single host with default port     `\"127.0.0.1:9801\"` or `[\"127.0.0.1:9801\"]` if single host with custom port     `[\"foo-bar.com\", \"foo-bar.com:9800\"]`     `[\"[::1]\", \"[::1]:9000\"]`     `\"[2001:0db8:85a3:0000:0000:8a2e:0370:7334]\"`"
        },
        "ssl_enabled": {
          "type": "boolean",
          "default": "true"
        },
        "user": {
          "type": "string",
          "deprecated": "Use `username` instead."
        },
        "username": {
          "type": "string"
        }
      }
    },
    "output/lumberjack": {
      "options": {
        "flush_size": {
          "type": "number",
          "default": "1024",
          "description": "To make efficient calls to the lumberjack output we are buffering events locally. if the number of events exceed the number the declared `flush_size` we will send them to the logstash server."
        },
        "hosts": {
          "type": "array",
          "required": true,
          "description": "list of addresses lumberjack can send to"
        },
        "idle_flush_time": {
          "type": "number",
          "default": "1",
          "description": "The amount of time since last flush before a flush is forced."
        },
        "port": {
          "type": "number",
          "required": true,
          "description": "the port to connect to"
        },
        "ssl_certificate": {
          "type": "path",
          "required": true,
          "description": "ssl certificate to use"
        }
      }
    },
    "output/nagios": {
      "description": "The Nagios output is used for sending passive check results to Nagios via the Nagios command file. This output currently supports Nagios 3.",
      "options": {
        "commandfile": {
          "default": "/var/lib/nagios3/rw/nagios.cmd",
          "description": "The full path to your Nagios command file."
        },
        "nagios_level": {
          "type": "string, one of: 0, 1, 2, 3",
          "default": "2",
          "description": "The Nagios check level. Should be one of 0=OK, 1=WARNING, 2=CRITICAL, 3=UNKNOWN. Defaults to 2 - CRITICAL."
        }
      }
    },
    "output/null": {
      "description": "A null output. This is useful for testing logstash inputs and filters for performance."
    },
    "output/pipe": {
      "description": "Pipe output.",
      "options": {
        "command": {
          "type": "string",
          "required": true,
          "description": "Command line to launch and pipe to"
        },
        "message_format": {
          "type": "string",
          "description": "The format to use when writing events to the pipe. This value supports any string and can include `%{name}` and other dynamic strings."
        },
        "ttl": {
          "type": "number",
          "default": "10",
          "description": "Close pipe that hasn't been used for TTL seconds. -1 or 0 means never close."
        }
      }
    },
    "output/rabbitmq": {
      "options": {
        "durable": {
          "type": "boolean",
          "default": "true",
          "description": "Is this exchange durable? (aka; Should it survive a broker restart?)"
        },
        "exchange": {
          "type": "string",
          "required": true,
          "description": "The name of the exchange"
        },
        "exchange_type": {
          "required": true,
          "description": "The exchange type (fanout, topic, direct)"
        },
        "key": {
          "type": "string",
          "default": "logstash",
          "description": "Key to route to by default. Defaults to 'logstash'"
        },
        "message_properties": {
          "type": "hash",
          "default": "{}",
          "description": "Properties to be passed along with the message"
        },
        "persistent": {
          "type": "boolean",
          "default": "true",
          "description": "Should RabbitMQ persist messages to disk?"
        }
      }
    },
    "output/redis": {
      "description": "This output will send events to a Redis queue using RPUSH. The RPUSH command is supported in Redis v0.0.7+. Using PUBLISH to a channel requires at least v1.3.8+. While you may be able to make these Redis versions work, the best performance and stability will be found in more recent stable versions.  Versions 2.6.0+ are recommended.",
      "options": {
        "batch": {
          "type": "boolean",
          "default": "false",
          "description": "Set to true if you want Redis to batch up values and send 1 RPUSH command instead of one command per value to push on the list.  Note that this only works with `data_type=\"list\"` mode right now."
        },
        "batch_events": {
          "type": "number",
          "default": "50",
          "description": "If batch is set to true, the number of events we queue up for an RPUSH."
        },
        "batch_timeout": {
          "type": "number",
          "default": "5",
          "description": "If batch is set to true, the maximum amount of time between RPUSH commands when there are pending events to flush."
        },
        "congestion_interval": {
          "type": "number",
          "default": "1",
          "description": "How often to check for congestion. Default is one second. Zero means to check on every event."
        },
        "congestion_threshold": {
          "type": "number",
          "default": "0",
          "description": "In case Redis `data_type` is `list` and has more than `@congestion_threshold` items, block until someone consumes them and reduces congestion, otherwise if there are no consumers Redis will run out of memory, unless it was configured with OOM protection. But even with OOM protection, a single Redis list can block all other users of Redis, until Redis CPU consumption reaches the max allowed RAM size. A default value of 0 means that this limit is disabled. Only supported for `list` Redis `data_type`."
        },
        "data_type": {
          "type": "string, one of: list, channel",
          "required": true,
          "description": "Either list or channel.  If `redis_type` is list, then we will set RPUSH to key. If `redis_type` is channel, then we will PUBLISH to `key`."
        },
        "db": {
          "type": "number",
          "default": "0",
          "description": "The Redis database number."
        },
        "host": {
          "type": "array",
          "default": "[\"127.0.0.1\"]",
          "description": "The hostname(s) of your Redis server(s). Ports may be specified on any hostname, which will override the global port config. If the hosts list is an array, Logstash will pick one random host to connect to, if that host is disconnected it will then pick another."
        },
        "key": {
          "type": "string",
          "required": true,
          "description": "The name of a Redis list or channel. Dynamic names are valid here, for example `logstash-%{type}`."
        },
        "password": {
          "type": "password",
          "description": "Password to authenticate with.  There is no authentication by default."
        },
        "port": {
          "type": "number",
          "default": "6379",
          "description": "The default port to connect on. Can be overridden on any hostname."
        },
        "reconnect_interval": {
          "type": "number",
          "default": "1",
          "description": "Interval for reconnecting to failed Redis connections"
        },
        "shuffle_hosts": {
          "type": "boolean",
          "default": "true",
          "description": "Shuffle the host list during Logstash startup."
        },
        "ssl_certificate": {
          "type": "path",
          "description": "SSL certificate path"
        },
        "ssl_certificate_authorities": {
          "type": "list of path",
          "description": "Validate the certificate chain against these authorities. You can define multiple files. All the certificates will be read and added to the trust store."
        },
        "ssl_cipher_suites": {
          "type": "list of string",
          "description": "The list of ciphers suite to use"
        },
        "ssl_enabled": {
          "type": "boolean",
          "default": "false",
          "description": "SSL"
        },
        "ssl_key": {
          "type": "path",
          "description": "SSL key path"
        },
        "ssl_key_passphrase": {
          "type": "password",
          "default": "nil",
          "description": "SSL key passphrase"
        },
        "ssl_supported_protocols": {
          "type": "list of string, one of: TLSv1.1, TLSv1.2, TLSv1.3",
          "default": "[]",
          "description": "NOTE: the default setting [] uses SSL engine defaults"
        },
        "ssl_verification_mode": {
          "type": "string, one of: full, none",
          "default": "full",
          "description": "Options to verify the server's certificate. \"full\": validates that the provided certificate has an issue date thats within the not_before and not_after dates; chains to a trusted Certificate Authority (CA); has a hostname or IP address that matches the names within the certificate. \"none\": performs no certificate validation. Disabling this severely compromises security (https://www.cs.utexas.edu/~shmat/shmat_ccs12.pdf)"
        },
        "timeout": {
          "type": "number",
          "default": "5",
          "description": "Redis initial connection timeout in seconds."
        }
      }
    },
    "output/s3": {
      "description": "INFORMATION:",
      "options": {
        "additional_settings": {
          "type": "hash",
          "default": "{}"
        },
        "bucket": {
          "type": "string",
          "required": true,
          "description": "S3 bucket"
        },
        "canned_acl": {
          "type": "string, one of: private, public-read, public-read-write, authenticated-read, aws-exec-read, bucket-owner-read, bucket-owner-full-control, log-delivery-write",
          "default": "private",
          "description": "The S3 canned ACL to use when putting the file. Defaults to \"private\"."
        },
        "encoding": {
          "type": "string, one of: none, GZIP_ENCODING",
          "default": "none",
          "description": "Specify the content encoding. Supports (\"gzip\"). Defaults to \"none\""
        },
        "prefix": {
          "type": "string",
          "description": "Specify a prefix to the uploaded filename, this can simulate directories on S3.  Prefix does not require leading slash. This option support string interpolation, be warned this can created a lot of temporary local files."
        },
        "restore": {
          "type": "boolean",
          "default": "true",
          "description": "If `restore =\u003e false` is specified and Logstash crashes, the unprocessed files are not sent into the bucket."
        },
        "retry_count": {
          "type": "number",
          "description": "The number of times to retry a failed S3 upload."
        },
        "retry_delay": {
          "type": "number",
          "default": "1",
          "description": "The amount of time to wait in seconds before attempting to retry a failed upload."
        },
        "rotation_strategy": {
          "type": "string, one of: size_and_time, size, time",
          "default": "size_and_time",
          "description": "Define the strategy to use to decide when we need to rotate the file and push it to S3, The default strategy is to check for both size and time, the first one to match will rotate the file."
        },
        "server_side_encryption": {
          "type": "boolean",
          "default": "false",
          "description": "Specifies whether or not to use S3's server side encryption. Defaults to no encryption."
        },
        "server_side_encryption_algorithm": {
          "type": "string, one of: AES256, aws:kms",
          "default": "AES256",
          "description": "Specifies what type of encryption to use when SSE is enabled."
        },
        "signature_version": {
          "type": "string, one of: v2, v4",
          "description": "The version of the S3 signature hash to use. Normally uses the internal client default, can be explicitly specified here"
        },
        "size_file": {
          "type": "number",
          "default": "1024 * 1024 * 5",
          "description": "Set the size of file in bytes, this means that files on bucket when have dimension \u003e file_size, they are stored in two or more file. If you have tags then it will generate a specific size file for every tags"
        },
        "ssekms_key_id": {
          "type": "string",
          "description": "The key to use when specified along with server_side_encryption =\u003e aws:kms. If server_side_encryption =\u003e aws:kms is set but this is not default KMS key is used. http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html"
        },
        "storage_class": {
          "type": "string, one of: STANDARD, REDUCED_REDUNDANCY, STANDARD_IA, ONEZONE_IA, INTELLIGENT_TIERING, GLACIER, DEEP_ARCHIVE, OUTPOSTS, GLACIER_IR, SNOW, EXPRESS_ONEZONE",
          "default": "STANDARD",
          "description": "Specifies what S3 storage class to use when uploading the file. More information about the different storage classes can be found: http://docs.aws.amazon.com/AmazonS3/latest/dev/storage-class-intro.html Defaults to STANDARD."
        },
        "tags": {
          "type": "array",
          "default": "[]",
          "description": "Define tags to be appended to the file on the S3 bucket."
        },
        "temporary_directory": {
          "type": "string",
          "default": "File.join(Dir.tmpdir, \"logstash\")",
          "description": "Set the directory where logstash will store the tmp files before sending it to S3 default to the current OS temporary directory in linux /tmp/logstash"
        },
        "time_file": {
          "type": "number",
          "default": "15",
          "description": "Set the time, in MINUTES, to close the current sub_time_section of bucket. If you also define file_size you have a number of files related to the section and the current tag. If it's valued 0 and rotation_strategy is 'time' or 'size_and_time' then the plugin raises a configuration error."
        },
        "upload_multipart_threshold": {
          "type": "number",
          "default": "15 * 1024 * 1024",
          "description": "Files larger than this number are uploaded using the S3 multipart APIs. Default threshold is 15MB."
        },
        "upload_queue_size": {
          "type": "number",
          "default": "2 * (Concurrent.processor_count * 0.25).ceil",
          "description": "Number of items we can keep in the local queue before uploading them"
        },
        "upload_workers_count": {
          "type": "number",
          "default": "(Concurrent.processor_count * 0.5).ceil",
          "description": "Specify how many workers to use to upload the files to S3"
        },
        "validate_credentials_on_root_bucket": {
          "type": "boolean",
          "default": "true",
          "description": "The common use case is to define permission on the root bucket and give Logstash full access to write its logs. In some circumstances you need finer grained permission on sub folder, this allow you to disable the check at startup."
        }
      }
    },
    "output/sns": {
      "description": "SNS output.",
      "options": {
        "arn": {
          "type": "string",
          "description": "Optional ARN to send messages to. If you do not set this you must include the `sns` field in your events to set the ARN on a per-message basis!"
        },
        "publish_boot_message_arn": {
          "type": "string",
          "description": "When an ARN for an SNS topic is specified here, the message \"Logstash successfully booted\" will be sent to it when this plugin is registered."
        }
      }
    },
    "output/sqs": {
      "description": "Push events to an Amazon Web Services (AWS) Simple Queue Service (SQS) queue.",
      "options": {
        "batch_events": {
          "type": "number",
          "default": "10",
          "description": "The number of events to be sent in each batch. Set this to `1` to disable the batch sending of messages."
        },
        "message_max_size": {
          "type": "bytes",
          "default": "256KiB",
          "description": "The maximum number of bytes for any message sent to SQS. Messages exceeding this size will be dropped. See http://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/limits-messages.html."
        },
        "queue": {
          "type": "string",
          "required": true,
          "description": "The name of the target SQS queue. Note that this is just the name of the queue, not the URL or ARN."
        },
        "queue_owner_aws_account_id": {
          "type": "string",
          "description": "Account ID of the AWS account which owns the queue. Note IAM permissions need to be configured on both accounts to function."
        }
      }
    },
    "output/stdout": {
      "description": "A simple output which prints to the STDOUT of the shell running Logstash. This output can be quite convenient when debugging plugin configurations, by allowing instant access to the event data after it has passed through the inputs and filters."
    },
    "output/tcp": {
      "description": "Write events over a TCP socket.",
      "options": {
        "host": {
          "type": "string",
          "required": true,
          "description": "When mode is `server`, the address to listen on. When mode is `client`, the address to connect to."
        },
        "mode": {
          "type": "string, one of: server, client",
          "default": "client",
          "description": "Mode to operate in. `server` listens for client connections, `client` connects to a server."
        },
        "port": {
          "type": "number",
          "required": true,
          "description": "When mode is `server`, the port to listen on. When mode is `client`, the port to connect to."
        },
        "reconnect_interval": {
          "type": "number",
          "default": "10",
          "description": "When connect failed,retry interval in sec."
        },
        "ssl_cacert": {
          "type": "path",
          "description": "The SSL CA certificate, chainfile or CA path. The system CA path is automatically included.",
          "deprecated": "Use "
        },
        "ssl_cert": {
          "type": "path",
          "description": "SSL certificate path",
          "deprecated": "Use "
        },
        "ssl_certificate": {
          "type": "path",
          "description": "SSL certificate path"
        },
        "ssl_certificate_authorities": {
          "type": "list of path",
          "description": "Validate client certificate or certificate chain against these authorities. You can define multiple files. All the certificates will be read and added to the trust store."
        },
        "ssl_cipher_suites": {
          "type": "list of string",
          "description": "The list of ciphers suite to use"
        },
        "ssl_client_authentication": {
          "type": "string, one of: none, optional, required",
          "default": "none",
          "description": "Controls the servers behavior in regard to requesting a certificate from client connections. `none`: No client authentication `optional`: Requests a client certificate but the client is not required to present one. `required`: Forces a client to present a certificate. This option needs to be used with `ssl_certificate_authorities` and a defined list of CAs."
        },
        "ssl_enable": {
          "type": "boolean",
          "default": "false",
          "description": "Enable SSL (must be set for other `ssl_` options to take effect).",
          "deprecated": "Use "
        },
        "ssl_enabled": {
          "type": "boolean",
          "default": "false",
          "description": "Enable SSL (must be set for other `ssl_` options to take effect)."
        },
        "ssl_key": {
          "type": "path",
          "description": "SSL key path"
        },
        "ssl_key_passphrase": {
          "type": "password",
          "default": "nil",
          "description": "SSL key passphrase"
        },
        "ssl_supported_protocols": {
          "type": "list of string, one of: TLSv1.1, TLSv1.2, TLSv1.3",
          "default": "[]",
          "description": "NOTE: the default setting [] uses SSL engine defaults"
        },
        "ssl_verification_mode": {
          "type": "string, one of: full, none",
          "default": "full",
          "description": "Options to verify the server's certificate. \"full\": validates that the provided certificate has an issue date thats within the not_before and not_after dates; chains to a trusted Certificate Authority (CA); has a hostname or IP address that matches the names within the certificate. \"certificate\": Validates the provided certificate and verifies that its signed by a trusted authority (CA), but doest check the certificate hostname. \"none\": performs no certificate validation. Disabling this severely compromises security (https://www.cs.utexas.edu/~shmat/shmat_ccs12.pdf)"
        },
        "ssl_verify": {
          "type": "boolean",
          "default": "false",
          "description": "Verify the identity of the other end of the SSL connection against the CA. For input, sets the field `sslsubject` to that of the client certificate.",
          "deprecated": "Use "
        }
      }
    },
    "output/udp": {
      "description": "Send events over UDP",
      "options": {
        "host": {
          "type": "string",
          "required": true,
          "description": "The address to send messages to"
        },
        "port": {
          "type": "string",
          "required": true,
          "description": "The port to send messages on"
        },
        "retry_backoff_ms": {
          "type": "number",
          "default": "100",
          "description": "The amount of time to wait in milliseconds before attempting to retry a failed UPD socket write"
        },
        "retry_count": {
          "type": "number",
          "default": "0",
          "description": "The number of times to retry a failed UPD socket write"
        }
      }
    },
    "output/webhdfs": {
      "description": "This plugin sends Logstash events into files in HDFS via the webhdfs REST API.",
      "options": {
        "compression": {
          "type": "string, one of: none, snappy, gzip",
          "default": "none",
          "description": "Compress output. One of ['none', 'snappy', 'gzip']"
        },
        "flush_size": {
          "type": "number",
          "default": "500",
          "description": "Sending data to webhdfs if event count is above, even if `store_interval_in_secs` is not reached."
        },
        "host": {
          "type": "string",
          "required": true,
          "description": "The server name for webhdfs/httpfs connections."
        },
        "idle_flush_time": {
          "type": "number",
          "default": "1",
          "description": "Sending data to webhdfs in x seconds intervals."
        },
        "kerberos_keytab": {
          "type": "string",
          "description": "Set kerberos keytab file. Note that the gssapi library needs to be available to use this."
        },
        "open_timeout": {
          "type": "number",
          "default": "30",
          "description": "WebHdfs open timeout, default 30s."
        },
        "path": {
          "type": "string",
          "required": true,
          "description": "The path to the file to write to. Event fields can be used here, as well as date fields in the joda time format, e.g.: `/user/logstash/dt=%{+YYYY-MM-dd}/%{@source_host}-%{+HH}.log`"
        },
        "port": {
          "type": "number",
          "default": "50070",
          "description": "The server port for webhdfs/httpfs connections."
        },
        "read_timeout": {
          "type": "number",
          "default": "30",
          "description": "The WebHdfs read timeout, default 30s."
        },
        "retry_interval": {
          "type": "number",
          "default": "0.5",
          "description": "How long should we wait between retries."
        },
        "retry_known_errors": {
          "type": "boolean",
          "default": "true",
          "description": "Retry some known webhdfs errors. These may be caused by race conditions when appending to same file, etc."
        },
        "retry_times": {
          "type": "number",
          "default": "5",
          "description": "How many times should we retry. If retry_times is exceeded, an error will be logged and the event will be discarded."
        },
        "single_file_per_thread": {
          "type": "boolean",
          "default": "false",
          "description": "Avoid appending to same file in multiple threads. This solves some problems with multiple logstash output threads and locked file leases in webhdfs. If this option is set to true, %{[@metadata][thread_id]} needs to be used in path config settting."
        },
        "snappy_bufsize": {
          "type": "number",
          "default": "32768",
          "description": "Set snappy chunksize. Only neccessary for stream format. Defaults to 32k. Max is 65536 @see http://code.google.com/p/snappy/source/browse/trunk/framing_format.txt"
        },
        "snappy_format": {
          "type": "string, one of: stream, file",
          "default": "stream",
          "description": "Set snappy format. One of \"stream\", \"file\". Set to stream to be hive compatible."
        },
        "ssl_cert": {
          "type": "string",
          "description": "Set ssl cert file."
        },
        "ssl_key": {
          "type": "string",
          "description": "Set ssl key file."
        },
        "standby_host": {
          "type": "string",
          "default": "false",
          "description": "Standby namenode for ha hdfs."
        },
        "standby_port": {
          "type": "number",
          "default": "50070",
          "description": "Standby namenode port for ha hdfs."
        },
        "use_httpfs": {
          "type": "boolean",
          "default": "false",
          "description": "Use httpfs mode if set to true, else webhdfs."
        },
        "use_kerberos_auth": {
          "type": "boolean",
          "default": "false",
          "description": "Set kerberos authentication."
        },
        "use_ssl_auth": {
          "type": "boolean",
          "default": "false",
          "description": "Set ssl authentication. Note that the openssl library needs to be available to use this."
        },
        "user": {
          "type": "string",
          "required": true,
          "description": "The Username for webhdfs."
        }
      }
    }
  },
  "codecDocs": {
    "avro": {
      "description": "Read serialized Avro records as Logstash events",
      "options": {
        "encoding": {
          "type": "string, one of: BINARY_ENCODING, BASE64_ENCODING",
          "default": "BASE64_ENCODING",
          "description": "Set encoding for Avro's payload.  Use `base64` (default) encoding to convert the raw binary bytes to a `base64` encoded string.  Set this option to `binary` to use the plain binary bytes."
        },
        "schema_uri": {
          "type": "string",
          "required": true,
          "description": "schema path to fetch the schema from. This can be a 'http' or 'file' scheme URI example:"
        },
        "tag_on_failure": {
          "type": "boolean",
          "default": "false",
          "description": "tag events with `_avroparsefailure` when decode fails"
        },
        "target": {
          "type": "field_reference",
          "description": "Defines a target field for placing decoded fields. If this setting is omitted, data gets stored at the root (top level) of the event."
        }
      }
    },
    "cef": {
      "description": "Implementation of a Logstash codec for the ArcSight Common Event Format (CEF) Based on Revision 20 of Implementing ArcSight CEF, dated from June 05, 2013 https://community.saas.hpe.com/dcvta86296/attachments/dcvta86296/connector-documentation/1116/1/CommonEventFormatv23.pdf",
      "options": {
        "default_timezone": {
          "type": "string",
          "description": "When parsing timestamps that do not include a UTC offset in payloads that do not include the device's timezone, the default timezone is used. If none is provided the system timezone is used."
        },
        "delimiter": {
          "type": "string",
          "description": "If your input puts a delimiter between each CEF event, you'll want to set this to be that delimiter."
        },
        "device": {
          "type": "string, one of: observer, host",
          "default": "observer",
          "description": "Defines whether a set of device-specific CEF fields represent the _observer_, or the actual `host` on which the event occurred. If this codec handles a mix, it is safe to use the default `observer`."
        },
        "fields": {
          "type": "array",
          "default": "[]",
          "description": "Fields to be included in CEV extension part as key/value pairs"
        },
        "locale": {
          "type": "string",
          "description": "The locale is used to parse abbreviated month names from some CEF timestamp formats. If none is provided, the system default is used."
        },
        "name": {
          "type": "string",
          "default": "Logstash",
          "description": "Name field in CEF header. The new value can include `%{foo}` strings to help you build a new value from other parts of the event."
        },
        "product": {
          "type": "string",
          "default": "Logstash",
          "description": "Device product field in CEF header. The new value can include `%{foo}` strings to help you build a new value from other parts of the event."
        },
        "raw_data_field": {
          "type": "string",
          "description": "If raw_data_field is set, during decode of an event an additional field with the provided name is added, which contains the raw data."
        },
        "reverse_mapping": {
          "type": "boolean",
          "default": "false",
          "description": "When encoding to CEF, set this to true to adhere to the specifications and encode using the CEF key name (short name) for the CEF field names. Defaults to false to preserve previous behaviour that was to use the long version of the CEF field names."
        },
        "severity": {
          "type": "string",
          "default": "6",
          "description": "Severity field in CEF header. The new value can include `%{foo}` strings to help you build a new value from other parts of the event."
        },
        "signature": {
          "type": "string",
          "default": "Logstash",
          "description": "Signature ID field in CEF header. The new value can include `%{foo}` strings to help you build a new value from other parts of the event."
        },
        "vendor": {
          "type": "string",
          "default": "Elasticsearch",
          "description": "Device vendor field in CEF header. The new value can include `%{foo}` strings to help you build a new value from other parts of the event."
        },
        "version": {
          "type": "string",
          "default": "1.0",
          "description": "Device version field in CEF header. The new value can include `%{foo}` strings to help you build a new value from other parts of the event."
        }
      }
    },
    "cloudfront": {
      "description": "This codec will read cloudfront encoded content",
      "options": {
        "charset": {
          "default": "UTF-8",
          "description": "The character encoding used in this codec. Examples include \"UTF-8\" and \"CP1252\""
        }
      }
    },
    "cloudtrail": {
      "description": "This is the base class for logstash codecs.",
      "options": {
        "charset": {
          "default": "UTF-8"
        }
      }
    },
    "collectd": {
      "description": "Read events from the collectd binary protocol over the network via udp. See https://collectd.org/wiki/index.php/Binary_protocol",
      "options": {
        "authfile": {
          "type": "string",
          "description": "Path to the authentication file. This file should have the same format as the AuthFile in collectd. You only need to set this option if the `security_level` is set to `Sign` or `Encrypt`"
        },
        "nan_handling": {
          "type": "string, one of: change_value, warn, drop",
          "default": "change_value",
          "description": "What to do when a value in the event is `NaN` (Not a Number)"
        },
        "nan_tag": {
          "type": "string",
          "default": "_collectdNaN",
          "description": "The tag to add to the event if a `NaN` value was found Set this to an empty string ('') if you don't want to tag"
        },
        "nan_value": {
          "type": "number",
          "default": "0",
          "description": "Only relevant when `nan_handeling` is set to `change_value` Change NaN to this configured value"
        },
        "prune_intervals": {
          "type": "boolean",
          "default": "true",
          "description": "Prune interval records.  Defaults to `true`."
        },
        "security_level": {
          "type": "string, one of: SECURITY_NONE, SECURITY_SIGN, SECURITY_ENCR",
          "default": "None",
          "description": "Security Level. Default is `None`. This setting mirrors the setting from the collectd Network plugin"
        },
        "target": {
          "type": "field_reference",
          "description": "Defines a target field for placing decoded fields. If this setting is omitted, data gets stored at the root (top level) of the event."
        },
        "typesdb": {
          "type": "array",
          "description": "File path(s) to collectd `types.db` to use. The last matching pattern wins if you have identical pattern names in multiple files. If no types.db is provided the included `types.db` will be used (currently 5.4.0)."
        }
      }
    },
    "edn": {
      "options": {
        "target": {
          "type": "field_reference",
          "description": "Defines a target field for placing decoded fields. If this setting is omitted, data gets stored at the root (top level) of the event."
        }
      }
    },
    "edn_lines": {
      "options": {
        "target": {
          "type": "field_reference",
          "description": "Defines a target field for placing decoded fields. If this setting is omitted, data gets stored at the root (top level) of the event."
        }
      }
    },
    "es_bulk": {
      "description": "This codec will decode the Elasticsearch bulk format into individual events, plus metadata into the `@metadata` field.",
      "options": {
        "target": {
          "type": "field_reference",
          "description": "Defines a target field for placing decoded fields. If this setting is omitted, data gets stored at the root (top level) of the event."
        }
      }
    },
    "fluent": {
      "description": "This codec handles fluentd's msgpack schema.",
      "options": {
        "nanosecond_precision": {
          "type": "boolean",
          "default": "false"
        },
        "target": {
          "type": "field_reference",
          "description": "Defines a target field for placing decoded fields. If this setting is omitted, data gets stored at the root (top level) of the event."
        }
      }
    },
    "graphite": {
      "description": "This codec will encode and decode Graphite formated lines.",
      "options": {
        "exclude_metrics": {
          "type": "array",
          "default": "[ \"%\\{[^}]+\\}\" ]",
          "description": "Exclude regex matched metric names, by default exclude unresolved %{field} strings"
        },
        "fields_are_metrics": {
          "type": "boolean",
          "default": "false",
          "description": "Indicate that the event @fields should be treated as metrics and will be sent as is to graphite"
        },
        "include_metrics": {
          "type": "array",
          "default": "[ \".*\" ]",
          "description": "Include only regex matched metric names"
        },
        "metrics": {
          "type": "hash",
          "default": "{}",
          "description": "The metric(s) to use. This supports dynamic strings like `%{host}` for metric names and also for values. This is a hash field with key of the metric name, value of the metric value. Example:"
        },
        "metrics_format": {
          "type": "string",
          "default": "DEFAULT_METRICS_FORMAT",
          "description": "Defines format of the metric string. The placeholder `*` will be replaced with the name of the actual metric. This supports dynamic strings like `%{host}`."
        }
      }
    },
    "json": {
      "description": "This codec may be used to decode (via inputs) and encode (via outputs) full JSON messages. If the data being sent is a JSON array at its root multiple events will be created (one per element).",
      "options": {
        "charset": {
          "default": "UTF-8",
          "description": "The character encoding used in this codec. Examples include \"UTF-8\" and \"CP1252\"."
        },
        "target": {
          "type": "field_reference",
          "description": "Defines a target field for placing decoded fields. If this setting is omitted, data gets stored at the root (top level) of the event. The target is only relevant while decoding data into a new event."
        }
      }
    },
    "json_lines": {
      "description": "This codec will decode streamed JSON that is newline delimited. Encoding will emit a single JSON string ending in a `@delimiter`",
      "options": {
        "charset": {
          "default": "UTF-8",
          "description": "The character encoding used in this codec. Examples include `UTF-8` and `CP1252`"
        },
        "decode_size_limit_bytes": {
          "type": "number",
          "default": "DEFAULT_DECODE_SIZE_LIMIT_BYTES # 512MB",
          "description": "Maximum number of bytes for a single line before a fatal exception is raised which will stop Logstash. The default is 512MB which is quite large for a JSON document"
        },
        "delimiter": {
          "type": "string",
          "default": "\\n",
          "description": "Change the delimiter that separates lines"
        },
        "target": {
          "type": "field_reference",
          "description": "Defines a target field for placing decoded fields. If this setting is omitted, data gets stored at the root (top level) of the event. The target is only relevant while decoding data into a new event."
        }
      }
    },
    "line": {
      "description": "Line-oriented text data.",
      "options": {
        "charset": {
          "default": "UTF-8",
          "description": "The character encoding used in this input. Examples include `UTF-8` and `cp1252`"
        },
        "delimiter": {
          "type": "string",
          "default": "\\n",
          "description": "Change the delimiter that separates lines"
        },
        "format": {
          "type": "string",
          "description": "Set the desired text format for encoding."
        }
      }
    },
    "msgpack": {
      "options": {
        "format": {
          "type": "string",
          "default": "nil"
        },
        "target": {
          "type": "field_reference",
          "description": "Defines a target field for placing decoded fields. If this setting is omitted, data gets stored at the root (top level) of the event."
        }
      }
    },
    "multiline": {
      "options": {
        "auto_flush_interval": {
          "type": "number",
          "description": "The accumulation of multiple lines will be converted to an event when either a matching new line is seen or there has been no new data appended for this many seconds. No default.  If unset, no auto_flush. Units: seconds"
        },
        "charset": {
          "default": "UTF-8",
          "description": "The character encoding used in this input. Examples include `UTF-8` and `cp1252`"
        },
        "max_bytes": {
          "type": "bytes",
          "default": "10 MiB",
          "description": "The accumulation of events can make logstash exit with an out of memory error if event boundaries are not correctly defined. This settings make sure to flush multiline events after reaching a number of bytes, it is used in combination max_lines."
        },
        "max_lines": {
          "type": "number",
          "default": "500",
          "description": "The accumulation of events can make logstash exit with an out of memory error if event boundaries are not correctly defined. This settings make sure to flush multiline events after reaching a number of lines, it is used in combination max_bytes."
        },
        "multiline_tag": {
          "type": "string",
          "default": "multiline",
          "description": "Tag multiline events with a given tag. This tag will only be added to events that actually have multiple lines in them."
        },
        "negate": {
          "type": "boolean",
          "default": "false",
          "description": "Negate the regexp pattern ('if not matched')."
        },
        "pattern": {
          "type": "string",
          "required": true,
          "description": "The regular expression to match."
        },
        "patterns_dir": {
          "type": "array",
          "default": "[]",
          "description": "Logstash ships by default with a bunch of patterns, so you don't necessarily need to define this yourself unless you are adding additional patterns."
        },
        "what": {
          "type": "string, one of: previous, next",
          "required": true,
          "description": "If the pattern matched, does event belong to the next or previous event?"
        }
      }
    },
    "netflow": {
      "options": {
        "cache_save_path": {
          "type": "path",
          "description": "Where to save the template cache This helps speed up processing when restarting logstash (So you don't have to await the arrival of templates) cache will save as path/netflow_templates.cache and/or path/ipfix_templates.cache"
        },
        "cache_ttl": {
          "type": "number",
          "default": "4000",
          "description": "Netflow v9/v10 template cache TTL (minutes)"
        },
        "include_flowset_id": {
          "type": "boolean",
          "default": "false",
          "description": "Only makes sense for ipfix, v9 already includes this Setting to true will include the flowset_id in events Allows you to work with sequences, for instance with the aggregate filter"
        },
        "ipfix_definitions": {
          "type": "path",
          "description": "Override YAML file containing IPFIX field definitions"
        },
        "netflow_definitions": {
          "type": "path",
          "description": "Override YAML file containing Netflow field definitions"
        },
        "target": {
          "type": "string",
          "default": "netflow",
          "description": "Specify into what field you want the Netflow data."
        },
        "versions": {
          "type": "array",
          "default": "[5, 9, 10]",
          "description": "Specify which Netflow versions you will accept."
        }
      }
    },
    "plain": {
      "description": "The \"plain\" codec is for plain text with no delimiting between events.",
      "options": {
        "charset": {
          "default": "UTF-8",
          "description": "The character encoding used in this input. Examples include `UTF-8` and `cp1252`"
        },
        "format": {
          "type": "string",
          "description": "Set the message you which to emit for each event. This supports `sprintf` strings."
        }
      }
    },
    "rubydebug": {
      "description": "The rubydebug codec will output your Logstash event data using the Ruby Amazing Print library.",
      "options": {
        "metadata": {
          "type": "boolean",
          "default": "false",
          "description": "Should the event's metadata be included?"
        }
      }
    }
  },
  "commonOptionDocs": {
    "filter": {
      "add_field": {
        "type": "hash",
        "description": "Add a field to an event if the filter is successful."
      },
      "add_tag": {
        "type": "array",
        "description": "Add tags to an event if the filter is successful."
      },
      "enable_metric": {
        "type": "boolean",
        "default": "true",
        "description": "Enable or disable metric logging."
      },
      "id": {
        "type": "string",
        "description": "Add a unique ID to the plugin configuration."
      },
      "periodic_flush": {
        "type": "boolean",
        "default": "false",
        "description": "Call the filter flush method at regular interval."
      },
      "remove_field": {
        "type": "array",
        "description": "Remove fields from an event if the filter is successful."
      },
      "remove_tag": {
        "type": "array",
        "description": "Remove tags from an event if the filter is successful."
      }
    },
    "input": {
      "add_field": {
        "type": "hash",
        "description": "Add a field to an event."
      },
      "codec": {
        "type": "codec",
        "default": "plain",
        "description": "The codec used for input data."
      },
      "enable_metric": {
        "type": "boolean",
        "default": "true",
        "description": "Enable or disable metric logging."
      },
      "id": {
        "type": "string",
        "description": "Add a unique ID to the plugin configuration."
      },
      "tags": {
        "type": "array",
        "description": "Add any number of arbitrary tags to your event."
      },
      "type": {
        "type": "string",
        "description": "Add a type field to all events handled by this input."
      }
    },
    "output": {
      "codec": {
        "type": "codec",
        "default": "plain",
        "description": "The codec used for output data."
      },
      "enable_metric": {
        "type": "boolean",
        "default": "true",
        "description": "Enable or disable metric logging."
      },
      "id": {
        "type": "string",
        "description": "Add a unique ID to the plugin configuration."
      },
      "workers": {
        "type": "number",
        "default": "1",
        "description": "Number of workers to use for this output."
      }
    }
  }
}
